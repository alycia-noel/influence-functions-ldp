{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3f320e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import copy\n",
    "import torch\n",
    "import pickle\n",
    "import random\n",
    "import logging\n",
    "import warnings\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import torch.optim as opt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.autograd import grad\n",
    "from torch.autograd.functional import vhp\n",
    "from get_datasets import get_diabetes, get_adult, get_law\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, accuracy_score\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "E = math.e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d88723b",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd5b9aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_result(e_k_actual, e_k_estimated, ep, k_):\n",
    "\n",
    "    #e_k_estimated = [-1*ek for ek in e_k_estimated]\n",
    "    fig, ax = plt.subplots()\n",
    "    palette = sns.color_palette(\"cool\", len(e_k_actual))\n",
    "    sns.set(font_scale=1.15)\n",
    "    sns.set_style(style='white')\n",
    "    min_x = np.min(e_k_actual)\n",
    "    max_x = np.max(e_k_actual)\n",
    "    min_y = np.min(e_k_estimated)\n",
    "    max_y = np.max(e_k_estimated)\n",
    "    \n",
    "    plt.rcParams['figure.figsize'] = 6, 5\n",
    "    z = np.polyfit(e_k_actual,  e_k_estimated, 1)\n",
    "    p = np.poly1d(z)\n",
    "    xx = np.linspace(-p(2)/p(1), max(e_k_actual)+.0001)\n",
    "    yy = np.polyval(p, xx)\n",
    "    #add trendline to plot\n",
    "    ax.plot(xx, yy, ls=\"-\", color='k')\n",
    "    for k in range(len(e_k_actual)):\n",
    "        ax.scatter(e_k_actual[k], e_k_estimated[k], zorder=2, s=45, color = palette[k], label=ep[k])\n",
    "\n",
    "    ax.set_title(f'Actual vs. Estimated loss for k={k_:.2f}%')\n",
    "    ax.set_xlabel('Actual loss difference')\n",
    "    ax.set_ylabel('Estimated loss difference')\n",
    "   \n",
    "    ax.set_xlim(min_x-.0001, max_x+.0001)\n",
    "    ax.set_ylim(min_y-.0001, max_y+.0001)\n",
    "    \n",
    "   \n",
    "    text = 'MAE = {:.03}\\nP = {:.03}'.format(mean_absolute_error(e_k_actual, e_k_estimated), spearmanr(e_k_actual, e_k_estimated).correlation)\n",
    "    #ax.text(max_x+.00009,min_y-.00008, text, verticalalignment='bottom', horizontalalignment='right')\n",
    "    #ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    #plt.tight_layout()\n",
    "    plt.xticks(rotation = 45)\n",
    "    plt.show()\n",
    "    # cooler color = smaller epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "741afa72",
   "metadata": {},
   "outputs": [],
   "source": [
    " class CreateData(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        out_data = self.data[idx]\n",
    "        out_label = self.targets[idx]\n",
    "\n",
    "        return out_data, out_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09d3bfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(new_train_df, feature_set, label, k):    \n",
    "\n",
    "    selected_group = new_train_df.loc[new_train_df['sex'] == 0]\n",
    "\n",
    "    num_to_sample = round((k / 100)*len(selected_group))\n",
    "\n",
    "    sampled_group = selected_group.sample(n=num_to_sample)\n",
    "    not_selected = new_train_df.drop(sampled_group.index)\n",
    "\n",
    "    selected_group_X = sampled_group[feature_set]\n",
    "    selected_group_y = sampled_group[label]\n",
    "\n",
    "    not_selected_group_X = not_selected[feature_set]\n",
    "    not_selected_group_y = not_selected[label]   \n",
    "    \n",
    "    return selected_group_X, selected_group_y, not_selected_group_X, not_selected_group_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e210d31",
   "metadata": {},
   "source": [
    "### Randomized Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f20c9e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_p(epsilon):\n",
    "    probability = float(E ** epsilon) / float(1 + (E ** epsilon))\n",
    "    p = torch.FloatTensor([[probability, 1-probability], [1-probability, probability]])\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07756821",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9daf5801",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(num_features, 1, bias=False)\n",
    "        self.criterion = torch.nn.BCEWithLogitsLoss(reduction='mean')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        logits = self.fc1(x)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "    def loss(self, logits, y):\n",
    "        loss = self.criterion(logits.ravel(), y)\n",
    "        \n",
    "        probabilities = torch.sigmoid(logits)\n",
    "        thresh_results = []\n",
    "        \n",
    "        for p in probabilities:\n",
    "            if p>.5:\n",
    "                thresh_results.append(1)\n",
    "            else:\n",
    "                thresh_results.append(0)\n",
    "                \n",
    "        num_correct = 0\n",
    "        for r,y_ in zip(thresh_results, y):\n",
    "            if r == y_:\n",
    "                num_correct += 1\n",
    "                \n",
    "        acc = num_correct / len(y)\n",
    "        \n",
    "        return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5c1a9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataset):\n",
    "    model.train()\n",
    "    \n",
    "    opt = torch.optim.SGD(model.parameters(), lr=.005, weight_decay=0)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss(reduction='mean')\n",
    "\n",
    "            \n",
    "    train_data = CreateData(dataset[0], dataset[1])\n",
    "    train_dataloader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "\n",
    "    for itr in range(0, 7):\n",
    "        for i, [x,y] in enumerate(train_dataloader):\n",
    "            opt.zero_grad()\n",
    "            oupt = model(x)\n",
    "            \n",
    "            try:\n",
    "                loss_val = criterion(oupt.ravel(), y)\n",
    "            except ValueError:\n",
    "                loss_val = criterion(oupt, y)\n",
    "            \n",
    "            loss_val.backward()\n",
    "            opt.step() \n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3c7499",
   "metadata": {},
   "source": [
    "### Influence Calculation Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cc60cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_influence_single(model, epsilon, train_data, test_data, group_data, device, num_features, criterion):\n",
    "    start = time.time()\n",
    "    est_hess = explicit_hess(model, train_data, device, criterion)\n",
    "\n",
    "    grad_test = grad_z([test_data[0], test_data[1]], model, device, criterion)\n",
    "    s_test_vec = torch.mm(grad_test[0], est_hess.to(device))\n",
    "\n",
    "    P = get_p(epsilon)\n",
    "    \n",
    "    p_01, p_10 = P[0][1].item(), P[1][0].item()\n",
    "    \n",
    "    pi_1 = sum(list(group_data[1]))\n",
    "    pi_0 = len(group_data[1]) - pi_1\n",
    "    \n",
    "    lam_0 = round(p_01 * pi_1)\n",
    "    lam_1 = round(p_10 * pi_0)\n",
    "\n",
    "    S_pert = 1 - group_data[1]\n",
    "    \n",
    "    y_w_group_pert = pd.concat([group_data[3], S_pert], axis = 0, ignore_index=True)\n",
    "    y_wo_pert = pd.concat([group_data[3], group_data[1]], axis = 0, ignore_index=True)\n",
    "    reconstructed_x = pd.concat([group_data[2], group_data[0]], axis = 0, ignore_index=True)\n",
    "  \n",
    "    assert len(S_pert) == len(group_data[1])\n",
    "    grad_z_vec = grad_training([group_data[0],group_data[1]], S_pert, [model], device, [lam_0, lam_1, epsilon], criterion)\n",
    "    \n",
    "    influence = torch.dot(s_test_vec.flatten(), grad_z_vec[0].flatten()) * (1/len(train_data[0]))\n",
    "    #print(influence)\n",
    "    end = time.time() - start\n",
    "\n",
    "    return influence.cpu(), end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50b0ea98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explicit_hess(model, train_data, device, criterion):\n",
    " \n",
    "    logits = model(train_data[0])\n",
    "    loss = criterion(logits.ravel(), train_data[1]) #reduction mean\n",
    "    \n",
    "    grads = grad(loss, model.parameters(), retain_graph=True, create_graph=True)\n",
    "\n",
    "    hess_params = torch.zeros(len(model.fc1.weight[0]), len(model.fc1.weight[0]))\n",
    "    for i in range(len(model.fc1.weight[0])):\n",
    "        hess_params_ = grad(grads[0][0][i], model.parameters(), retain_graph=True)[0][0]\n",
    "        for j, hp in enumerate(hess_params_):\n",
    "            hess_params[i,j] = hp\n",
    "    \n",
    "    inv_hess = torch.linalg.inv(hess_params)\n",
    "    return inv_hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1656b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_z(test_data, model, device, criterion):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    test_data_features = test_data[0]\n",
    "    test_data_labels = test_data[1]\n",
    "\n",
    "    logits = model(test_data_features)\n",
    "    loss = criterion(logits, torch.atleast_2d(test_data_labels).T) # reduction mean\n",
    "    \n",
    "    return grad(loss, model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38ac6b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_training(train_data, y_perts, parameters, device, epsilon, criterion):\n",
    "    \n",
    "    criterion = torch.nn.BCEWithLogitsLoss(reduction='sum')\n",
    "    \n",
    "    lam_0, lam_1, ep = epsilon\n",
    "    lam = lam_0 + lam_1\n",
    "    len_s = len(y_perts)\n",
    "    \n",
    "    train_data_features = torch.FloatTensor(train_data[0].values).to(device)\n",
    "    train_data_labels = torch.FloatTensor(train_data[1].values).to(device)\n",
    "    train_pert_data_labels = torch.FloatTensor(y_perts.values).to(device)\n",
    "    \n",
    "    model = parameters[0]\n",
    "    model.eval()\n",
    "\n",
    "    logits = model(train_data_features)\n",
    "\n",
    "    orig_loss = criterion(logits, torch.atleast_2d(train_data_labels).T)\n",
    "    pert_loss = criterion(logits, torch.atleast_2d(train_pert_data_labels).T)\n",
    "    loss = float(1/(1 + (E ** ep)))*(pert_loss - orig_loss)\n",
    "    \n",
    "    to_return = grad(loss, model.parameters())\n",
    "    \n",
    "        \n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f72f48",
   "metadata": {},
   "source": [
    "### Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe081fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Main(dataset, epsilons, ks, num_rounds):\n",
    "\n",
    "    device = 'cuda:3' if torch.cuda.is_available() else 'cpu'\n",
    "    criterion = torch.nn.BCEWithLogitsLoss(reduction='mean')\n",
    "    \n",
    "    all_orig_loss_e_k = []\n",
    "    all_est_loss_e_k = []\n",
    "    all_time = []\n",
    "    \n",
    "    for nr in range(num_rounds):\n",
    "        print(f'\\nRound {nr+1}')\n",
    "        ############\n",
    "        # Get data #\n",
    "        ############\n",
    "        print('\\nGetting Data...')\n",
    "        if dataset == 'adult':\n",
    "            data = get_adult()\n",
    "            label = 'income_class'\n",
    "        elif dataset == 'diabetes':\n",
    "            data = get_diabetes()\n",
    "            label = 'readmitted'\n",
    "        else:\n",
    "            data = get_law()\n",
    "            label = 'admit'\n",
    "\n",
    "        feature_set = set(data.columns) - {label}\n",
    "        num_features = len(feature_set)\n",
    "    \n",
    "        X = data[feature_set]\n",
    "        y = data[label]\n",
    "\n",
    "        if dataset == 'diabetes':\n",
    "            undersample = RandomUnderSampler(random_state=42)\n",
    "            new_X, new_y = undersample.fit_resample(X, y)\n",
    "        else:\n",
    "            new_X = X\n",
    "            new_y = y\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(new_X, new_y, test_size=0.20, random_state=42)\n",
    "  \n",
    "        new_train_df = pd.concat([X_train, y_train], axis=1)\n",
    "  \n",
    "        train_sample_num = len(X_train)\n",
    "    \n",
    "        x_test_input = torch.FloatTensor(X_test.values).to(device)\n",
    "        y_test_input = torch.FloatTensor(y_test.values).to(device)\n",
    "\n",
    "        x_train_input = torch.FloatTensor(X_train.values).to(device)\n",
    "        y_train_input = torch.FloatTensor(y_train.values).to(device)\n",
    "   \n",
    "        ##############################################\n",
    "        # Train original model and get original loss #\n",
    "        ##############################################\n",
    "        print('Training original model...')\n",
    "        start = time.time()\n",
    "        torch_model = LogisticRegression(num_features)\n",
    "        torch.save(torch_model.state_dict(), 'models/initial_config.pth')\n",
    "        torch_model.to(device)\n",
    "        torch_model = train(torch_model, [x_train_input, y_train_input])\n",
    "        end = time.time() - start\n",
    "        \n",
    "        print(f'Total model training time: {end:.3f}')\n",
    "        test_loss_ori, acc_ori = torch_model.loss(torch_model(x_test_input), y_test_input)\n",
    "        \n",
    "        e_k_act_losses = []\n",
    "        e_k_est_losses = []\n",
    "        influence_time = []\n",
    "        \n",
    "        ################################################################\n",
    "        # Perform influence and retraining for all epsilons a k values #\n",
    "        ################################################################\n",
    "        print('\\nBegining epsilon and k rounds')\n",
    "        print('-----------------------------')\n",
    "        for k in ks:\n",
    "            print(f'\\nk: {k}')\n",
    "            \n",
    "            k_act_losses = []\n",
    "            k_est_losses = []\n",
    "            inf_time = []\n",
    "            total_time = 0\n",
    "            for ep in epsilons:\n",
    "                # Influence\n",
    "                print(f'ep: {ep}')\n",
    "                selected_group_X, selected_group_y, not_selected_group_X, not_selected_group_y = get_data(new_train_df, feature_set, label, k)\n",
    "\n",
    "#                 loss_diff_approx, tot_time = calc_influence_single(torch_model, ep, [x_train_input, y_train_input], [x_test_input, y_test_input], [selected_group_X, selected_group_y, not_selected_group_X, not_selected_group_y], device, num_features, criterion)\n",
    "#                 total_time += tot_time\n",
    "#                 loss_diff_approx = -torch.FloatTensor(loss_diff_approx).cpu().numpy()\n",
    "                #print(f'Ep {ep} retrain time: {tot_time:.3f}')\n",
    "                # Retrain\n",
    "                start = time.time()\n",
    "                P = get_p(ep)\n",
    "\n",
    "                p_01, p_10 = P[0][1].item(), P[1][0].item()\n",
    "\n",
    "                pi_1 = sum(list(selected_group_y))\n",
    "                pi_0 = len(selected_group_y) - pi_1\n",
    "\n",
    "                lam_0 = round(p_01 * pi_1)\n",
    "                lam_1 = round(p_10 * pi_0)\n",
    "\n",
    "                S = pd.concat([selected_group_X, selected_group_y], axis=1, ignore_index=False)\n",
    "\n",
    "                G0 = S[label][S[label].eq(1)].sample(lam_0).index\n",
    "                G1 = S[label][S[label].eq(0)].sample(lam_1).index\n",
    "\n",
    "                G = S.loc[G0.union(G1)]\n",
    "                not_g = S.drop(G0.union(G1))\n",
    "\n",
    "                G_pert = 1 - G[label]\n",
    "\n",
    "                y_w_group_pert = pd.concat([not_selected_group_y, not_g[label], G_pert], axis = 0, ignore_index=True)\n",
    "                y_wo_pert = pd.concat([not_selected_group_y, not_g[label], G[label]], axis = 0, ignore_index=True)\n",
    "                reconstructed_x = pd.concat([not_selected_group_X, not_g[feature_set], G[feature_set]], axis = 0, ignore_index=True)\n",
    "\n",
    "                model_pert = LogisticRegression(num_features)\n",
    "                model_pert.load_state_dict(torch.load('models/initial_config.pth'))\n",
    "                model_pert.to(device)\n",
    "                model_pert = train(model_pert, [torch.FloatTensor(reconstructed_x.values).to(device), torch.FloatTensor(y_w_group_pert.values).to(device)])\n",
    "                test_loss_retrain, acc_retrain = model_pert.loss(model_pert(x_test_input), y_test_input)\n",
    "\n",
    "                 # get true loss diff\n",
    "                loss_diff_true = (test_loss_retrain - test_loss_ori).detach().cpu().item()\n",
    "                print(f'Retrain time: {time.time() - start:.3f}')\n",
    "#                 print(loss_diff_true)\n",
    "#                 k_act_losses.append(loss_diff_true)\n",
    "#                 k_est_losses.append(loss_diff_approx)\n",
    "#                 inf_time.append(tot_time)\n",
    "            print(f'Total influence time: {total_time}, Average influence time: {total_time/len(epsilons)}')\n",
    "#             e_k_act_losses.append(k_act_losses)\n",
    "#             e_k_est_losses.append(k_est_losses)\n",
    "#             influence_time.append(inf_time)\n",
    "            \n",
    "#         all_orig_loss_e_k.append(e_k_act_losses)\n",
    "#         all_est_loss_e_k.append(e_k_est_losses) \n",
    "#         all_time.append(influence_time)\n",
    "    \n",
    "#     return all_orig_loss_e_k, all_est_loss_e_k, all_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dfc9de",
   "metadata": {},
   "source": [
    "### Perform Experiment "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2747a8a8",
   "metadata": {},
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0e3ef67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epsilons = [.01, .02, .03, .04, .05, .06, .07, .08, .09, .1, .2, .3, .4, .5, .6, .7, .8, .9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "k = np.linspace(1, 25, 10)\n",
    "rounds = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15d8f406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.          3.66666667  6.33333333  9.         11.66666667 14.33333333\n",
      " 17.         19.66666667 22.33333333 25.        ]\n"
     ]
    }
   ],
   "source": [
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff4d7588",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Round 1\n",
      "\n",
      "Getting Data...\n",
      "Training original model...\n",
      "Total model training time: 3.403\n",
      "\n",
      "Begining epsilon and k rounds\n",
      "-----------------------------\n",
      "\n",
      "k: 1.0\n",
      "ep: 0.01\n",
      "Retrain time: 3.060\n",
      "ep: 0.02\n",
      "Retrain time: 3.038\n",
      "ep: 0.03\n",
      "Retrain time: 3.019\n",
      "ep: 0.04\n",
      "Retrain time: 3.302\n",
      "ep: 0.05\n",
      "Retrain time: 3.313\n",
      "ep: 0.06\n",
      "Retrain time: 3.247\n",
      "ep: 0.07\n",
      "Retrain time: 3.149\n",
      "ep: 0.08\n",
      "Retrain time: 3.201\n",
      "ep: 0.09\n",
      "Retrain time: 3.332\n",
      "ep: 0.1\n",
      "Retrain time: 3.405\n",
      "ep: 0.2\n",
      "Retrain time: 2.803\n",
      "ep: 0.3\n",
      "Retrain time: 3.166\n",
      "ep: 0.4\n",
      "Retrain time: 3.025\n",
      "ep: 0.5\n",
      "Retrain time: 3.474\n",
      "ep: 0.6\n",
      "Retrain time: 2.987\n",
      "ep: 0.7\n",
      "Retrain time: 3.359\n",
      "ep: 0.8\n",
      "Retrain time: 3.240\n",
      "ep: 0.9\n",
      "Retrain time: 2.898\n",
      "ep: 1\n",
      "Retrain time: 3.036\n",
      "ep: 2\n",
      "Retrain time: 3.089\n",
      "ep: 3\n",
      "Retrain time: 3.094\n",
      "ep: 4\n",
      "Retrain time: 3.145\n",
      "ep: 5\n",
      "Retrain time: 3.325\n",
      "ep: 6\n",
      "Retrain time: 3.179\n",
      "ep: 7\n",
      "Retrain time: 3.252\n",
      "ep: 8\n",
      "Retrain time: 2.996\n",
      "ep: 9\n",
      "Retrain time: 3.094\n",
      "ep: 10\n",
      "Retrain time: 3.250\n",
      "Total influence time: 0, Average influence time: 0.0\n",
      "\n",
      "k: 3.6666666666666665\n",
      "ep: 0.01\n",
      "Retrain time: 3.135\n",
      "ep: 0.02\n",
      "Retrain time: 3.129\n",
      "ep: 0.03\n",
      "Retrain time: 2.865\n",
      "ep: 0.04\n",
      "Retrain time: 2.868\n",
      "ep: 0.05\n",
      "Retrain time: 3.447\n",
      "ep: 0.06\n",
      "Retrain time: 3.605\n",
      "ep: 0.07\n",
      "Retrain time: 3.063\n",
      "ep: 0.08\n",
      "Retrain time: 3.134\n",
      "ep: 0.09\n",
      "Retrain time: 3.061\n",
      "ep: 0.1\n",
      "Retrain time: 2.932\n",
      "ep: 0.2\n",
      "Retrain time: 3.016\n",
      "ep: 0.3\n",
      "Retrain time: 3.002\n",
      "ep: 0.4\n",
      "Retrain time: 3.067\n",
      "ep: 0.5\n",
      "Retrain time: 2.763\n",
      "ep: 0.6\n",
      "Retrain time: 3.205\n",
      "ep: 0.7\n",
      "Retrain time: 3.394\n",
      "ep: 0.8\n",
      "Retrain time: 3.126\n",
      "ep: 0.9\n",
      "Retrain time: 3.044\n",
      "ep: 1\n",
      "Retrain time: 3.069\n",
      "ep: 2\n",
      "Retrain time: 3.268\n",
      "ep: 3\n",
      "Retrain time: 3.407\n",
      "ep: 4\n",
      "Retrain time: 2.937\n",
      "ep: 5\n",
      "Retrain time: 3.148\n",
      "ep: 6\n",
      "Retrain time: 3.327\n",
      "ep: 7\n",
      "Retrain time: 3.099\n",
      "ep: 8\n",
      "Retrain time: 3.092\n",
      "ep: 9\n",
      "Retrain time: 3.145\n",
      "ep: 10\n",
      "Retrain time: 3.033\n",
      "Total influence time: 0, Average influence time: 0.0\n",
      "\n",
      "k: 6.333333333333333\n",
      "ep: 0.01\n",
      "Retrain time: 3.241\n",
      "ep: 0.02\n",
      "Retrain time: 3.069\n",
      "ep: 0.03\n",
      "Retrain time: 3.101\n",
      "ep: 0.04\n",
      "Retrain time: 3.042\n",
      "ep: 0.05\n",
      "Retrain time: 3.040\n",
      "ep: 0.06\n",
      "Retrain time: 3.193\n",
      "ep: 0.07\n",
      "Retrain time: 3.043\n",
      "ep: 0.08\n",
      "Retrain time: 3.026\n",
      "ep: 0.09\n",
      "Retrain time: 2.890\n",
      "ep: 0.1\n",
      "Retrain time: 2.771\n",
      "ep: 0.2\n",
      "Retrain time: 2.934\n",
      "ep: 0.3\n",
      "Retrain time: 3.125\n",
      "ep: 0.4\n",
      "Retrain time: 3.144\n",
      "ep: 0.5\n",
      "Retrain time: 3.113\n",
      "ep: 0.6\n",
      "Retrain time: 3.068\n",
      "ep: 0.7\n",
      "Retrain time: 3.293\n",
      "ep: 0.8\n",
      "Retrain time: 3.307\n",
      "ep: 0.9\n",
      "Retrain time: 3.185\n",
      "ep: 1\n",
      "Retrain time: 2.802\n",
      "ep: 2\n",
      "Retrain time: 3.005\n",
      "ep: 3\n",
      "Retrain time: 2.920\n",
      "ep: 4\n",
      "Retrain time: 2.851\n",
      "ep: 5\n",
      "Retrain time: 3.411\n",
      "ep: 6\n",
      "Retrain time: 3.041\n",
      "ep: 7\n",
      "Retrain time: 3.017\n",
      "ep: 8\n",
      "Retrain time: 3.048\n",
      "ep: 9\n",
      "Retrain time: 3.058\n",
      "ep: 10\n",
      "Retrain time: 3.172\n",
      "Total influence time: 0, Average influence time: 0.0\n",
      "\n",
      "k: 9.0\n",
      "ep: 0.01\n",
      "Retrain time: 3.369\n",
      "ep: 0.02\n",
      "Retrain time: 3.049\n",
      "ep: 0.03\n",
      "Retrain time: 3.266\n",
      "ep: 0.04\n",
      "Retrain time: 3.067\n",
      "ep: 0.05\n",
      "Retrain time: 3.152\n",
      "ep: 0.06\n",
      "Retrain time: 3.158\n",
      "ep: 0.07\n",
      "Retrain time: 2.975\n",
      "ep: 0.08\n",
      "Retrain time: 3.005\n",
      "ep: 0.09\n",
      "Retrain time: 3.234\n",
      "ep: 0.1\n",
      "Retrain time: 3.078\n",
      "ep: 0.2\n",
      "Retrain time: 3.288\n",
      "ep: 0.3\n",
      "Retrain time: 2.988\n",
      "ep: 0.4\n",
      "Retrain time: 3.042\n",
      "ep: 0.5\n",
      "Retrain time: 3.385\n",
      "ep: 0.6\n",
      "Retrain time: 3.085\n",
      "ep: 0.7\n",
      "Retrain time: 3.095\n",
      "ep: 0.8\n",
      "Retrain time: 3.304\n",
      "ep: 0.9\n",
      "Retrain time: 3.258\n",
      "ep: 1\n",
      "Retrain time: 3.217\n",
      "ep: 2\n",
      "Retrain time: 3.033\n",
      "ep: 3\n",
      "Retrain time: 3.027\n",
      "ep: 4\n",
      "Retrain time: 3.047\n",
      "ep: 5\n",
      "Retrain time: 3.020\n",
      "ep: 6\n",
      "Retrain time: 2.930\n",
      "ep: 7\n",
      "Retrain time: 2.931\n",
      "ep: 8\n",
      "Retrain time: 3.020\n",
      "ep: 9\n",
      "Retrain time: 2.867\n",
      "ep: 10\n",
      "Retrain time: 3.459\n",
      "Total influence time: 0, Average influence time: 0.0\n",
      "\n",
      "k: 11.666666666666666\n",
      "ep: 0.01\n",
      "Retrain time: 2.947\n",
      "ep: 0.02\n",
      "Retrain time: 3.199\n",
      "ep: 0.03\n",
      "Retrain time: 3.140\n",
      "ep: 0.04\n",
      "Retrain time: 3.194\n",
      "ep: 0.05\n",
      "Retrain time: 3.096\n",
      "ep: 0.06\n",
      "Retrain time: 3.201\n",
      "ep: 0.07\n",
      "Retrain time: 3.067\n",
      "ep: 0.08\n",
      "Retrain time: 3.072\n",
      "ep: 0.09\n",
      "Retrain time: 2.954\n",
      "ep: 0.1\n",
      "Retrain time: 3.178\n",
      "ep: 0.2\n",
      "Retrain time: 3.101\n",
      "ep: 0.3\n",
      "Retrain time: 3.072\n",
      "ep: 0.4\n",
      "Retrain time: 3.090\n",
      "ep: 0.5\n",
      "Retrain time: 2.789\n",
      "ep: 0.6\n",
      "Retrain time: 2.785\n",
      "ep: 0.7\n",
      "Retrain time: 2.975\n",
      "ep: 0.8\n",
      "Retrain time: 3.110\n",
      "ep: 0.9\n",
      "Retrain time: 2.990\n",
      "ep: 1\n",
      "Retrain time: 3.005\n",
      "ep: 2\n",
      "Retrain time: 3.160\n",
      "ep: 3\n",
      "Retrain time: 2.964\n",
      "ep: 4\n",
      "Retrain time: 2.904\n",
      "ep: 5\n",
      "Retrain time: 3.357\n",
      "ep: 6\n",
      "Retrain time: 3.070\n",
      "ep: 7\n",
      "Retrain time: 3.212\n",
      "ep: 8\n",
      "Retrain time: 3.147\n",
      "ep: 9\n",
      "Retrain time: 3.184\n",
      "ep: 10\n",
      "Retrain time: 3.534\n",
      "Total influence time: 0, Average influence time: 0.0\n",
      "\n",
      "k: 14.333333333333332\n",
      "ep: 0.01\n",
      "Retrain time: 3.068\n",
      "ep: 0.02\n",
      "Retrain time: 3.188\n",
      "ep: 0.03\n",
      "Retrain time: 3.131\n",
      "ep: 0.04\n",
      "Retrain time: 3.265\n",
      "ep: 0.05\n",
      "Retrain time: 3.216\n",
      "ep: 0.06\n",
      "Retrain time: 2.776\n",
      "ep: 0.07\n",
      "Retrain time: 2.766\n",
      "ep: 0.08\n",
      "Retrain time: 3.002\n",
      "ep: 0.09\n",
      "Retrain time: 3.066\n",
      "ep: 0.1\n",
      "Retrain time: 3.177\n",
      "ep: 0.2\n",
      "Retrain time: 3.093\n",
      "ep: 0.3\n",
      "Retrain time: 3.112\n",
      "ep: 0.4\n",
      "Retrain time: 3.203\n",
      "ep: 0.5\n",
      "Retrain time: 3.185\n",
      "ep: 0.6\n",
      "Retrain time: 3.464\n",
      "ep: 0.7\n",
      "Retrain time: 3.042\n",
      "ep: 0.8\n",
      "Retrain time: 3.002\n",
      "ep: 0.9\n",
      "Retrain time: 3.568\n",
      "ep: 1\n",
      "Retrain time: 3.370\n",
      "ep: 2\n",
      "Retrain time: 3.105\n",
      "ep: 3\n",
      "Retrain time: 3.130\n",
      "ep: 4\n",
      "Retrain time: 3.227\n",
      "ep: 5\n",
      "Retrain time: 3.216\n",
      "ep: 6\n",
      "Retrain time: 3.309\n",
      "ep: 7\n",
      "Retrain time: 3.098\n",
      "ep: 8\n",
      "Retrain time: 3.419\n",
      "ep: 9\n",
      "Retrain time: 3.041\n",
      "ep: 10\n",
      "Retrain time: 3.536\n",
      "Total influence time: 0, Average influence time: 0.0\n",
      "\n",
      "k: 17.0\n",
      "ep: 0.01\n",
      "Retrain time: 3.231\n",
      "ep: 0.02\n",
      "Retrain time: 3.134\n",
      "ep: 0.03\n",
      "Retrain time: 2.962\n",
      "ep: 0.04\n",
      "Retrain time: 3.112\n",
      "ep: 0.05\n",
      "Retrain time: 3.067\n",
      "ep: 0.06\n",
      "Retrain time: 3.095\n",
      "ep: 0.07\n",
      "Retrain time: 3.052\n",
      "ep: 0.08\n",
      "Retrain time: 3.074\n",
      "ep: 0.09\n",
      "Retrain time: 3.011\n",
      "ep: 0.1\n",
      "Retrain time: 3.108\n",
      "ep: 0.2\n",
      "Retrain time: 3.083\n",
      "ep: 0.3\n",
      "Retrain time: 3.315\n",
      "ep: 0.4\n",
      "Retrain time: 3.052\n",
      "ep: 0.5\n",
      "Retrain time: 3.090\n",
      "ep: 0.6\n",
      "Retrain time: 2.854\n",
      "ep: 0.7\n",
      "Retrain time: 2.934\n",
      "ep: 0.8\n",
      "Retrain time: 2.935\n",
      "ep: 0.9\n",
      "Retrain time: 2.875\n",
      "ep: 1\n",
      "Retrain time: 2.911\n",
      "ep: 2\n",
      "Retrain time: 2.981\n",
      "ep: 3\n",
      "Retrain time: 2.983\n",
      "ep: 4\n",
      "Retrain time: 2.906\n",
      "ep: 5\n",
      "Retrain time: 3.016\n",
      "ep: 6\n",
      "Retrain time: 2.828\n",
      "ep: 7\n",
      "Retrain time: 2.963\n",
      "ep: 8\n",
      "Retrain time: 2.980\n",
      "ep: 9\n",
      "Retrain time: 3.027\n",
      "ep: 10\n",
      "Retrain time: 2.743\n",
      "Total influence time: 0, Average influence time: 0.0\n",
      "\n",
      "k: 19.666666666666664\n",
      "ep: 0.01\n",
      "Retrain time: 3.039\n",
      "ep: 0.02\n",
      "Retrain time: 3.031\n",
      "ep: 0.03\n",
      "Retrain time: 3.035\n",
      "ep: 0.04\n",
      "Retrain time: 3.105\n",
      "ep: 0.05\n",
      "Retrain time: 2.711\n",
      "ep: 0.06\n",
      "Retrain time: 3.039\n",
      "ep: 0.07\n",
      "Retrain time: 3.068\n",
      "ep: 0.08\n",
      "Retrain time: 2.917\n",
      "ep: 0.09\n",
      "Retrain time: 3.111\n",
      "ep: 0.1\n",
      "Retrain time: 2.942\n",
      "ep: 0.2\n",
      "Retrain time: 2.925\n",
      "ep: 0.3\n",
      "Retrain time: 3.164\n",
      "ep: 0.4\n",
      "Retrain time: 2.984\n",
      "ep: 0.5\n",
      "Retrain time: 2.939\n",
      "ep: 0.6\n",
      "Retrain time: 3.144\n",
      "ep: 0.7\n",
      "Retrain time: 3.150\n",
      "ep: 0.8\n",
      "Retrain time: 2.961\n",
      "ep: 0.9\n",
      "Retrain time: 3.050\n",
      "ep: 1\n",
      "Retrain time: 3.086\n",
      "ep: 2\n",
      "Retrain time: 2.957\n",
      "ep: 3\n",
      "Retrain time: 3.120\n",
      "ep: 4\n",
      "Retrain time: 2.936\n",
      "ep: 5\n",
      "Retrain time: 2.915\n",
      "ep: 6\n",
      "Retrain time: 2.961\n",
      "ep: 7\n",
      "Retrain time: 3.279\n",
      "ep: 8\n",
      "Retrain time: 3.080\n",
      "ep: 9\n",
      "Retrain time: 2.910\n",
      "ep: 10\n",
      "Retrain time: 3.191\n",
      "Total influence time: 0, Average influence time: 0.0\n",
      "\n",
      "k: 22.333333333333332\n",
      "ep: 0.01\n",
      "Retrain time: 3.052\n",
      "ep: 0.02\n",
      "Retrain time: 2.847\n",
      "ep: 0.03\n",
      "Retrain time: 2.965\n",
      "ep: 0.04\n",
      "Retrain time: 3.268\n",
      "ep: 0.05\n",
      "Retrain time: 3.152\n",
      "ep: 0.06\n",
      "Retrain time: 3.061\n",
      "ep: 0.07\n",
      "Retrain time: 3.014\n",
      "ep: 0.08\n",
      "Retrain time: 3.024\n",
      "ep: 0.09\n",
      "Retrain time: 2.925\n",
      "ep: 0.1\n",
      "Retrain time: 3.106\n",
      "ep: 0.2\n",
      "Retrain time: 3.002\n",
      "ep: 0.3\n",
      "Retrain time: 3.223\n",
      "ep: 0.4\n",
      "Retrain time: 3.064\n",
      "ep: 0.5\n",
      "Retrain time: 2.937\n",
      "ep: 0.6\n",
      "Retrain time: 3.097\n",
      "ep: 0.7\n",
      "Retrain time: 2.988\n",
      "ep: 0.8\n",
      "Retrain time: 2.952\n",
      "ep: 0.9\n",
      "Retrain time: 2.963\n",
      "ep: 1\n",
      "Retrain time: 3.133\n",
      "ep: 2\n",
      "Retrain time: 3.249\n",
      "ep: 3\n",
      "Retrain time: 3.245\n",
      "ep: 4\n",
      "Retrain time: 3.059\n",
      "ep: 5\n",
      "Retrain time: 3.279\n",
      "ep: 6\n",
      "Retrain time: 3.126\n",
      "ep: 7\n",
      "Retrain time: 3.085\n",
      "ep: 8\n",
      "Retrain time: 3.102\n",
      "ep: 9\n",
      "Retrain time: 3.124\n",
      "ep: 10\n",
      "Retrain time: 3.006\n",
      "Total influence time: 0, Average influence time: 0.0\n",
      "\n",
      "k: 25.0\n",
      "ep: 0.01\n",
      "Retrain time: 3.425\n",
      "ep: 0.02\n",
      "Retrain time: 3.237\n",
      "ep: 0.03\n",
      "Retrain time: 3.261\n",
      "ep: 0.04\n",
      "Retrain time: 3.146\n",
      "ep: 0.05\n",
      "Retrain time: 3.434\n",
      "ep: 0.06\n",
      "Retrain time: 3.187\n",
      "ep: 0.07\n",
      "Retrain time: 2.937\n",
      "ep: 0.08\n",
      "Retrain time: 3.139\n",
      "ep: 0.09\n",
      "Retrain time: 2.882\n",
      "ep: 0.1\n",
      "Retrain time: 3.039\n",
      "ep: 0.2\n",
      "Retrain time: 3.349\n",
      "ep: 0.3\n",
      "Retrain time: 3.081\n",
      "ep: 0.4\n",
      "Retrain time: 3.308\n",
      "ep: 0.5\n",
      "Retrain time: 3.286\n",
      "ep: 0.6\n",
      "Retrain time: 3.369\n",
      "ep: 0.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrain time: 3.384\n",
      "ep: 0.8\n",
      "Retrain time: 3.081\n",
      "ep: 0.9\n",
      "Retrain time: 3.256\n",
      "ep: 1\n",
      "Retrain time: 2.932\n",
      "ep: 2\n",
      "Retrain time: 3.679\n",
      "ep: 3\n",
      "Retrain time: 3.525\n",
      "ep: 4\n",
      "Retrain time: 3.149\n",
      "ep: 5\n",
      "Retrain time: 3.311\n",
      "ep: 6\n",
      "Retrain time: 3.054\n",
      "ep: 7\n",
      "Retrain time: 3.505\n",
      "ep: 8\n",
      "Retrain time: 2.891\n",
      "ep: 9\n",
      "Retrain time: 3.587\n",
      "ep: 10\n",
      "Retrain time: 3.130\n",
      "Total influence time: 0, Average influence time: 0.0\n",
      "\n",
      "Round 2\n",
      "\n",
      "Getting Data...\n",
      "Training original model...\n",
      "Total model training time: 2.780\n",
      "\n",
      "Begining epsilon and k rounds\n",
      "-----------------------------\n",
      "\n",
      "k: 1.0\n",
      "ep: 0.01\n",
      "Retrain time: 3.361\n",
      "ep: 0.02\n",
      "Retrain time: 3.146\n",
      "ep: 0.03\n",
      "Retrain time: 3.019\n",
      "ep: 0.04\n",
      "Retrain time: 3.299\n",
      "ep: 0.05\n",
      "Retrain time: 3.614\n",
      "ep: 0.06\n",
      "Retrain time: 4.085\n",
      "ep: 0.07\n",
      "Retrain time: 3.703\n",
      "ep: 0.08\n",
      "Retrain time: 3.263\n",
      "ep: 0.09\n",
      "Retrain time: 3.620\n",
      "ep: 0.1\n",
      "Retrain time: 3.484\n",
      "ep: 0.2\n",
      "Retrain time: 3.720\n",
      "ep: 0.3\n",
      "Retrain time: 3.562\n",
      "ep: 0.4\n",
      "Retrain time: 3.636\n",
      "ep: 0.5\n",
      "Retrain time: 3.324\n",
      "ep: 0.6\n",
      "Retrain time: 3.507\n",
      "ep: 0.7\n",
      "Retrain time: 3.455\n",
      "ep: 0.8\n",
      "Retrain time: 3.511\n",
      "ep: 0.9\n",
      "Retrain time: 3.828\n",
      "ep: 1\n",
      "Retrain time: 3.822\n",
      "ep: 2\n",
      "Retrain time: 3.681\n",
      "ep: 3\n",
      "Retrain time: 3.479\n",
      "ep: 4\n",
      "Retrain time: 3.600\n",
      "ep: 5\n",
      "Retrain time: 3.807\n",
      "ep: 6\n",
      "Retrain time: 4.144\n",
      "ep: 7\n",
      "Retrain time: 3.573\n",
      "ep: 8\n",
      "Retrain time: 3.323\n",
      "ep: 9\n",
      "Retrain time: 3.821\n",
      "ep: 10\n",
      "Retrain time: 3.176\n",
      "Total influence time: 0, Average influence time: 0.0\n",
      "\n",
      "k: 3.6666666666666665\n",
      "ep: 0.01\n",
      "Retrain time: 3.432\n",
      "ep: 0.02\n",
      "Retrain time: 3.418\n",
      "ep: 0.03\n",
      "Retrain time: 3.653\n",
      "ep: 0.04\n",
      "Retrain time: 3.788\n",
      "ep: 0.05\n",
      "Retrain time: 3.690\n",
      "ep: 0.06\n",
      "Retrain time: 3.485\n",
      "ep: 0.07\n",
      "Retrain time: 3.539\n",
      "ep: 0.08\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m all_orig_loss_e_k, all_est_loss_e_k, all_time \u001b[38;5;241m=\u001b[39m \u001b[43mMain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43madult\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilons\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrounds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# [round1[epsilon1[k1,...k10], epsilon2[k1,...k10],...], round2[...]]     \u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall_orig_loss_e_k.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:   \u001b[38;5;66;03m#Pickling\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[12], line 119\u001b[0m, in \u001b[0;36mMain\u001b[0;34m(dataset, epsilons, ks, num_rounds)\u001b[0m\n\u001b[1;32m    117\u001b[0m model_pert\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/initial_config.pth\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    118\u001b[0m model_pert\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 119\u001b[0m model_pert \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_pert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFloatTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreconstructed_x\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFloatTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_w_group_pert\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m test_loss_retrain, acc_retrain \u001b[38;5;241m=\u001b[39m model_pert\u001b[38;5;241m.\u001b[39mloss(model_pert(x_test_input), y_test_input)\n\u001b[1;32m    122\u001b[0m  \u001b[38;5;66;03m# get true loss diff\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 14\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataset)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, [x,y] \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m     13\u001b[0m     opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 14\u001b[0m     oupt \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m         loss_val \u001b[38;5;241m=\u001b[39m criterion(oupt\u001b[38;5;241m.\u001b[39mravel(), y)\n",
      "File \u001b[0;32m~/.conda/envs/influence/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[6], line 9\u001b[0m, in \u001b[0;36mLogisticRegression.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m----> 9\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "File \u001b[0;32m~/.conda/envs/influence/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/influence/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m)\n",
      "File \u001b[0;32m~/.conda/envs/influence/lib/python3.9/site-packages/torch/nn/modules/module.py:1252\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_is_full_backward_hook\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1250\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_full_backward_hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModule\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m   1253\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1254\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_orig_loss_e_k, all_est_loss_e_k, all_time = Main('adult', epsilons, k, rounds)\n",
    "# [round1[epsilon1[k1,...k10], epsilon2[k1,...k10],...], round2[...]]     \n",
    "\n",
    "with open('all_orig_loss_e_k.txt', \"wb\") as file:   #Pickling\n",
    "    pickle.dump(all_orig_loss_e_k, file)\n",
    "\n",
    "with open('all_est_loss_e_k.txt', \"wb\") as file2:   #Pickling\n",
    "    pickle.dump(all_est_loss_e_k, file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eec92f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d743c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1619a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('all_orig_loss_e_k.txt', 'rb') as f:\n",
    "    all_orig_loss_e_k = pickle.load(f)\n",
    "    \n",
    "with open('all_est_loss_e_k.txt', 'rb') as f:\n",
    "    all_est_loss_e_k = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68ef165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [actual, estimate]\n",
    "\n",
    "sum_orig_loss_e_k = [[0 for _ in range(len(k))] for _ in range(len(epsilons))]\n",
    "sum_est_loss_e_k = [[0 for _ in range(len(k))] for _ in range(len(epsilons))]\n",
    "sum_time = [[0 for _ in range(len(k))] for _ in range(len(epsilons))]\n",
    "\n",
    "avg_orig_loss = []\n",
    "avg_est_loss = []\n",
    "avg_time = []\n",
    "\n",
    "for round_ in range(len(all_orig_loss_e_k)):\n",
    "    for e in range(len(epsilons)):\n",
    "        for k_ in range(len(k)):\n",
    "            sum_orig_loss_e_k[e][k_] = sum_orig_loss_e_k[e][k_] + all_orig_loss_e_k[round_][e][k_]\n",
    "            sum_est_loss_e_k[e][k_] = sum_est_loss_e_k[e][k_] + all_est_loss_e_k[round_][e][k_]\n",
    "            sum_time[e][k_] = sum_time[e][k_] + all_time[round_][e][k_]\n",
    "\n",
    "for e in range(len(epsilons)):\n",
    "    avg_orig_loss.append([ elem / len(all_orig_loss_e_k) for elem in sum_orig_loss_e_k[e]])\n",
    "    avg_est_loss.append([elem/ len(all_orig_loss_e_k) for elem in sum_est_loss_e_k[e]])\n",
    "    avg_time.append([elem/ len(all_orig_loss_e_k) for elem in sum_time[e]])\n",
    "\n",
    "k_e_orig = [[] for _ in range(len(k))]\n",
    "k_e_est = [[] for _ in range(len(k))]\n",
    "\n",
    "for e in range(len(epsilons)):\n",
    "    for k_ in range(len(k)):\n",
    "        k_e_orig[k_].append(avg_orig_loss[e][k_])\n",
    "        k_e_est[k_].append(avg_est_loss[e][k_])\n",
    "        \n",
    "print(k_e_est)\n",
    "\n",
    "averaged_time = []\n",
    "\n",
    "for e in range(len(epsilons)):\n",
    "    averaged_time.append(sum_time[e][0])\n",
    "\n",
    "average_time_final = sum(averaged_time) / len(averaged_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8527e23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k = np.linspace(1, 25, 10)\n",
    "k_ = list(k)\n",
    "k_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752355d6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(k_e_orig)):\n",
    "    visualize_result(k_e_orig[i], k_e_est[i], epsilons, k_[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d815add",
   "metadata": {},
   "outputs": [],
   "source": [
    "spearman_g_ep = []\n",
    "spearman_g_k = []\n",
    "\n",
    "mae_g_ep = []\n",
    "mae_g_k = []\n",
    "    \n",
    "for i in range(len(avg_orig_loss)):\n",
    "    \n",
    "\n",
    "    spearman = []\n",
    "    mae = []\n",
    "    \n",
    "    for j in range(len(actual)):\n",
    "        spearman.append(spearmanr(actual[j], estimated[j]).correlation)\n",
    "        mae.append(mean_absolute_error(actual[j], estimated[j]))\n",
    "        \n",
    "    spearman_g_ep.append(spearman)\n",
    "    mae_g_ep.append(mae)\n",
    "    \n",
    "for i, kge in enumerate(k_g_e):\n",
    "    actual = [[x[0] for x in kge[0]], [x[0] for x in kge[1]]]#, [x[0] for x in kge[2]], [x[0] for x in kge[3]], [x[0] for x in kge[4]]]\n",
    "    estimated = [[x[1] for x in kge[0]], [x[1] for x in kge[1]]]#, [x[1] for x in kge[2]], [x[1] for x in kge[3]], [x[1] for x in kge[4]]]\n",
    "\n",
    "    spearman = []\n",
    "    mae = []\n",
    "    \n",
    "    for j in range(len(actual)):\n",
    "        spearman.append(spearmanr(actual[j], estimated[j]).correlation)\n",
    "        mae.append(mean_absolute_error(actual[j], estimated[j]))\n",
    "        \n",
    "    spearman_g_k.append(spearman)\n",
    "    mae_g_k.append(mae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73eb9ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(avg_orig_loss[4])\n",
    "print(avg_est_loss[4])\n",
    "print()\n",
    "print(avg_orig_loss[13])\n",
    "print(avg_est_loss[13])\n",
    "print()\n",
    "print(avg_orig_loss[22])\n",
    "print(avg_est_loss[22])\n",
    "print()\n",
    "print(avg_orig_loss[ -5])\n",
    "print(avg_est_loss[-5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4998b0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(spearman_g_ep), len(spearman_g_ep[0])\n",
    "spearman_g_ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f1ae1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(spearman_g_k), len(spearman_g_k[0])\n",
    "spearman_g_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c888d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mae_g_ep), len(mae_g_ep[0])\n",
    "mae_g_ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e4f546",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mae_g_k), len(mae_g_k[0])\n",
    "mae_g_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d614d025",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_est = [22, 20,23,21,19,18,16,17,15,14,12,13,11,10,9,8,7,6,4,5,3,2,1]\n",
    "rank_act = [23,20,21,22,17,18,16,19,14,15,13,12,11,10,8,7,9,6,4,5,2,3,1]\n",
    "a= np.cov(np.array(rank_est), np.array(rank_act), bias=True)[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af31895",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.std(np.array(rank_est))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53aa47f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.std(np.array(rank_act))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df00495f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a /(b*c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f76788c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a, b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb85715",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
