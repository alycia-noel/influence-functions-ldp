{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3f320e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import copy\n",
    "import torch\n",
    "import pickle\n",
    "import random\n",
    "import logging\n",
    "import warnings\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import torch.optim as opt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.autograd import grad\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd.functional import vhp\n",
    "from get_datasets import get_diabetes, get_adult, get_law\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, accuracy_score\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "CUDA_LAUNCH_BLOCKING = 1\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "E = math.e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d88723b",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd5b9aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_result(e_k_actual, e_k_estimated, ep, k_):\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    palette = sns.color_palette(\"cool\", len(e_k_actual))\n",
    "    sns.set(font_scale=1.15)\n",
    "    sns.set_style(style='white')\n",
    "    min_x = np.min(e_k_actual)\n",
    "    max_x = np.max(e_k_actual)\n",
    "    min_y = np.min(e_k_estimated)\n",
    "    max_y = np.max(e_k_estimated)\n",
    "    \n",
    "    plt.rcParams['figure.figsize'] = 6, 5\n",
    "    z = np.polyfit(e_k_actual,  e_k_estimated, 1)\n",
    "    p = np.poly1d(z)\n",
    "    xx = np.linspace(-p(2)/p(1), max(e_k_actual)+.0001)\n",
    "    yy = np.polyval(p, xx)\n",
    "    #add trendline to plot\n",
    "    ax.plot(xx, yy, ls=\"-\", color='k')\n",
    "    for k in range(len(e_k_actual)):\n",
    "        ax.scatter(e_k_actual[k], e_k_estimated[k], zorder=2, s=45, color = palette[k], label=ep[k])\n",
    "\n",
    "    ax.set_title(f'Actual vs. Estimated loss for k={k_:.2f}%')\n",
    "    ax.set_xlabel('Actual loss difference')\n",
    "    ax.set_ylabel('Estimated loss difference')\n",
    "   \n",
    "    ax.set_xlim(min_x-.0001, max_x+.0001)\n",
    "    ax.set_ylim(min_y-.0001, max_y+.0001)\n",
    "    \n",
    "   \n",
    "    text = 'MAE = {:.03}\\nP = {:.03}'.format(mean_absolute_error(e_k_actual, e_k_estimated), spearmanr(e_k_actual, e_k_estimated).correlation)\n",
    "    ax.text(max_x+.00009,min_y-.00008, text, verticalalignment='bottom', horizontalalignment='right')\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    #plt.tight_layout()\n",
    "    plt.xticks(rotation = 45)\n",
    "    plt.show()\n",
    "    # cooler color = smaller epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "741afa72",
   "metadata": {},
   "outputs": [],
   "source": [
    " class CreateData(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, targets, pert_status):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.pert = pert_status\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        out_data = self.data[idx]\n",
    "        out_label = self.targets[idx]\n",
    "        pert_label = self.pert[idx]\n",
    "\n",
    "        return out_data, out_label, pert_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0416ac8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(new_train_df, feature_set, label, k):    \n",
    "\n",
    "    # based on Race or Gender\n",
    "    selected_group = new_train_df.loc[new_train_df['sex'] == 0]\n",
    "\n",
    "    num_to_sample = round((k / 100)*len(selected_group))\n",
    "\n",
    "    sampled_group = selected_group.sample(n=num_to_sample)\n",
    "    not_selected = new_train_df.drop(sampled_group.index)\n",
    "\n",
    "    selected_group_X = sampled_group[feature_set]\n",
    "    selected_group_y = sampled_group[label]\n",
    "\n",
    "    not_selected_group_X = not_selected[feature_set]\n",
    "    not_selected_group_y = not_selected[label]   \n",
    "   \n",
    "    return selected_group_X, selected_group_y, not_selected_group_X, not_selected_group_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e210d31",
   "metadata": {},
   "source": [
    "### Randomized Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14f44d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_p(epsilon):\n",
    "    probability = float(E ** epsilon) / float(1 + (E ** epsilon))\n",
    "    p = torch.FloatTensor([[probability, 1-probability], [1-probability, probability]])\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11dbe6d",
   "metadata": {},
   "source": [
    "### Forward Loss Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8171a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_correct_loss(y_act, oupt, epsilon, criterion, no_correct):\n",
    "    \n",
    "    if no_correct:\n",
    "        y_pred_c = oupt\n",
    "    else:\n",
    "        p = get_p(epsilon)\n",
    "        y_pred_c = torch.matmul(oupt, p) \n",
    "        \n",
    "    loss = criterion(torch.log(y_pred_c), y_act)\n",
    "    \n",
    "    return loss "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07756821",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9daf5801",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(num_features, 2, bias=True)\n",
    "        self.criterion = torch.nn.NLLLoss()\n",
    "        self.activ = torch.nn.Softmax()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        logits = self.activ(self.fc1(x))\n",
    "\n",
    "        return logits\n",
    "    \n",
    "    def loss(self, logits, y):\n",
    "        loss = forward_correct_loss(y, logits, None, self.criterion, no_correct=True)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ebf1727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataset, epsilon, lengths):\n",
    "    model.train()\n",
    "    \n",
    "    opt = torch.optim.SGD(model.parameters(), lr=.001, weight_decay=0)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    pert_status = np.zeros(len(dataset[0]))\n",
    "    \n",
    "    if lengths is not None:\n",
    "        len_original = lengths[0]\n",
    "        len_perts = lengths[1]\n",
    "        total_len = len(dataset[0])\n",
    "        pert_status = []\n",
    "        pert_status.extend(np.zeros(len_original))\n",
    "        pert_status.extend(np.ones(len_perts))\n",
    "            \n",
    "    train_data = CreateData(dataset[0], dataset[1], pert_status)\n",
    "    train_dataloader = DataLoader(train_data, batch_size=1, shuffle=True)\n",
    "\n",
    "    for itr in range(0, 7):\n",
    "        itr_loss = 0\n",
    "        correct = 0\n",
    "        for i, [x,y,p] in enumerate(train_dataloader):\n",
    "            opt.zero_grad()\n",
    "            oupt = model(x)\n",
    "            if p == 0:\n",
    "                loss_val = forward_correct_loss(y, oupt, epsilon, criterion, no_correct=True)\n",
    "            else:\n",
    "                loss_val = forward_correct_loss(y, oupt, epsilon, criterion, no_correct=False)\n",
    "            itr_loss += loss_val\n",
    "            loss_val.backward()\n",
    "            opt.step()\n",
    "            \n",
    "            _, predicted = torch.max(oupt.data, 1)\n",
    "            correct+= (predicted == y.data).sum()\n",
    "            \n",
    "        print(f'Epoch {itr+1} - Loss: {itr_loss/len(train_dataloader):.3f} - Acc: {100*correct/len(train_dataloader):.3f}')\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3c7499",
   "metadata": {},
   "source": [
    "### Influence Calculation Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cc60cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_influence_single(model, epsilon, train_data, test_data, group_data, device, num_features, criterion):\n",
    "    start = time.time()\n",
    " \n",
    "    est_hess = explicit_hess(model, train_data, device, criterion)\n",
    "\n",
    "    grad_test = grad_z([test_data[0], test_data[1]], model, device, criterion)\n",
    "    s_test_vec = torch.mm(grad_test[0], est_hess.to(device))\n",
    "\n",
    "    P = get_p(epsilon)\n",
    "    \n",
    "    p_01, p_10 = P[0][1].item(), P[1][0].item()\n",
    "    \n",
    "    pi_1 = sum(list(group_data[1]))\n",
    "    pi_0 = len(group_data[1]) - pi_1\n",
    "    \n",
    "    lam_0 = round(p_01 * pi_1)\n",
    "    lam_1 = round(p_10 * pi_0)\n",
    "\n",
    "    S_pert = 1 - group_data[1]\n",
    "    \n",
    "    y_w_group_pert = pd.concat([group_data[3], S_pert], axis = 0, ignore_index=True)\n",
    "    y_wo_pert = pd.concat([group_data[3], group_data[1]], axis = 0, ignore_index=True)\n",
    "    reconstructed_x = pd.concat([group_data[2], group_data[0]], axis = 0, ignore_index=True)\n",
    "  \n",
    "    assert len(S_pert) == len(group_data[1])\n",
    "    grad_z_vec = grad_training([group_data[0],group_data[1]], S_pert, [model], device, [lam_0, lam_1, epsilon], criterion, 'loss')\n",
    "  \n",
    "    influence = torch.dot(s_test_vec.flatten(), grad_z_vec[0].flatten()) * (1/len(train_data[0]))\n",
    "    end = time.time() - start\n",
    "\n",
    "    return influence.cpu(), end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e371a36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explicit_hess(model, train_data, device, criterion):\n",
    " \n",
    "    pred_prob = model(train_data[0])\n",
    "    loss = forward_correct_loss(train_data[1], pred_prob, None, criterion, no_correct=True)\n",
    "    \n",
    "    grads = grad(loss, model.parameters(), retain_graph=True, create_graph=True)\n",
    "\n",
    "    hess_params = torch.zeros(len(model.fc1.weight[0]), len(model.fc1.weight[0]))\n",
    "    for i in range(len(model.fc1.weight[0])):\n",
    "        hess_params_ = grad(grads[0][0][i], model.parameters(), retain_graph=True)[0][0]\n",
    "        for j, hp in enumerate(hess_params_):\n",
    "            hess_params[i,j] = hp\n",
    "    \n",
    "    inv_hess = torch.linalg.inv(hess_params)\n",
    "    \n",
    "    return inv_hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1656b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_z(test_data, model, device, criterion):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    test_data_features = test_data[0]\n",
    "    test_data_labels = test_data[1]\n",
    "\n",
    "    pred_prob = model(test_data_features)\n",
    "    loss = forward_correct_loss(test_data_labels, pred_prob, None, criterion, no_correct=True)\n",
    "    \n",
    "    return grad(loss, model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38ac6b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_training(train_data, y_perts, parameters, device, epsilon, criterion, model_or_loss):\n",
    "\n",
    "    lam_0, lam_1, ep = epsilon\n",
    "    lam = lam_0 + lam_1\n",
    "    len_s = len(y_perts)\n",
    "\n",
    "    train_data_features = torch.FloatTensor(train_data[0].values).to(device)\n",
    "    train_data_labels = torch.LongTensor(train_data[1].values).to(device)\n",
    "    train_pert_data_labels = torch.LongTensor(y_perts.values).to(device)\n",
    "    \n",
    "    model = parameters[0]\n",
    "    model.eval()\n",
    "    \n",
    "    pred_prob = model(train_data_features)\n",
    "\n",
    "    pert_loss = forward_correct_loss(train_pert_data_labels, pred_prob, ep, criterion, no_correct=False)\n",
    "    orig_loss = forward_correct_loss(train_data_labels, pred_prob, ep, criterion, no_correct=True)\n",
    "    \n",
    "    loss = float(1/(1 + (E ** ep)))*(pert_loss - orig_loss)\n",
    "    to_return = grad(loss, model.parameters())\n",
    "\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f72f48",
   "metadata": {},
   "source": [
    "### Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe081fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Main(dataset, epsilons, ks, num_rounds):\n",
    "\n",
    "    device = 'cpu'#'cuda:3' if torch.cuda.is_available() else 'cpu'\n",
    "    criterion = torch.nn.NLLLoss(reduction='mean')\n",
    "    \n",
    "    all_orig_loss_e_k = []\n",
    "    all_est_loss_e_k = []\n",
    "    all_time = []\n",
    "    \n",
    "    for nr in range(num_rounds):\n",
    "        print(f'\\nRound {nr+1}')\n",
    "        ############\n",
    "        # Get data #\n",
    "        ############\n",
    "        print('\\nGetting Data...')\n",
    "        if dataset == 'adult':\n",
    "            data = get_adult()\n",
    "            label = 'income_class'\n",
    "        elif dataset == 'diabetes':\n",
    "            data = get_diabetes()\n",
    "            label = 'readmitted'\n",
    "        else:\n",
    "            data = get_law()\n",
    "            label = 'admit'\n",
    "\n",
    "        feature_set = set(data.columns) - {label}\n",
    "        num_features = len(feature_set)\n",
    "    \n",
    "        X = data[feature_set]\n",
    "        y = data[label]\n",
    "\n",
    "        if dataset == 'diabetes':\n",
    "            undersample = RandomUnderSampler(random_state=42)\n",
    "            new_X, new_y = undersample.fit_resample(X, y)\n",
    "        else:\n",
    "            new_X = X\n",
    "            new_y = y\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(new_X, new_y, test_size=0.20, random_state=42)\n",
    "  \n",
    "        new_train_df = pd.concat([X_train, y_train], axis=1)\n",
    "  \n",
    "        train_sample_num = len(X_train)\n",
    "    \n",
    "        x_test_input = torch.FloatTensor(X_test.values).to(device)\n",
    "        y_test_input = torch.LongTensor(y_test.values).to(device)\n",
    "\n",
    "        x_train_input = torch.FloatTensor(X_train.values).to(device)\n",
    "        y_train_input = torch.LongTensor(y_train.values).to(device)\n",
    "   \n",
    "        ##############################################\n",
    "        # Train original model and get original loss #\n",
    "        ##############################################\n",
    "        print('Training original model...')\n",
    "        torch_model = LogisticRegression(num_features)\n",
    "        torch.save(torch_model.state_dict(), 'models/initial_config_flc.pth')\n",
    "        torch_model.to(device)\n",
    "        torch_model = train(torch_model, [x_train_input, y_train_input], None, None)\n",
    "        test_loss_ori = torch_model.loss(torch_model(x_test_input), y_test_input)\n",
    "        \n",
    "        e_k_act_losses = []\n",
    "        e_k_est_losses = []\n",
    "        influence_time = []\n",
    "        \n",
    "        ################################################################\n",
    "        # Perform influence and retraining for all epsilons a k values #\n",
    "        ################################################################\n",
    "        print('\\nBegining epsilon and k rounds')\n",
    "        print('-----------------------------')\n",
    "        for ep in epsilons:\n",
    "            print(f'\\nEpsilon: {ep}')\n",
    "            \n",
    "            k_act_losses = []\n",
    "            k_est_losses = []\n",
    "            inf_time = []\n",
    "            \n",
    "            for k in ks:\n",
    "                \n",
    "                # Influence\n",
    "                print(f'k: {k:.2f}')\n",
    "                \n",
    "                selected_group_X, selected_group_y, not_selected_group_X, not_selected_group_y = get_data(new_train_df, feature_set, label, k)\n",
    "\n",
    "                loss_diff_approx, tot_time = calc_influence_single(torch_model, ep, [x_train_input, y_train_input], [x_test_input, y_test_input], [selected_group_X, selected_group_y, not_selected_group_X, not_selected_group_y], device, num_features, criterion)\n",
    "                loss_diff_approx = -torch.FloatTensor(loss_diff_approx).cpu().numpy()\n",
    "                print(f'Approx diff: {loss_diff_approx:.5f}')\n",
    "                # Retrain\n",
    "                P = get_p(ep)\n",
    "\n",
    "                p_01, p_10 = P[0][1].item(), P[1][0].item()\n",
    "\n",
    "                pi_1 = sum(list(selected_group_y))\n",
    "                pi_0 = len(selected_group_y) - pi_1\n",
    "\n",
    "                lam_0 = round(p_01 * pi_1)\n",
    "                lam_1 = round(p_10 * pi_0)\n",
    "\n",
    "                S = pd.concat([selected_group_X, selected_group_y], axis=1, ignore_index=False)\n",
    "\n",
    "                G0 = S[label][S[label].eq(1)].sample(lam_0).index\n",
    "                G1 = S[label][S[label].eq(0)].sample(lam_1).index\n",
    "\n",
    "                G = S.loc[G0.union(G1)]\n",
    "                not_g = S.drop(G0.union(G1))\n",
    "                print(len(G), len(not_g))\n",
    "                \n",
    "                G_pert = 1 - G[label]\n",
    "\n",
    "                y_w_group_pert = pd.concat([not_selected_group_y, not_g[label], G_pert], axis = 0, ignore_index=True)\n",
    "                y_wo_pert = pd.concat([not_selected_group_y, not_g[label], G[label]], axis = 0, ignore_index=True)\n",
    "                reconstructed_x = pd.concat([not_selected_group_X, not_g[feature_set], G[feature_set]], axis = 0, ignore_index=True)\n",
    "\n",
    "                model_pert = LogisticRegression(num_features)\n",
    "                model_pert.load_state_dict(torch.load('models/initial_config_flc.pth'))\n",
    "                model_pert.to(device)\n",
    "                model_pert = train(model_pert, [torch.FloatTensor(reconstructed_x.values).to(device), torch.LongTensor(y_w_group_pert.values).to(device)], ep, [len(not_selected_group_y)+ len(not_g), len(G)])\n",
    "                test_loss_retrain = model_pert.loss(model_pert(x_test_input), y_test_input)\n",
    "\n",
    "                 # get true loss diff\n",
    "                loss_diff_true = (test_loss_retrain - test_loss_ori).detach().cpu().item()\n",
    "                print(f'True diff: {loss_diff_true:.5f}')\n",
    "                k_act_losses.append(loss_diff_true)\n",
    "                k_est_losses.append(loss_diff_approx)\n",
    "                inf_time.append(tot_time)\n",
    "            \n",
    "            e_k_act_losses.append(k_act_losses)\n",
    "            e_k_est_losses.append(k_est_losses)\n",
    "            influence_time.append(inf_time)\n",
    "            \n",
    "        all_orig_loss_e_k.append(e_k_act_losses)\n",
    "        all_est_loss_e_k.append(e_k_est_losses) \n",
    "        all_time.append(influence_time)\n",
    "    \n",
    "    return all_orig_loss_e_k, all_est_loss_e_k, all_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dfc9de",
   "metadata": {},
   "source": [
    "### Perform Experiment "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2747a8a8",
   "metadata": {},
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e80e77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilons = [.01, .02, .03, .04, .05, .06, .07, .08, .09]#, .1, .2, .3, .4, .5, .6, .7, .8, .9]#, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "k = np.linspace(1, 25, 10)\n",
    "rounds = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53d56f1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.        ,  3.66666667,  6.33333333,  9.        , 11.66666667,\n",
       "       14.33333333, 17.        , 19.66666667, 22.33333333, 25.        ])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "391d9b94",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Round 1\n",
      "\n",
      "Getting Data...\n",
      "Training original model...\n",
      "Epoch 1 - Loss: 0.395 - Acc: 81.380\n",
      "Epoch 2 - Loss: 0.359 - Acc: 83.082\n",
      "Epoch 3 - Loss: 0.353 - Acc: 83.335\n",
      "Epoch 4 - Loss: 0.350 - Acc: 83.392\n",
      "Epoch 5 - Loss: 0.348 - Acc: 83.473\n",
      "Epoch 6 - Loss: 0.347 - Acc: 83.468\n",
      "Epoch 7 - Loss: 0.347 - Acc: 83.473\n",
      "\n",
      "Begining epsilon and k rounds\n",
      "-----------------------------\n",
      "\n",
      "Epsilon: 0.01\n",
      "k: 1.00\n",
      "Approx diff: 0.00001\n",
      "34 35\n",
      "Epoch 1 - Loss: 0.396 - Acc: 81.366\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m all_orig_loss_e_k, all_est_loss_e_k, all_time \u001b[38;5;241m=\u001b[39m \u001b[43mMain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43madult\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilons\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrounds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# [round1[epsilon1[k1,...k10], epsilon2[k1,...k10],...], round2[...]]     \u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall_orig_loss_e_k_flc.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:   \u001b[38;5;66;03m#Pickling\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[13], line 116\u001b[0m, in \u001b[0;36mMain\u001b[0;34m(dataset, epsilons, ks, num_rounds)\u001b[0m\n\u001b[1;32m    114\u001b[0m model_pert\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/initial_config_flc.pth\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    115\u001b[0m model_pert\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 116\u001b[0m model_pert \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_pert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFloatTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreconstructed_x\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLongTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_w_group_pert\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnot_selected_group_y\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnot_g\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m test_loss_retrain \u001b[38;5;241m=\u001b[39m model_pert\u001b[38;5;241m.\u001b[39mloss(model_pert(x_test_input), y_test_input)\n\u001b[1;32m    119\u001b[0m  \u001b[38;5;66;03m# get true loss diff\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 31\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataset, epsilon, lengths)\u001b[0m\n\u001b[1;32m     29\u001b[0m     loss_val \u001b[38;5;241m=\u001b[39m forward_correct_loss(y, oupt, epsilon, criterion, no_correct\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     30\u001b[0m itr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_val\n\u001b[0;32m---> 31\u001b[0m \u001b[43mloss_val\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m opt\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     34\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(oupt\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/influence/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/influence/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_orig_loss_e_k, all_est_loss_e_k, all_time = Main('adult', epsilons, k, rounds)\n",
    "# [round1[epsilon1[k1,...k10], epsilon2[k1,...k10],...], round2[...]]     \n",
    "\n",
    "with open('all_orig_loss_e_k_flc.txt', \"wb\") as file:   #Pickling\n",
    "    pickle.dump(all_orig_loss_e_k, file)\n",
    "\n",
    "with open('all_est_loss_e_k_fkc.txt', \"wb\") as file2:   #Pickling\n",
    "    pickle.dump(all_est_loss_e_k, file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4d7588",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# [actual, estimate]\n",
    "\n",
    "sum_orig_loss_e_k = [[0 for _ in range(len(k))] for _ in range(len(epsilons))]\n",
    "sum_est_loss_e_k = [[0 for _ in range(len(k))] for _ in range(len(epsilons))]\n",
    "sum_time = [[0 for _ in range(len(k))] for _ in range(len(epsilons))]\n",
    "\n",
    "avg_orig_loss = []\n",
    "avg_est_loss = []\n",
    "avg_time = []\n",
    "\n",
    "for round_ in range(len(all_orig_loss_e_k)):\n",
    "    for e in range(len(epsilons)):\n",
    "        for k_ in range(len(k)):\n",
    "            sum_orig_loss_e_k[e][k_] = sum_orig_loss_e_k[e][k_] + all_orig_loss_e_k[round_][e][k_]\n",
    "            sum_est_loss_e_k[e][k_] = sum_est_loss_e_k[e][k_] + all_est_loss_e_k[round_][e][k_]\n",
    "            sum_time[e][k_] = sum_time[e][k_] + all_time[round_][e][k_]\n",
    "\n",
    "for e in range(len(epsilons)):\n",
    "    avg_orig_loss.append([ elem / len(all_orig_loss_e_k) for elem in sum_orig_loss_e_k[e]])\n",
    "    avg_est_loss.append([elem/ len(all_orig_loss_e_k) for elem in sum_est_loss_e_k[e]])\n",
    "    avg_time.append([elem/ len(all_orig_loss_e_k) for elem in sum_time[e]])\n",
    "\n",
    "k_e_orig = [[] for _ in range(len(k))]\n",
    "k_e_est = [[] for _ in range(len(k))]\n",
    "\n",
    "for e in range(len(epsilons)):\n",
    "    for k_ in range(len(k)):\n",
    "        k_e_orig[k_].append(avg_orig_loss[e][k_])\n",
    "        k_e_est[k_].append(avg_est_loss[e][k_])\n",
    "        \n",
    "print(k_e_est)\n",
    "\n",
    "averaged_time = []\n",
    "\n",
    "for e in range(len(epsilons)):\n",
    "    averaged_time.append(sum_time[e][0])\n",
    "\n",
    "average_time_final = sum(averaged_time) / len(averaged_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cf3659",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(k_e_orig)):\n",
    "    visualize_result(k_e_orig[i], k_e_est[i], epsilons, k[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a092c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd192444",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
