{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b521de8",
   "metadata": {},
   "source": [
    "### Import Packages and Set Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f320e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import copy\n",
    "import torch\n",
    "import pickle\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scienceplots\n",
    "import torch.nn as nn\n",
    "import torch.optim as opt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from copy import deepcopy\n",
    "from torch.autograd import grad\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from folktables import ACSDataSource, ACSPublicCoverage\n",
    "from sklearn.metrics import mean_absolute_error, log_loss, accuracy_score\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "E = math.e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d88723b",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40a88cf",
   "metadata": {},
   "source": [
    "#### Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5b9aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_result(e_k_actual, e_k_estimated, ep, k_):\n",
    "    plt.rcParams['figure.dpi'] = 300\n",
    "    plt.style.use(['science'])\n",
    "    colors = cm.cool(np.linspace(0, 1, len(e_k_estimated)))\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    ax.yaxis.set_major_formatter(FormatStrFormatter('%.4f'))\n",
    "    ax.xaxis.set_major_formatter(FormatStrFormatter('%.4f'))\n",
    "    \n",
    "    min_x = np.min(e_k_actual)\n",
    "    max_x = np.max(e_k_actual)\n",
    "    min_y = np.min(e_k_estimated)\n",
    "    max_y = np.max(e_k_estimated)\n",
    "    \n",
    "    z = np.polyfit(e_k_actual,  e_k_estimated, 1)\n",
    "    p = np.poly1d(z)\n",
    "    xx = np.linspace(-p(2)/p(1), max(e_k_actual)+.0001)\n",
    "    yy = np.polyval(p, xx)\n",
    "    \n",
    "    ax.plot(xx, yy, ls=\"-\", color='k')\n",
    "    \n",
    "    for k in range(len(e_k_actual)):\n",
    "        ax.scatter(e_k_actual[k], e_k_estimated[k], zorder=2, s=15, color=colors[k])\n",
    "\n",
    "    ax.set_title(f'Actual vs. Estimated loss for k={k_:.2f}%', fontsize=8)\n",
    "    ax.set_xlabel('Actual loss difference', fontsize=8)\n",
    "    ax.set_ylabel('Estimated loss difference', fontsize=8)\n",
    "   \n",
    "    ax.set_xlim(min_x-.0001, max_x+.0001)\n",
    "    ax.set_ylim(min_y-.0001, max_y+.0001)\n",
    "\n",
    "    text = 'MAE = {:.03}\\nP = {:.03}'.format(mean_absolute_error(e_k_actual, e_k_estimated), spearmanr(e_k_actual, e_k_estimated).correlation)\n",
    "    print(text)\n",
    "    plt.xticks(rotation = 45, fontsize=7, visible=True)\n",
    "    plt.yticks(fontsize=7)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64433d9",
   "metadata": {},
   "source": [
    "#### Select k% of a group (based on gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d3bfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_group(dfTrain, feature_set, label, k):    \n",
    "\n",
    "    selected_group = dfTrain.loc[dfTrain['sex'] == 0]\n",
    "\n",
    "    num_to_sample = int((k/100)*len(dfTrain))\n",
    "\n",
    "    sampled_group = dfTrain.sample(n=num_to_sample, ignore_index=False)\n",
    "    not_selected = dfTrain.drop(sampled_group.index)\n",
    "\n",
    "    selected_group_X = sampled_group[feature_set]\n",
    "    selected_group_y = sampled_group[label]\n",
    "\n",
    "    not_selected_group_X = not_selected[feature_set]\n",
    "    not_selected_group_y = not_selected[label]   \n",
    "    \n",
    "    return selected_group_X, selected_group_y, not_selected_group_X, not_selected_group_y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3af350d",
   "metadata": {},
   "source": [
    "#### Get and clean Adult dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5db6ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adult():\n",
    "    train_url = 'data/adult.data'\n",
    "    test_url = 'data/adult.test'\n",
    "\n",
    "    dfTrain = pd.read_csv(train_url, header=None, sep=',')\n",
    "    dfTest = pd.read_csv(test_url, header=None, sep=',', skiprows=[0])\n",
    "    \n",
    "    # assign column names\n",
    "    columns = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', \n",
    "                       'relationship','race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country',\n",
    "                       'income-class']\n",
    "    \n",
    "    dfTrain.columns, dfTest.columns = columns, columns \n",
    "    \n",
    "    dfTrain[\"income-class\"] = dfTrain[\"income-class\"].str.replace(\".\",\"\",regex=True)\n",
    "    dfTest[\"income-class\"] = dfTest[\"income-class\"].str.replace(\".\",\"\",regex=True)\n",
    "  \n",
    "    # Remove question mark\n",
    "    dfTrain = dfTrain[(dfTrain.values !='?').all(axis=1)]\n",
    "    dfTest = dfTest[(dfTest.values !='?').all(axis=1)]\n",
    "\n",
    "    dfTrain = dfTrain.drop(['fnlwgt','education'], axis=1)\n",
    "    dfTrain = dfTrain.drop_duplicates()\n",
    "    dfTrain = dfTrain.dropna(how='any', axis=0)\n",
    "    \n",
    "    dfTrain['workclass'] = dfTrain['workclass'].str.replace('State-gov', 'Government', regex=True)\n",
    "    dfTrain['workclass'] = dfTrain['workclass'].str.replace('Federal-gov', 'Government', regex=True)\n",
    "    dfTrain['workclass'] = dfTrain['workclass'].str.replace('Local-gov', 'Government', regex=True)\n",
    "    dfTrain['workclass'] = dfTrain['workclass'].str.replace('Self-emp-not-inc', 'Self-Employed', regex=True)\n",
    "    dfTrain['workclass'] = dfTrain['workclass'].str.replace('Self-emp-inc', 'Self-Employed', regex=True)\n",
    "    dfTrain['workclass'] = dfTrain['workclass'].str.replace('Private', 'Privately-Employed', regex=True)\n",
    "    \n",
    "    dfTrain['occupation'] = dfTrain['occupation'].str.replace('Armed-Forces', 'Protective-serv', regex=True)\n",
    "    \n",
    "    dfTrain['marital-status'] = dfTrain['marital-status'].str.replace('Married-AF-spouse', 'Married', regex=True)\n",
    "    dfTrain['marital-status'] = dfTrain['marital-status'].str.replace('Married-civ-spouse', 'Married', regex=True)\n",
    "    dfTrain['marital-status'] = dfTrain['marital-status'].str.replace('Married-spouse-absent', 'Married', regex=True)\n",
    "    dfTrain['marital-status'] = dfTrain['marital-status'].str.replace('Divorced', 'Not-married', regex=True)\n",
    "    dfTrain['marital-status'] = dfTrain['marital-status'].str.replace('Never-married', 'Not-married', regex=True)\n",
    "    dfTrain['marital-status'] = dfTrain['marital-status'].str.replace('Separated', 'Not-married', regex=True)\n",
    "    dfTrain['marital-status'] = dfTrain['marital-status'].str.replace('Widowed', 'Not-married', regex=True)\n",
    "    \n",
    "    dfTrain['sex'] = dfTrain['sex'].astype('category').cat.codes\n",
    "    dfTrain['income-class'] = dfTrain['income-class'].astype('category').cat.codes\n",
    "    dfTrain['race'] = np.where(dfTrain['race'] == ' White', 1,0)\n",
    "    \n",
    "    to_replace = ['workclass', 'education-num', 'marital-status', 'occupation','relationship', 'native-country']\n",
    "    dfTrain = pd.get_dummies(dfTrain, columns=to_replace, drop_first = False)\n",
    "    \n",
    "    dfTrain = dfTrain.drop('native-country_ Holand-Netherlands', axis=1)\n",
    "    dfTrain = dfTrain.drop(['workclass_ ?', 'native-country_ ?', 'occupation_ ?'], axis=1)\n",
    "    \n",
    "    def numericalBinary(dataset, features):\n",
    "        dataset[features] = np.where(dataset[features] >= dataset[features].mean(), 1,0)\n",
    "\n",
    "    numericalBinary(dfTrain,['age', 'hours-per-week', 'capital-gain', 'capital-loss'])\n",
    " \n",
    "    \n",
    "    dfTest = dfTest.drop(['fnlwgt','education'], axis=1)\n",
    "    dfTest = dfTest.drop_duplicates()\n",
    "    dfTest = dfTest.dropna(how='any', axis=0)\n",
    "        \n",
    "    dfTest['workclass'] = dfTest['workclass'].str.replace('State-gov', 'Government', regex=True)\n",
    "    dfTest['workclass'] = dfTest['workclass'].str.replace('Federal-gov', 'Government', regex=True)\n",
    "    dfTest['workclass'] = dfTest['workclass'].str.replace('Local-gov', 'Government', regex=True)\n",
    "    dfTest['workclass'] = dfTest['workclass'].str.replace('Self-emp-not-inc', 'Self-Employed', regex=True)\n",
    "    dfTest['workclass'] = dfTest['workclass'].str.replace('Self-emp-inc', 'Self-Employed', regex=True)\n",
    "    dfTest['workclass'] = dfTest['workclass'].str.replace('Private', 'Privately-Employed', regex=True)\n",
    "    \n",
    "    dfTest['occupation'] = dfTest['occupation'].str.replace('Armed-Forces', 'Protective-serv', regex=True)\n",
    "    \n",
    "    dfTest['marital-status'] = dfTest['marital-status'].str.replace('Married-AF-spouse', 'Married', regex=True)\n",
    "    dfTest['marital-status'] = dfTest['marital-status'].str.replace('Married-civ-spouse', 'Married', regex=True)\n",
    "    dfTest['marital-status'] = dfTest['marital-status'].str.replace('Married-spouse-absent', 'Married', regex=True)\n",
    "    dfTest['marital-status'] = dfTest['marital-status'].str.replace('Divorced', 'Not-married', regex=True)\n",
    "    dfTest['marital-status'] = dfTest['marital-status'].str.replace('Never-married', 'Not-married', regex=True)\n",
    "    dfTest['marital-status'] = dfTest['marital-status'].str.replace('Separated', 'Not-married', regex=True)\n",
    "    dfTest['marital-status'] = dfTest['marital-status'].str.replace('Widowed', 'Not-married', regex=True)\n",
    "    \n",
    "    dfTest['sex'] = dfTest['sex'].astype('category').cat.codes\n",
    "    dfTest['income-class'] = dfTest['income-class'].astype('category').cat.codes\n",
    "    dfTest['race'] = np.where(dfTest['race'] == ' White', 1,0)\n",
    "    \n",
    "    dfTest = pd.get_dummies(dfTest, columns=to_replace, drop_first = False)\n",
    "\n",
    "    numericalBinary(dfTest,['age', 'hours-per-week', 'capital-gain', 'capital-loss'])\n",
    "    \n",
    "    dfTest = dfTest.drop(['workclass_ ?', 'native-country_ ?', 'occupation_ ?'], axis=1)\n",
    "    \n",
    "    data = pd.concat([dfTrain, dfTest], axis=0, ignore_index=False)\n",
    "    data = data.sample(frac=1, ignore_index=True)\n",
    "    \n",
    "\n",
    "    num_train = int(len(data) * .8)\n",
    "    dfTrain = data.sample(n=num_train, replace=False, axis=0, ignore_index=False)\n",
    "\n",
    "    dfTest = data.drop(dfTrain.index, axis=0)\n",
    "    \n",
    "    dfTrain = dfTrain.dropna(how='any', axis=0)\n",
    "    dfTest = dfTest.dropna(how='any', axis=0)\n",
    "\n",
    "    label = 'income-class'\n",
    "    return dfTrain, dfTest, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8762d3",
   "metadata": {},
   "source": [
    "#### Sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4b6937",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e210d31",
   "metadata": {},
   "source": [
    "### Randomized Response\n",
    "Get the corresponding p and q values based on an epsilon value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20c9e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_p(epsilon):\n",
    "    p = float(E ** epsilon) / float(1 + (E ** epsilon))\n",
    "    q = 1-p\n",
    "    \n",
    "    return p, q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07756821",
   "metadata": {},
   "source": [
    "### Models\n",
    "Pytorch logistic regression model used in calculating the influence function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9daf5801",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogReg(torch.nn.Module):\n",
    "    def __init__(self, num_features, scikit_model):\n",
    "        super(LogReg, self).__init__()\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(num_features, 1, bias=False)\n",
    "        self.criterion = torch.nn.BCEWithLogitsLoss(reduction='mean')\n",
    "        \n",
    "        weights = torch.from_numpy(scikit_model.coef_).type(torch.float32)\n",
    "        biases = torch.from_numpy(np.array(scikit_model.intercept_)).type(torch.float32)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.fc1.weight = nn.Parameter(weights)\n",
    "            self.fc1.bias = nn.Parameter(biases)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.fc1(x)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3c7499",
   "metadata": {},
   "source": [
    "### Influence Calculation Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a2654b",
   "metadata": {},
   "source": [
    "#### Main function for calling all required parts to calculate the influence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc60cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_influence_single(scikit_model, epsilon, train_data, test_data, group_data, device, num_features, criterion):\n",
    "        \n",
    "    torch_model = LogReg(num_features, scikit_model)\n",
    "    torch_model.to(device)\n",
    "    \n",
    "    start = time.time()\n",
    "    est_hess = explicit_hess(torch_model, train_data, device, criterion)\n",
    "    \n",
    "    grad_test = grad_z([test_data[0], test_data[1]], torch_model, device, criterion)\n",
    " \n",
    "    s_test_vec = torch.mm(grad_test[0], est_hess.to(device))\n",
    "\n",
    "    p, q = get_p(epsilon) \n",
    "    \n",
    "    p_01, p_10 = q, q\n",
    "    \n",
    "    S_pert = 1 - group_data[1]\n",
    "    \n",
    "    y_w_group_pert = pd.concat([group_data[3], S_pert], axis = 0, ignore_index=True)\n",
    "    y_wo_pert = pd.concat([group_data[3], group_data[1]], axis = 0, ignore_index=True)\n",
    "    reconstructed_x = pd.concat([group_data[2], group_data[0]], axis = 0, ignore_index=True)\n",
    "  \n",
    "    assert len(S_pert) == len(group_data[1])\n",
    "    grad_z_vec = grad_training([group_data[0], group_data[1]], S_pert, torch_model, device, epsilon)\n",
    "   \n",
    "    influence = torch.dot(s_test_vec.flatten(), grad_z_vec[0].flatten()) * (1/len(reconstructed_x))\n",
    "    end = time.time() - start\n",
    "    \n",
    "    return influence.cpu().detach().numpy(), end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ce8d64",
   "metadata": {},
   "source": [
    "#### Explicitly calculate the Hessian matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b0ea98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explicit_hess(model, train_data, device, criterion):\n",
    "\n",
    "    logits = model(train_data[0])\n",
    "    loss = criterion(logits.ravel(), train_data[1]) #reduction mean\n",
    "\n",
    "    grads = grad(loss, model.parameters(), retain_graph=True, create_graph=True)\n",
    "\n",
    "    hess_params = torch.zeros(len(model.fc1.weight[0]), len(model.fc1.weight[0]))\n",
    "    \n",
    "    for i in range(len(model.fc1.weight[0])):\n",
    "        hess_params_ = grad(grads[0][0][i], model.parameters(), retain_graph=True)[0][0]\n",
    "        for j, hp in enumerate(hess_params_):\n",
    "            hess_params[i,j] = hp\n",
    "\n",
    "    inv_hess = torch.linalg.inv(hess_params)\n",
    "    return inv_hess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a34905",
   "metadata": {},
   "source": [
    "#### Get the gradient of the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1656b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_z(test_data, model, device, criterion):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    logits = model(test_data[0])\n",
    "    loss = criterion(logits.ravel(), test_data[1]) # reduction mean\n",
    "    \n",
    "    return grad(loss, model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc98188",
   "metadata": {},
   "source": [
    "#### Get the gradient of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ac6b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_training(train_data, y_perts, model, device, epsilon):\n",
    "    \n",
    "    criterion = torch.nn.BCEWithLogitsLoss(reduction='sum')\n",
    "    \n",
    "    train_data_features = torch.FloatTensor(train_data[0].values).to(device)\n",
    "    train_data_labels = torch.FloatTensor(train_data[1].values).to(device)\n",
    "    train_pert_data_labels = torch.FloatTensor(y_perts.values).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    logits = model(train_data_features)\n",
    "\n",
    "    orig_loss = criterion(logits.ravel(), train_data_labels)\n",
    "    pert_loss = criterion(logits.ravel(), train_pert_data_labels)\n",
    "    loss = float(1/(1 + (E ** epsilon)))*(pert_loss - orig_loss)\n",
    "    \n",
    "    to_return = grad(loss, model.parameters())\n",
    "    \n",
    "        \n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f72f48",
   "metadata": {},
   "source": [
    "### Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe081fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Main(dataset, epsilons, ks, num_rounds):\n",
    "\n",
    "    device = 'cuda:7' if torch.cuda.is_available() else 'cpu'\n",
    "    criterion = torch.nn.BCEWithLogitsLoss(reduction='mean')\n",
    "    \n",
    "    all_orig_loss_e_k = []\n",
    "    all_est_loss_e_k = []\n",
    "    all_time = []\n",
    "    \n",
    "    for nr in range(num_rounds):\n",
    "        print(f'\\nRound {nr+1}')\n",
    "        ############\n",
    "        # Get data #\n",
    "        ############\n",
    "        print('\\nGetting Data...')\n",
    "        \n",
    "        dfTrain, dfTest, label = get_adult()\n",
    "\n",
    "        feature_set = list(set(dfTrain.columns) - {label})\n",
    "        num_features = len(feature_set)\n",
    "\n",
    "        X_train, X_test = dfTrain[feature_set].values, dfTest[feature_set].values\n",
    "        y_train, y_test = dfTrain[label].values, dfTest[label].values\n",
    "    \n",
    "        x_test_input = torch.FloatTensor(X_test).to(device)\n",
    "        y_test_input = torch.FloatTensor(y_test).to(device)\n",
    "\n",
    "        x_train_input = torch.FloatTensor(X_train).to(device)\n",
    "        y_train_input = torch.FloatTensor(y_train).to(device)\n",
    "   \n",
    "        ##############################################\n",
    "        # Train original model and get original loss #\n",
    "        ##############################################\n",
    "        print('Training original model...')\n",
    "        LR = SGDClassifier(loss='log_loss', penalty='None', eta0=0.01, fit_intercept=False, learning_rate='constant')\n",
    "        LR.fit(X_train, y_train)\n",
    "        \n",
    "        model_to_send = deepcopy(LR)\n",
    "        \n",
    "        predictions = LR.predict_proba(X_test)\n",
    "        label_predictions = [np.argmax(p) for p in predictions]\n",
    "      \n",
    "        acc_ori = accuracy_score(y_test, label_predictions)\n",
    "        test_loss_ori = log_loss(y_test, predictions, eps=1e-15, labels=[0,1])\n",
    "        \n",
    "        e_k_act_losses = []\n",
    "        e_k_est_losses = []\n",
    "        influence_time = []\n",
    "                \n",
    "        ################################################################\n",
    "        # Perform influence and retraining for all epsilons a k values #\n",
    "        ################################################################\n",
    "        print('\\nBegining epsilon and k rounds')\n",
    "        print('-----------------------------')\n",
    "        for ep in epsilons:\n",
    "            print(f'\\nEpsilon: {ep}')\n",
    "            \n",
    "            k_act_losses = []\n",
    "            k_est_losses = []\n",
    "            inf_time = []\n",
    "            \n",
    "            for k in ks:\n",
    "                # Influence\n",
    "                print(f'k: {k:.2f}')\n",
    "                selected_group_X, selected_group_y, not_selected_group_X, not_selected_group_y = get_data_group(dfTrain, feature_set, label, k)\n",
    "                loss_diff_approx, tot_time = calc_influence_single(model_to_send, ep, [x_train_input, y_train_input], [x_test_input, y_test_input], [selected_group_X, selected_group_y, not_selected_group_X, not_selected_group_y], device, num_features, criterion)\n",
    "                print(f'Approx difference: {loss_diff_approx:.5f}')\n",
    "              \n",
    "                ###########\n",
    "                # Retrain #\n",
    "                ###########\n",
    "                \n",
    "                p, q = get_p(ep)\n",
    "                \n",
    "                pert_selected_group_y = []\n",
    "                \n",
    "                for i, elem in enumerate(selected_group_y.values):\n",
    "                    rnd = np.random.random()\n",
    "                    if rnd <= p:\n",
    "                        pert_selected_group_y.append(elem)\n",
    "                    else:\n",
    "                        pert_selected_group_y.append(1 - elem)\n",
    "\n",
    "                y_w_group_pert = pd.concat([not_selected_group_y, pd.DataFrame(pert_selected_group_y)], axis = 0, ignore_index=True)\n",
    "                y_wo_pert = pd.concat([not_selected_group_y, selected_group_y], axis = 0, ignore_index=True)\n",
    "                reconstructed_x = pd.concat([not_selected_group_X, selected_group_X], axis = 0, ignore_index=True)\n",
    "                \n",
    "                pert_LR = SGDClassifier(loss='log_loss', penalty='None', eta0=0.01, fit_intercept=False, learning_rate='constant')\n",
    "                pert_LR.fit(reconstructed_x, y_w_group_pert)\n",
    "                pert_param = LR.coef_\n",
    "\n",
    "                pert_predictions = pert_LR.predict_proba(X_test)\n",
    "                pert_label_predictions = [np.argmax(p) for p in pert_predictions]\n",
    "\n",
    "                acc_pert = accuracy_score(y_test, pert_label_predictions)\n",
    "                test_loss_retrain = log_loss(y_test, pert_predictions, eps=1e-15, labels=[0,1])\n",
    "\n",
    "                 # get true loss diff\n",
    "                loss_diff_true = test_loss_retrain - test_loss_ori\n",
    "                print(f'True difference: {loss_diff_true:.5f}')\n",
    "                k_act_losses.append(loss_diff_true)\n",
    "                k_est_losses.append(loss_diff_approx)\n",
    "                inf_time.append(tot_time)\n",
    "            \n",
    "            e_k_act_losses.append(k_act_losses)\n",
    "            e_k_est_losses.append(k_est_losses)\n",
    "            influence_time.append(inf_time)\n",
    "            \n",
    "        all_orig_loss_e_k.append(e_k_act_losses)\n",
    "        all_est_loss_e_k.append(e_k_est_losses) \n",
    "        all_time.append(influence_time)\n",
    "    \n",
    "    return all_orig_loss_e_k, all_est_loss_e_k, all_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dfc9de",
   "metadata": {},
   "source": [
    "### Perform Experiment "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2747a8a8",
   "metadata": {},
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e3ef67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epsilons = np.linspace(.001, 5, 30)\n",
    "k = np.linspace(1, 30, 10)\n",
    "rounds = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2b1576",
   "metadata": {},
   "source": [
    "#### Run experiment and save results to pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4d7588",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_orig_loss_e_k, all_est_loss_e_k, all_time = Main('adult', epsilons, k, rounds)\n",
    "\n",
    "with open('results/adult/all_orig_loss_e_k_adult.txt', \"wb\") as file:   #Pickling\n",
    "    pickle.dump(all_orig_loss_e_k, file)\n",
    "\n",
    "with open('results/adult/all_est_loss_e_k_adult.txt', \"wb\") as file2:   #Pickling\n",
    "    pickle.dump(all_est_loss_e_k, file2)\n",
    "    \n",
    "with open('results/adult/all_time_adult.txt', \"wb\") as file3:   #Pickling\n",
    "    pickle.dump(all_time, file3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf8cb47",
   "metadata": {},
   "source": [
    "#### Reorganize results for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68ef165",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_orig_loss_e_k = [[0 for _ in range(len(k))] for _ in range(len(epsilons))]\n",
    "sum_est_loss_e_k = [[0 for _ in range(len(k))] for _ in range(len(epsilons))]\n",
    "sum_time = [[0 for _ in range(len(k))] for _ in range(len(epsilons))]\n",
    "\n",
    "avg_orig_loss = []\n",
    "avg_est_loss = []\n",
    "avg_time = []\n",
    "\n",
    "for round_ in range(len(all_orig_loss_e_k)):\n",
    "    for e in range(len(epsilons)):\n",
    "        for k_ in range(len(k)):\n",
    "            sum_orig_loss_e_k[e][k_] = sum_orig_loss_e_k[e][k_] + all_orig_loss_e_k[round_][e][k_]\n",
    "            sum_est_loss_e_k[e][k_] = sum_est_loss_e_k[e][k_] + all_est_loss_e_k[round_][e][k_]\n",
    "            \n",
    "\n",
    "for e in range(len(epsilons)):\n",
    "    avg_orig_loss.append([ elem / len(all_orig_loss_e_k) for elem in sum_orig_loss_e_k[e]])\n",
    "    avg_est_loss.append([elem/ len(all_orig_loss_e_k) for elem in sum_est_loss_e_k[e]])\n",
    "\n",
    "k_e_orig = [[] for _ in range(len(k))]\n",
    "k_e_est = [[] for _ in range(len(k))]\n",
    "\n",
    "for e in range(len(epsilons)):\n",
    "    for k_ in range(len(k)):\n",
    "        k_e_orig[k_].append(avg_orig_loss[e][k_])\n",
    "        k_e_est[k_].append(avg_est_loss[e][k_])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de87d2f",
   "metadata": {},
   "source": [
    "#### Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752355d6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(k_e_orig)):\n",
    "    visualize_result(k_e_orig[i], k_e_est[i], epsilons, k[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3b58b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
