{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e364451",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import copy\n",
    "import torch\n",
    "import random\n",
    "import logging\n",
    "import warnings\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import torch.optim as opt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn import linear_model\n",
    "from torch.autograd import grad\n",
    "from torch.autograd.functional import vhp\n",
    "from data_processing import get_data_adult\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, accuracy_score\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "E = math.e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d88723b",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dafa202c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph(data, title, x_label, y_label, labels):\n",
    "    sns.set(font_scale=1)\n",
    "\n",
    "    ticks = np.arange(0, len(data[2]), step=1)\n",
    "    plt.xticks(ticks=ticks, labels=data[2], rotation='vertical')\n",
    "    \n",
    "    plt.plot(data[0], 'b-', linewidth=2.0, label=labels[0])\n",
    "    plt.plot(data[1], 'r-', linewidth=2.0, label=labels[1])\n",
    "    plt.plot([a_i - b_i for a_i, b_i in zip(data[0], data[1])], 'k', linewidth=2.0, label='difference')\n",
    "    \n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "741afa72",
   "metadata": {},
   "outputs": [],
   "source": [
    " class CreateData(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        out_data = self.data[idx]\n",
    "        out_label = self.targets[idx]\n",
    "\n",
    "        return out_data, out_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e210d31",
   "metadata": {},
   "source": [
    "### Randomized Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "096c08d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize_resp(label, epsilon):\n",
    "\n",
    "    probability = float(E ** epsilon) / float(1 + (E ** epsilon))\n",
    "    \n",
    "    if label == 0:\n",
    "        new_label = np.random.choice([0,1], p=[probability, 1-probability])\n",
    "    else:\n",
    "        new_label = np.random.choice([0,1], p=[1-probability, probability])\n",
    "\n",
    "    return new_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07756821",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9daf5801",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, num_features, weight_decay, device):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "\n",
    "        self.wd = torch.FloatTensor([weight_decay]).to(device)\n",
    "        self.w = torch.nn.Parameter(torch.zeros([num_features], requires_grad=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = torch.matmul(x, torch.reshape(self.w, [-1, 1]))\n",
    "\n",
    "        return logits\n",
    "    \n",
    "    def loss(self, logits, y):\n",
    "        preds = torch.sigmoid(logits)\n",
    "        loss = -torch.mean(y * log_clip(preds) + (1 - y) * log_clip(1 - preds))\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5c1a9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_clip(x):\n",
    "    return torch.log(torch.clamp(x, 1e-10, None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3c7499",
   "metadata": {},
   "source": [
    "### Influence Calculation Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cc60cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_influence_single(model, epsilon, train_data, test_data, device, rec_depth, r, damp, scale):\n",
    "    \n",
    "        x_test, y_test = test_data[1], test_data[2]\n",
    "        x_test = test_data[0].collate_fn([x_test])\n",
    "        y_test = test_data[0].collate_fn([y_test])\n",
    "        \n",
    "        s_test_vec = s_test_sample(model, train_data, [x_test, y_test], device, rec_depth, r, damp, scale)\n",
    "\n",
    "        # Calculate the influence function\n",
    "        train_dataset_size = len(train_data[0].dataset)\n",
    "        y_perts = []\n",
    "\n",
    "        for i, y_ in enumerate(train_data[2]):\n",
    "            y_pert = randomize_resp(y_, epsilon)\n",
    "            y_perts.append(y_pert)\n",
    "\n",
    "        time_a = datetime.datetime.now()\n",
    "\n",
    "        grad_z_vec = grad_training([train_data[1], train_data[2]], y_perts, model, device)\n",
    "\n",
    "        time_b = datetime.datetime.now()\n",
    "        time_delta = time_b - time_a\n",
    "        logging.info(f\"Time for grad_z iter:\" f\" {time_delta.total_seconds() * 1000}\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            influence = sum([torch.sum(k * j).data for k, j in zip(grad_z_vec, s_test_vec)])\n",
    "\n",
    "        return influence.cpu(), y_perts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e371a36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def s_test_sample(model, train_data, test_data, device, rec_depth, r, damp, scale):\n",
    "\n",
    "    inverse_hvp = [torch.zeros_like(params, dtype=torch.float) for params in model.parameters()]\n",
    "\n",
    "    for i in range(r):\n",
    "\n",
    "        hessian_loader = DataLoader(train_data[0].dataset, sampler=torch.utils.data.RandomSampler(train_data[0].dataset, True, num_samples=rec_depth),batch_size=1,num_workers=4,)\n",
    "\n",
    "        cur_estimate = s_test(test_data, model, i, hessian_loader, device, damp, scale)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inverse_hvp = [old + (cur / scale) for old, cur in zip(inverse_hvp, cur_estimate)]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inverse_hvp = [component / r for component in inverse_hvp]\n",
    "\n",
    "    return inverse_hvp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f2e41c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_z(test_data, model, device):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    test_data_features = test_data[0]\n",
    "    test_data_labels = test_data[1]\n",
    "\n",
    "    logits = model(test_data_features)\n",
    "    prediction = torch.sigmoid(logits)\n",
    "    loss = -torch.mean(test_data_labels * log_clip(prediction) + (1 - test_data_labels) * log_clip(1 - prediction))\n",
    "    \n",
    "    return grad(loss, model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38ac6b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_training(train_data, y_perts, model, device):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    train_data_features = torch.FloatTensor(train_data[0]).to(device)\n",
    "    train_data_labels = torch.FloatTensor(train_data[1]).to(device)\n",
    "    train_pert_data_labels = torch.FloatTensor(y_perts).to(device)\n",
    "\n",
    "    logits = model(train_data_features)\n",
    "    prediction = torch.sigmoid(logits)\n",
    "    orig_loss = -torch.mean(train_data_labels * log_clip(prediction) + (1 - train_data_labels) * log_clip(1 - prediction))\n",
    "    pert_loss = -torch.mean(train_pert_data_labels * log_clip(prediction) + (1 - train_pert_data_labels) * log_clip(1 - prediction))\n",
    "    \n",
    "    loss = (pert_loss -  orig_loss)\n",
    "    \n",
    "    return grad(loss, model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "956886b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def s_test(test_data, model, i, hessian_loader, device, damp, scale):\n",
    "\n",
    "    v = grad_z(test_data, model, device)\n",
    "    print(v)\n",
    "    \n",
    "    h_estimate = v\n",
    "\n",
    "    params, names = make_functional(model)\n",
    "    params = tuple(p.detach().requires_grad_() for p in params)\n",
    "\n",
    "    progress_bar = tqdm(hessian_loader, desc=f\"IHVP sample {i}\")\n",
    "    for i, (x_train, y_train) in enumerate(progress_bar):\n",
    "\n",
    "        x_train, y_train = x_train.type(torch.FloatTensor).to(device), y_train.type(torch.FloatTensor).to(device)\n",
    "\n",
    "        def f(*new_params):\n",
    "            load_weights(model, names, new_params)\n",
    "            out = model(x_train)\n",
    "            loss = calc_loss(out, y_train)\n",
    "            return loss\n",
    "\n",
    "        hv = vhp(f, params, tuple(h_estimate), strict=True)[1]\n",
    "\n",
    "        # Recursively calculate h_estimate\n",
    "        \n",
    "        # Influence = grad(x_test)^T*(H_1 + lambda*I)*\\sum grad(x_train)\n",
    "        with torch.no_grad():\n",
    "            h_estimate = [_v + (1 - damp) * _h_e - _hv / scale for _v, _h_e, _hv in zip(v, h_estimate, hv)]\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                norm = sum([h_.norm() for h_ in h_estimate])\n",
    "                progress_bar.set_postfix({\"est_norm\": norm.item()})\n",
    "\n",
    "    with torch.no_grad():\n",
    "        load_weights(model, names, params, as_params=True)\n",
    "\n",
    "    return h_estimate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b8e23c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(logits, labels):\n",
    "    preds = torch.sigmoid(logits)\n",
    "    loss = -torch.mean(labels * log_clip(preds) + (1 - labels) * log_clip(1 - preds))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1413935",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_functional(model):\n",
    "    orig_params = tuple(model.parameters())\n",
    "\n",
    "    names = []\n",
    "\n",
    "    for name, p in list(model.named_parameters()):\n",
    "        del_attr(model, name.split(\".\"))\n",
    "        names.append(name)\n",
    "\n",
    "    return orig_params, names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0232f0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_attr(obj, names):\n",
    "    if len(names) == 1:\n",
    "        delattr(obj, names[0])\n",
    "    else:\n",
    "        del_attr(getattr(obj, names[0]), names[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a98806be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_attr(obj, names, val):\n",
    "    if len(names) == 1:\n",
    "        setattr(obj, names[0], val)\n",
    "    else:\n",
    "        set_attr(getattr(obj, names[0]), names[1:], val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37c996a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights(model, names, params, as_params=False):\n",
    "    for name, p in zip(names, params):\n",
    "        if not as_params:\n",
    "            set_attr(model, name.split(\".\"), p)\n",
    "        else:\n",
    "            set_attr(model, name.split(\".\"), torch.nn.Parameter(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f72f48",
   "metadata": {},
   "source": [
    "### Perform Influence Calculation and LOO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe081fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestLeaveOneOut():\n",
    "    def test_leave_one_out(self, epsilon, weight_decay, rec_depth, r, scale, damp):\n",
    "\n",
    "        num_features, train_data, test_data = get_data_adult()\n",
    "\n",
    "        device = 'cuda:3' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "        train_sample_num = len(train_data[1])\n",
    "\n",
    "        train_dataset = CreateData(train_data[0], train_data[1])\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "        # prepare sklearn model to train w\n",
    "        C = 1.0 / (train_sample_num * weight_decay)\n",
    "        sklearn_model = linear_model.LogisticRegression(C=C, solver='lbfgs', tol=1e-8, fit_intercept=False)\n",
    "\n",
    "        # prepare pytorch model to compute influence function\n",
    "        torch_model = LogisticRegression(num_features, weight_decay, device)\n",
    "\n",
    "        # train\n",
    "        sklearn_model.fit(train_data[0], train_data[1])\n",
    "        pred_logr = sklearn_model.predict(test_data[0])\n",
    "        score = accuracy_score(test_data[1], pred_logr)\n",
    "        print(f'lbfgs training took {sklearn_model.n_iter_} iter. Accuracy: {score:0.3f}'  )\n",
    "\n",
    "        # assign W into pytorch model\n",
    "        w_opt = sklearn_model.coef_.ravel()\n",
    "        with torch.no_grad():\n",
    "            torch_model.w = torch.nn.Parameter(\n",
    "                torch.tensor(w_opt, dtype=torch.float)\n",
    "            )\n",
    "        torch_model = torch_model.to(device)\n",
    "        \n",
    "        # calculate original loss\n",
    "        x_test_input = torch.FloatTensor(test_data[0][12345]).to(device)\n",
    "        y_test_input = torch.LongTensor(test_data[1])[12345].to(device)\n",
    "\n",
    "        test_dataset = CreateData(test_data[0][12345], test_data[1][12345])\n",
    "        test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "      \n",
    "        test_loss_ori = torch_model.loss(torch_model(x_test_input), y_test_input).detach().cpu().numpy()\n",
    "\n",
    "        loss_diff_approx, train_data_perts = calc_influence_single(torch_model, epsilon, [train_loader, train_data[0], train_data[1]], [test_loader, x_test_input, y_test_input], device, rec_depth, r, damp, scale)\n",
    "        \n",
    "        loss_diff_approx = - torch.FloatTensor(loss_diff_approx).cpu().numpy()\n",
    "\n",
    "        # retrain\n",
    "        sklearn_model_pert = linear_model.LogisticRegression(C=C, fit_intercept=False, tol=1e-8, solver='lbfgs')\n",
    "        sklearn_model_pert.fit(train_data[0], train_data_perts)\n",
    "        \n",
    "        pred_logr = sklearn_model_pert.predict(test_data[0])\n",
    "        score = accuracy_score(test_data[1], pred_logr)\n",
    "        print(f'Perturbation lbfgs training took {sklearn_model_pert.n_iter_} iter. Accuracy: {score:0.3f}'  )\n",
    "       \n",
    "        w_retrain = sklearn_model_pert.coef_.T.ravel()\n",
    "        with torch.no_grad():\n",
    "            torch_model.w = torch.nn.Parameter(\n",
    "                torch.tensor(w_retrain, dtype=torch.float)\n",
    "            )\n",
    "\n",
    "        torch_model = torch_model.to(device)\n",
    "\n",
    "        # get retrain loss\n",
    "        test_loss_retrain = torch_model.loss(torch_model(x_test_input), y_test_input).detach().cpu().numpy()\n",
    "\n",
    "        # get true loss diff\n",
    "        loss_diff_true = test_loss_retrain - test_loss_ori\n",
    "        \n",
    "        est_loss_diff = loss_diff_approx\n",
    "        avg_loss_diff = loss_diff_true\n",
    "     \n",
    "        print('Real avg. loss diff: ', avg_loss_diff, 'Est. avg. loss diff: ', est_loss_diff)\n",
    "        \n",
    "        return avg_loss_diff, est_loss_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd5b9aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_result(actual_loss_diff, estimated_loss_diff):\n",
    "    r2_s = r2_score(actual_loss_diff, estimated_loss_diff)\n",
    "\n",
    "    max_abs = np.max([np.abs(actual_loss_diff), np.abs(estimated_loss_diff)])\n",
    "    min_, max_ = -max_abs * 1.1, max_abs * 1.1\n",
    "    plt.rcParams['figure.figsize'] = 6, 5\n",
    "    plt.scatter(actual_loss_diff, estimated_loss_diff, zorder=2, s=10)\n",
    "    plt.title('Loss diff Pert.')\n",
    "    plt.xlabel('Actual loss diff')\n",
    "    plt.ylabel('Estimated loss diff')\n",
    "    range_ = [min_, max_]\n",
    "    plt.plot(range_, range_, 'k-', alpha=0.2, zorder=1)\n",
    "    text = 'MAE = {:.03}\\nR2 score = {:.03}'.format(mean_absolute_error(actual_loss_diff, estimated_loss_diff),\n",
    "                                                    r2_s)\n",
    "    plt.text(max_abs, -max_abs, text, verticalalignment='bottom', horizontalalignment='right')\n",
    "    plt.xlim(min_, max_)\n",
    "    plt.ylim(min_, max_)\n",
    "\n",
    "    plt.savefig(\"result.png\")\n",
    "\n",
    "    return r2_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "391d9b94",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on epsilon:  0.001\n",
      "lbfgs training took [21] iter. Accuracy: 0.868\n",
      "(tensor([0.0000, 0.0000, 0.0606, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0606, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0606, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0606, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0606, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0606, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0606,\n",
      "        0.0000, 0.0000, 0.0000, 0.0606, 0.0000, 0.0000], device='cuda:3'),)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IHVP sample 0:  19%|█████████                                       | 1009/5350 [00:03<00:16, 267.28it/s, est_norm=11.5]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m epsilons:\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWorking on epsilon: \u001b[39m\u001b[38;5;124m'\u001b[39m, e)\n\u001b[0;32m---> 20\u001b[0m     avg_loss_diff, est_loss_diff \u001b[38;5;241m=\u001b[39m \u001b[43mLOO\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_leave_one_out\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrec_depth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdamp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     avg_losses\u001b[38;5;241m.\u001b[39mappend(avg_loss_diff)\n\u001b[1;32m     22\u001b[0m     est_losses\u001b[38;5;241m.\u001b[39mappend(est_loss_diff)\n",
      "Cell \u001b[0;32mIn[17], line 43\u001b[0m, in \u001b[0;36mTestLeaveOneOut.test_leave_one_out\u001b[0;34m(self, epsilon, weight_decay, rec_depth, r, scale, damp)\u001b[0m\n\u001b[1;32m     39\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(test_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     41\u001b[0m test_loss_ori \u001b[38;5;241m=\u001b[39m torch_model\u001b[38;5;241m.\u001b[39mloss(torch_model(x_test_input), y_test_input)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m---> 43\u001b[0m loss_diff_approx, train_data_perts \u001b[38;5;241m=\u001b[39m \u001b[43mcalc_influence_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_test_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_input\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrec_depth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdamp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m loss_diff_approx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(loss_diff_approx)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# retrain\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m, in \u001b[0;36mcalc_influence_single\u001b[0;34m(model, epsilon, train_data, test_data, device, rec_depth, r, damp, scale)\u001b[0m\n\u001b[1;32m      4\u001b[0m x_test \u001b[38;5;241m=\u001b[39m test_data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcollate_fn([x_test])\n\u001b[1;32m      5\u001b[0m y_test \u001b[38;5;241m=\u001b[39m test_data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcollate_fn([y_test])\n\u001b[0;32m----> 7\u001b[0m s_test_vec \u001b[38;5;241m=\u001b[39m \u001b[43ms_test_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrec_depth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdamp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Calculate the influence function\u001b[39;00m\n\u001b[1;32m     10\u001b[0m train_dataset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdataset)\n",
      "Cell \u001b[0;32mIn[8], line 9\u001b[0m, in \u001b[0;36ms_test_sample\u001b[0;34m(model, train_data, test_data, device, rec_depth, r, damp, scale)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(r):\n\u001b[1;32m      7\u001b[0m     hessian_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdataset, sampler\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mRandomSampler(train_data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;28;01mTrue\u001b[39;00m, num_samples\u001b[38;5;241m=\u001b[39mrec_depth),batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,)\n\u001b[0;32m----> 9\u001b[0m     cur_estimate \u001b[38;5;241m=\u001b[39m \u001b[43ms_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhessian_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdamp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     12\u001b[0m         inverse_hvp \u001b[38;5;241m=\u001b[39m [old \u001b[38;5;241m+\u001b[39m (cur \u001b[38;5;241m/\u001b[39m scale) \u001b[38;5;28;01mfor\u001b[39;00m old, cur \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(inverse_hvp, cur_estimate)]\n",
      "Cell \u001b[0;32mIn[11], line 14\u001b[0m, in \u001b[0;36ms_test\u001b[0;34m(test_data, model, i, hessian_loader, device, damp, scale)\u001b[0m\n\u001b[1;32m     11\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m tqdm(hessian_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIHVP sample \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (x_train, y_train) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(progress_bar):\n\u001b[0;32m---> 14\u001b[0m     x_train, y_train \u001b[38;5;241m=\u001b[39m \u001b[43mx_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFloatTensor\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device), y_train\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mFloatTensor)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf\u001b[39m(\u001b[38;5;241m*\u001b[39mnew_params):\n\u001b[1;32m     17\u001b[0m         load_weights(model, names, new_params)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epsilons = [.001, .005, .01, .05, .1, .15, .2, .25, .3, .35, .4, .45, .5, .55, .6, .65, .7, .75, .8, .85, .9, .95, 1, 5, 10]\n",
    "\n",
    "all_avg = []\n",
    "all_est = []\n",
    "\n",
    "weight_decay = 0.01\n",
    "rec_depth = 5350\n",
    "r = 5\n",
    "scale = 10\n",
    "damp = 0.01 \n",
    "rounds = 5\n",
    "\n",
    "LOO = TestLeaveOneOut()\n",
    "\n",
    "for ro in range(rounds):\n",
    "    avg_losses = []\n",
    "    est_losses = []\n",
    "    for e in epsilons:\n",
    "        print('Working on epsilon: ', e)\n",
    "        avg_loss_diff, est_loss_diff = LOO.test_leave_one_out(e, weight_decay, rec_depth, r, scale, damp)\n",
    "        avg_losses.append(avg_loss_diff)\n",
    "        est_losses.append(est_loss_diff)\n",
    "    all_avg.append(avg_losses)\n",
    "    all_est.append(est_losses)\n",
    "    \n",
    "final_real = [0 for x in range(len(epsilons))]\n",
    "final_est = [0 for x in range(len(epsilons))]\n",
    "\n",
    "for i in range(len(final_real)):\n",
    "    for j in range(len(all_avg)):\n",
    "        final_real[i] = final_real[i] + all_avg[j][i]\n",
    "        final_est[i] = final_est[i] + all_est[j][i]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060094e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph([final_real, final_est, epsilons], 'Actual vs. Estimated Loss per Epsilon', 'epsilon', 'Average Loss', ['Actual', 'Estimated'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752355d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_s = visualize_result(final_real, final_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cc7497",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d815add",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
