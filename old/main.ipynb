{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b700702",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import copy\n",
    "import torch\n",
    "import random\n",
    "import logging\n",
    "import warnings\n",
    "import datetime\n",
    "import torch.optim as opt\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from torch.autograd import grad, Variable\n",
    "from torch.utils.data import Subset\n",
    "from data_processing import get_data_adult\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "device = f\"cuda:5\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d363fd6c",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db9fd419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph(data, acc_or_l, train_or_test, lr, batch_size):\n",
    "    sns.set(font_scale=1)\n",
    "\n",
    "    plt.plot(data, 'b-', linewidth=2.0)\n",
    "\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(acc_or_l)\n",
    "    plt.title(train_or_test + ' ' + acc_or_l + ' lr: ' + str(lr) + ' batch: ' + str(batch_size))\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56ef5437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random():\n",
    "    random.seed(0)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(0)\n",
    "    np.random.seed(0)\n",
    "    torch.manual_seed(0)\n",
    "    torch.cuda.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "685b6575",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_sample_ids_per_class(class_id, num_samples, test_loader, start_index):\n",
    "    sample_list = []\n",
    "    data_count = 0\n",
    "    \n",
    "    for i in range(len(test_loader.dataset)):\n",
    "        _, y = test_loader.dataset[i]\n",
    "        if class_id == y:\n",
    "            data_count += 1\n",
    "            if start_index < data_count <= start_index + num_samples:\n",
    "                sample_list.append(i)\n",
    "            elif data_count > start_index + num_samples:\n",
    "                break\n",
    "    \n",
    "    return sample_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc9bd266",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_sample_ids(num_samples, test_loader, num_classes, start_index):\n",
    "    sample_dict = {}\n",
    "    sample_list = []\n",
    "    \n",
    "    for i in range(num_classes):\n",
    "        sample_dict[str(i)] = get_dataset_sample_ids_per_class(i, num_samples, test_loader, start_index)\n",
    "        sample_list[len(sample_list):len(sample_list)] = sample_dict[str(i)]\n",
    "\n",
    "    return sample_dict, sample_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06abef55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_progress(text, current_step, last_step, enabled=True, fix_zero_start=True):\n",
    "    if fix_zero_start:\n",
    "        current_step += 1\n",
    "        \n",
    "    term_line_len = 80\n",
    "    final_chars = [':', ';', ',','.']\n",
    "    \n",
    "    if text[-1:] not in final_chars:\n",
    "        text = text + ' '\n",
    "    \n",
    "    if len(text) < term_line_len:\n",
    "        bar_len = term_line_len - (len(text) + len(str(current_step)) + len(str(last_step)) + len(\" / \"))\n",
    "    else:\n",
    "        bar_len = 30\n",
    "\n",
    "    filled_len = int(round(bar_len * current_step / float(last_step)))\n",
    "    bar = '=' * filled_len + \".\" * (bar_len - filled_len)\n",
    "\n",
    "    bar = f\"{text}[{bar:s}] {current_step:d} / {last_step:d}\"\n",
    "\n",
    "    if current_step < last_step-1:\n",
    "        sys.stdout.write(\"\\033[K\" + bar + \"\\r\")\n",
    "    else:\n",
    "        sys.stdout.write(bar + \"\\n\")\n",
    "\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09542d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(json_obj, json_path, append_if_exists=False, overwrite_if_exists=True, unique_fn_if_exists=True):\n",
    "    if isinstance(json_path, str):\n",
    "        json_path = Path(json_path)\n",
    "\n",
    "    if overwrite_if_exists:\n",
    "        append_if_exists = False\n",
    "        unique_fn_if_exists = False\n",
    "\n",
    "    if unique_fn_if_exists:\n",
    "        overwrite_if_exists = False\n",
    "        append_if_exists = False\n",
    "        if json_path.exists():\n",
    "            time = dt.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "            json_path = json_path.parents[0] / f'{str(json_path.stem)}_{time}'\\\n",
    "                                               f'{str(json_path.suffix)}'\n",
    "\n",
    "    if overwrite_if_exists:\n",
    "        append_if_exists = False\n",
    "        with open(json_path, 'w+') as fout:\n",
    "            json.dump(json_obj, fout, indent=2)\n",
    "        return\n",
    "\n",
    "    if append_if_exists:\n",
    "        if json_path.exists():\n",
    "            with open(json_path, 'r') as fin:\n",
    "                read_file = json.load(fin)\n",
    "            read_file.update(json_obj)\n",
    "            with open(json_path, 'w+') as fout:\n",
    "                json.dump(read_file, fout, indent=2)\n",
    "            return\n",
    "\n",
    "    with open(json_path, 'w+') as fout:\n",
    "        print('hit2')\n",
    "        json.dump(json_obj, fout, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e51bccf",
   "metadata": {},
   "source": [
    "### Influence Calculation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7617c01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_point_wise(model, train_loader, test_loader, incorrect_indicies):\n",
    "    test_sample_num = 1\n",
    "    test_start_index = 0\n",
    "    recursion_depth = 2623 \n",
    "    r_averaging = 2 \n",
    "    \n",
    "    outdir = Path('output')\n",
    "    outdir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    influences = {}\n",
    "    \n",
    "    # Calculate the influence function on test sample per iteration\n",
    "    for j in (incorrect_indicies):\n",
    "        i = j\n",
    "        \n",
    "        start_time = time.time()\n",
    "     \n",
    "        influence, harmful, helpful, _ = calc_influence_single(model, train_loader, test_loader, i, recursion_depth, r_averaging)\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        influences[str(i)] = {}\n",
    "        _, label = test_loader.dataset[i]\n",
    "        influences[str(i)]['label'] = label.cpu().numpy().tolist()\n",
    "        influences[str(i)]['num_in_dataset'] = j\n",
    "        influences[str(i)]['time_calc_influence_s'] = end_time - start_time\n",
    "        infl = [ifl.tolist() for ifl in influence]\n",
    "        influences[str(i)]['influence'] = infl\n",
    "        influences[str(i)]['harmful'] = harmful[:500]\n",
    "        influences[str(i)]['helpful'] = helpful[:500]\n",
    "\n",
    "        print(f\"The results for this run are:\")\n",
    "        print(\"Influences: \")\n",
    "        print(influence[:5])\n",
    "        print(\"Most harmful data IDs: \")\n",
    "        print(harmful[:5])\n",
    "        print(\"Harmful values:\")\n",
    "        print(influence[harmful[0]], influence[harmful[1]])\n",
    "        print(\"Most helpful data IDs: \")\n",
    "        print(helpful[:5])\n",
    "        print(\"Helpful values: \")\n",
    "        print(influence[helpful[0]], influence[helpful[1]])\n",
    "        \n",
    "    influence_path = outdir.joinpath(f\"influence_results_{test_start_index}_{test_sample_num}.json\")\n",
    "    save_json(influences, influence_path)\n",
    "    \n",
    "    return influences\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f5ced81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_influence_single(model, train_loader, test_loader, test_id_num, recursion_depth, r):\n",
    "    x_test, y_test = test_loader.dataset[test_id_num]\n",
    "    x_test = test_loader.collate_fn([x_test])\n",
    "    y_test = test_loader.collate_fn([y_test])\n",
    "    ihvp_test_vec = calc_ihvp_single_test(model, x_test, y_test, train_loader, recursion_depth, r)\n",
    "    print('Done calculating ihvp') # this gets the ihvp for one test point\n",
    "    \n",
    "    # calculate the influence function\n",
    "    train_dataset_size = len(train_loader.dataset)\n",
    "    influences = []\n",
    "    \n",
    "    for i in range(train_dataset_size):\n",
    "        x, y = train_loader.dataset[i]\n",
    "        x = train_loader.collate_fn([x])\n",
    "        y = train_loader.collate_fn([y])\n",
    "        \n",
    "        grad_x_vec = grad_x(x, y, model) # get grad of training point\n",
    "        \n",
    "        tmp_influence = -sum([torch.sum(k*j).data for k, j in zip(grad_x_vec, ihvp_test_vec)]) / train_dataset_size\n",
    "        \n",
    "        influences.append(tmp_influence.cpu().numpy())\n",
    "        \n",
    "        display_progress(\"Calc. influence function: \", i, train_dataset_size)\n",
    "        \n",
    "    harmful = np.argsort(influences)\n",
    "    helpful = harmful[::-1]\n",
    "    \n",
    "    return influences, harmful.tolist(), helpful.tolist(), test_id_num\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e27b48c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ihvp_single_test(model, x_test, y_test, train_loader, recursion_depth, r):\n",
    "    damp = 0\n",
    "    scale = 500\n",
    "   \n",
    "    ihvp_test_vec_list = []\n",
    "   \n",
    "    print('x_test len', len(x_test))\n",
    "\n",
    "    avg_ihvp_test_vec = ihvp_test(x_test, y_test, model, train_loader, damp, scale, recursion_depth)\n",
    "                      \n",
    "    for i in range(1,r):\n",
    "        ith_ihvp_test_vec = ihvp_test(x_test, y_test, model, train_loader, damp, scale, recursion_depth)\n",
    "        avg_ihvp_test_vec = [avg + ith for avg, ith in zip(avg_ihvp_test_vec, ith_ihvp_test_vec)]        \n",
    "        display_progress(\"Averaging r-times: \", i, r)\n",
    "       \n",
    "    avg_ihvp_test_vec = [i / r for i in avg_ihvp_test_vec]\n",
    "       \n",
    "    return avg_ihvp_test_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f74a36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#done\n",
    "def ihvp_test(x_test, y_test, model, train_loader, damp, scale, recursion_depth):\n",
    "    v = grad_x(x_test, y_test, model) # get grad of test point\n",
    "    h_estimate = v.copy()  # h_estimate is a copy of the test pnt grad\n",
    "    \n",
    "    criterion = torch.nn.BCELoss()\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    \n",
    "    for i in range(recursion_depth):\n",
    "        x, y = next(iter(train_loader))\n",
    "        x, y = x.type(torch.FloatTensor).to(device), y.type(torch.FloatTensor).to(device)\n",
    "        \n",
    "        pred = model(x)\n",
    "        loss = criterion(torch.squeeze(pred), y)\n",
    "        \n",
    "        hv = hvp(loss, params, h_estimate) # Hessian vector product\n",
    "    \n",
    "        h_estimate = [v_ + (1-damp) * h_e_ - hv_ / scale for v_, h_e_, hv_ in zip(v, h_estimate, hv)]\n",
    "        \n",
    "    return h_estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5fc641b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_x(x, y, model):\n",
    "    model.eval()\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    \n",
    "    x, y = x.type(torch.FloatTensor).to(device), y.type(torch.FloatTensor).to(device)\n",
    "    \n",
    "    pred = model(x)\n",
    "    loss = criterion(pred.ravel(), y)\n",
    "    \n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    grad_x_ = list(grad(loss, params, create_graph=True))\n",
    "    \n",
    "    return grad_x_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40910e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hvp(loss, w, v):\n",
    "    first_grads = grad(loss, w, create_graph=True)#, retain_graph=True, create_graph=True)\n",
    "    elementwise_products = [torch.mul(grad_elem, v_elem.detach()) for grad_elem, v_elem in zip(first_grads, v) if grad_elem is not torch.isnan(grad_elem)]\n",
    "    loss.backward(retain_graph=True, create_graph=True)\n",
    "    \n",
    "    print(elementwise_products[0].grad)\n",
    "    #elem_prod_v = Variable(torch.FloatTensor(elementwise_products), requires_grad=True)\n",
    "    \n",
    "    grads_with_none = grad(elementwise_products, w, grad_outputs=[x for x in elementwise_products[0].grad])\n",
    "    \n",
    "    return_grads = [grad_element if grad_element is not torch.isnan(grad_element) else torch.zeros_like(x) for x, grad_element in zip(w, grads_with_none)]\n",
    "    \n",
    "    return grads_with_none "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1d8250b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hvp_old(loss, w, v):\n",
    "    first_grads = grad(loss, w, retain_graph=True, create_graph=True)\n",
    "    #loss.backward(retain_graph=True, create_graph=True)\n",
    "    \n",
    "    elementwise_products = torch.zeros(1).to(device)\n",
    "    \n",
    "    for grad_elem, v_elem in zip(first_grads, v):\n",
    "        elementwise_products += torch.sum(grad_elem * v_elem.detach())\n",
    "        \n",
    "    elementwise_products.backward(create_graph=True)\n",
    "    #torch.nn.utils.clip_grad_norm_(w, max_norm=500.0, norm_type=2)\n",
    "    return_grads = [ww.grad for ww in w]\n",
    "    \n",
    "    return return_grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07756821",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e614aafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogReg(torch.nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(LogReg, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.fc1 = nn.Linear(self.input_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        prediction = torch.sigmoid(self.fc1(x))\n",
    "        \n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9d6f13",
   "metadata": {},
   "source": [
    "### Training and Testing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12d8427c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, test_dataloader, optimizer, criterion, P, device, save=False):\n",
    "    all_acc, all_loss = [], []\n",
    "    correct_indx = None\n",
    "    \n",
    "    for i in range(30):\n",
    "        model.train()\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            data, target = data.type(torch.FloatTensor).to(device), target.type(torch.FloatTensor).to(device)\n",
    "            pred = model(data)\n",
    "            loss = criterion(torch.squeeze(pred), target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        if save:\n",
    "            acc, lo, correct_indx = evaluate(model, test_dataloader, i)\n",
    "            all_acc.append(acc)\n",
    "            all_loss.append(lo)\n",
    "            \n",
    "    if save:\n",
    "        torch.save(model.state_dict(), 'models/logreg.pt')\n",
    "        \n",
    "    return all_acc, all_loss, correct_indx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ba4a2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_dataloader, i):\n",
    "    model.eval()\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    running_correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0\n",
    "    iters = 0\n",
    "    \n",
    "    correct_indx = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(test_dataloader):\n",
    "            data, target = data.type(torch.FloatTensor).to(device), target.type(torch.FloatTensor).to(device)\n",
    "            pred = model(data)\n",
    "            running_loss += criterion(torch.squeeze(pred), target)\n",
    "            pred_thresh = torch.squeeze(pred).round().detach().cpu().numpy()\n",
    "            running_correct += np.sum(pred_thresh == target.detach().cpu().numpy())\n",
    "            \n",
    "            correct_indx.extend(pred_thresh == target.detach().cpu().numpy())\n",
    "            \n",
    "            total += len(target)\n",
    "            iters = batch_idx\n",
    "\n",
    "    acc = running_correct / total\n",
    "    loss = (running_loss / iters).cpu()\n",
    "\n",
    "    print(f'Epoch: {i} | Acc: {acc:.3f} | Loss: {loss:.3f}')\n",
    "\n",
    "    return acc, loss, correct_indx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "25461e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_losses(model, test_dataloader, test_id_nums):\n",
    "    model.eval()\n",
    "    criterion = torch.nn.BCELoss()\n",
    "\n",
    "    actual_losses = []\n",
    "\n",
    "    for i in test_id_nums:\n",
    "        x_test, y_test = test_dataloader.dataset[i]\n",
    "        x_test = test_dataloader.collate_fn([x_test])\n",
    "\n",
    "        y_test = test_dataloader.collate_fn([y_test])\n",
    "        x_test, y_test = x_test.type(torch.FloatTensor).to(device), y_test.type(torch.FloatTensor).to(device)\n",
    "        outputs = model(x_test)\n",
    "        actual_losses.append(criterion(outputs, y_test.unsqueeze(1)).item())\n",
    "\n",
    "    return actual_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43608c00",
   "metadata": {},
   "source": [
    "### Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bbe8b652",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, test_dataloader, num_features, train_dataset, probability = get_data_adult(batch_size=256, randomize='false', epsilon=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce5d4d4",
   "metadata": {},
   "source": [
    "### Train base model and get Original Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f760306",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Acc: 0.773 | Loss: 0.548\n",
      "Epoch: 1 | Acc: 0.777 | Loss: 0.478\n",
      "Epoch: 2 | Acc: 0.792 | Loss: 0.452\n",
      "Epoch: 3 | Acc: 0.808 | Loss: 0.440\n",
      "Epoch: 4 | Acc: 0.809 | Loss: 0.425\n",
      "Epoch: 5 | Acc: 0.806 | Loss: 0.417\n",
      "Epoch: 6 | Acc: 0.818 | Loss: 0.411\n",
      "Epoch: 7 | Acc: 0.813 | Loss: 0.405\n",
      "Epoch: 8 | Acc: 0.823 | Loss: 0.403\n",
      "Epoch: 9 | Acc: 0.820 | Loss: 0.398\n",
      "Epoch: 10 | Acc: 0.824 | Loss: 0.401\n",
      "Epoch: 11 | Acc: 0.825 | Loss: 0.396\n",
      "Epoch: 12 | Acc: 0.824 | Loss: 0.391\n",
      "Epoch: 13 | Acc: 0.825 | Loss: 0.390\n",
      "Epoch: 14 | Acc: 0.817 | Loss: 0.390\n",
      "Epoch: 15 | Acc: 0.824 | Loss: 0.394\n",
      "Epoch: 16 | Acc: 0.827 | Loss: 0.384\n",
      "Epoch: 17 | Acc: 0.826 | Loss: 0.382\n",
      "Epoch: 18 | Acc: 0.824 | Loss: 0.382\n",
      "Epoch: 19 | Acc: 0.826 | Loss: 0.380\n",
      "Epoch: 20 | Acc: 0.827 | Loss: 0.379\n",
      "Epoch: 21 | Acc: 0.828 | Loss: 0.381\n",
      "Epoch: 22 | Acc: 0.827 | Loss: 0.377\n",
      "Epoch: 23 | Acc: 0.828 | Loss: 0.376\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m criterion \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mBCELoss()\n\u001b[1;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 10\u001b[0m all_acc, all_loss, correct_indx \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 8\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_dataloader, test_dataloader, optimizer, criterion, P, device, save)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m30\u001b[39m):\n\u001b[1;32m      6\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (data, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m      9\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     11\u001b[0m         data, target \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mFloatTensor)\u001b[38;5;241m.\u001b[39mto(device), target\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mFloatTensor)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.conda/envs/influence/lib/python3.9/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.conda/envs/influence/lib/python3.9/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.conda/envs/influence/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.conda/envs/influence/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/influence/data_processing.py:25\u001b[0m, in \u001b[0;36mTabularData.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m---> 25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx[idx], \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "set_random()\n",
    "\n",
    "model = LogReg(input_size=num_features)\n",
    "torch.save(model.state_dict(), 'models/initial_config.pt')\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=.001, weight_decay=.0001, momentum=.9, nesterov=False)\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "model.to(device)\n",
    "all_acc, all_loss, correct_indx = train(model, train_dataloader, test_dataloader, optimizer, criterion, None, device, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df197c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "incorrect_indicies = [i for i, a in enumerate(correct_indx) if a == False][:2]\n",
    "\n",
    "actual_losses = get_test_losses(model, test_dataloader, incorrect_indicies)\n",
    "\n",
    "print('Original losses: ')\n",
    "count = 0\n",
    "for i in incorrect_indicies:\n",
    "    print(f'Test Sample {i}: {actual_losses[count]}')\n",
    "    count += 1\n",
    "    \n",
    "graph(all_loss, 'Loss', 'Testing', .001, 256)\n",
    "graph(all_acc, 'Accuracy', 'Testing', .001, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58521e41",
   "metadata": {},
   "source": [
    "### Perform Influence Calculation on Test Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03e7961",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "influences = calc_point_wise(model, train_dataloader, test_dataloader, incorrect_indicies)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cb873d55",
   "metadata": {},
   "source": [
    "### Leave One Out Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb7964f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loo_losses = {}\n",
    "\n",
    "for i, s in enumerate(incorrect_indicies):\n",
    "    indicies_to_pop = influences[str(s)]['harmful'][:50]\n",
    "    print(f'\\n-- LOO for Test Sample {s}')\n",
    "    key = f'sample_{s}'\n",
    "    loo_losses[key] = []\n",
    "\n",
    "    for j in range(len(indicies_to_pop)):\n",
    "        temp_indicies = [x for x in range(len(train_dataset))]\n",
    "        temp_pop = indicies_to_pop.copy()\n",
    "        index = temp_pop.pop(j)\n",
    "        temp_indicies.pop(index)\n",
    "        train_minus_one = Subset(train_dataset, temp_indicies)\n",
    "\n",
    "        random.seed(0)\n",
    "        os.environ[\"PYTHONHASHSEED\"] = str(0)\n",
    "        np.random.seed(0)\n",
    "        torch.manual_seed(0)\n",
    "        torch.cuda.manual_seed(0)\n",
    "\n",
    "        train_dataloader = torch.utils.data.DataLoader(train_minus_one, batch_size=256, shuffle=False,\n",
    "                                                       pin_memory=True, num_workers=0)\n",
    "        model = LogReg(input_size=num_features)\n",
    "        model.load_state_dict(torch.load('models/initial_config.pt'))\n",
    "\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=.001, weight_decay=.0001, momentum=.9, nesterov=False)\n",
    "        criterion = torch.nn.BCELoss()\n",
    "        model.to(device)\n",
    "\n",
    "        _, _, _ = train(model, train_dataloader, test_dataloader, optimizer, criterion, None, device, save=False)\n",
    "\n",
    "        loo_loss = get_test_losses(model, test_dataloader, [s])\n",
    "        loo_losses[key].append(loo_loss[0] - actual_losses[i]) # differences\n",
    "\n",
    "        print('-- data item: ', j, ' | ', loo_losses[key][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c7bcc4",
   "metadata": {},
   "source": [
    "### Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7612e477",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i, s in enumerate(incorrect_indicies):\n",
    "    sns.set(font_scale=1)\n",
    "    plt.scatter(loo_losses[f'sample_{s}'], [influences[str(s)]['influence'][v] for v in influences[str(s)]['harmful'][:50]])\n",
    "    z = np.polyfit(loo_losses[f'sample_{s}'], [influences[str(s)]['influence'][v] for v in influences[str(s)]['harmful'][:50]],1)\n",
    "    p = np.poly1d(z)\n",
    "    plt.plot(loo_losses[f'sample_{s}'], p(loo_losses[f'sample_{s}']))\n",
    "    plt.xlabel('Actual Loss Difference')\n",
    "    plt.ylabel('Estimated Loss Difference')\n",
    "    plt.title(f'Actual Loss vs. Estimated Loss Sample {s}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.savefig(f'sample_{i}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c687cd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
