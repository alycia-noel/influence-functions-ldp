{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3f320e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import copy\n",
    "import torch\n",
    "import pickle\n",
    "import random\n",
    "import logging\n",
    "import warnings\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import torch.optim as opt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.autograd import grad\n",
    "from torch.autograd.functional import vhp\n",
    "from get_datasets import get_diabetes, get_adult, get_law\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, accuracy_score\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "E = math.e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d88723b",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dafa202c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_k(data, ep, real_est):\n",
    "    title = f'{real_est} Loss vs. k for epsilon = {ep}'\n",
    "    sns.set(font_scale=1)\n",
    "    palette = sns.color_palette(\"colorblind\")\n",
    "    ticks = np.arange(0, len(data[-1]), step=1)\n",
    "    \n",
    "    plt.xticks(ticks=ticks, labels=data[-1], rotation='vertical')\n",
    "\n",
    "    if real_est == 'Real':\n",
    "        plt.plot([x[0] for x in data[0][0]], color=palette[0], linestyle='-', linewidth=2.0, label='G1')\n",
    "        #plt.plot([x[0] for x in data[0][1]], color=palette[1], linestyle='-', linewidth=2.0, label='G2')\n",
    "#         plt.plot([x[0] for x in data[0][2]], color=palette[2], linestyle='-', linewidth=2.0, label='G3')\n",
    "#         plt.plot([x[0] for x in data[0][3]], color=palette[3], linestyle='-', linewidth=2.0, label='G4')\n",
    "#         plt.plot([x[0] for x in data[0][4]], color=palette[4], linestyle='-', linewidth=2.0, label='G5')\n",
    "    else:\n",
    "        plt.plot([x[1] for x in data[0][0]], color=palette[0], linestyle='-', linewidth=2.0, label='G1')\n",
    "        #plt.plot([x[1] for x in data[0][1]], color=palette[1], linestyle='-', linewidth=2.0, label='G2')\n",
    "#         plt.plot([x[1] for x in data[0][2]], color=palette[2], linestyle='-', linewidth=2.0, label='G3')\n",
    "#         plt.plot([x[1] for x in data[0][3]], color=palette[3], linestyle='-', linewidth=2.0, label='G4')\n",
    "#         plt.plot([x[1] for x in data[0][4]], color=palette[4], linestyle='-', linewidth=2.0, label='G5')\n",
    "        \n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5efdfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_ep(data, k, real_est):\n",
    "    \n",
    "    title = f'{real_est} Loss vs. epsilon for k = {k}'\n",
    "    sns.set(font_scale=1)\n",
    "    palette = sns.color_palette(\"colorblind\")\n",
    "    ticks = np.arange(0, len(data[-1]), step=1)\n",
    "    plt.xticks(ticks=ticks, labels=data[-1], rotation='vertical')\n",
    "    \n",
    "    if real_est == 'Real':\n",
    "        plt.plot([x[0] for x in data[0][0]], color=palette[0], linestyle='-', linewidth=2.0, label='G1')\n",
    "        #plt.plot([x[0] for x in data[0][1]], color=palette[1], linestyle='-', linewidth=2.0, label='G2')\n",
    "#         plt.plot([x[0] for x in data[0][2]], color=palette[2], linestyle='-', linewidth=2.0, label='G3')\n",
    "#         plt.plot([x[0] for x in data[0][3]], color=palette[3], linestyle='-', linewidth=2.0, label='G4')\n",
    "#         plt.plot([x[0] for x in data[0][4]], color=palette[4], linestyle='-', linewidth=2.0, label='G5')\n",
    "    else:\n",
    "        plt.plot([x[1] for x in data[0][0]], color=palette[0], linestyle='-', linewidth=2.0, label='G1')\n",
    "        #plt.plot([x[1] for x in data[0][1]], color=palette[1], linestyle='-', linewidth=2.0, label='G2')\n",
    "#         plt.plot([x[1] for x in data[0][2]], color=palette[2], linestyle='-', linewidth=2.0, label='G3')\n",
    "#         plt.plot([x[1] for x in data[0][3]], color=palette[3], linestyle='-', linewidth=2.0, label='G4')\n",
    "#         plt.plot([x[1] for x in data[0][4]], color=palette[4], linestyle='-', linewidth=2.0, label='G5')\n",
    "     \n",
    "    plt.xlabel('epsilon')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd5b9aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_result(e_g_k, ep, title):\n",
    "    \n",
    "    # actual_loss_diff = [group 1: [k=1, k=12, ....], group 2: [k=1, k=12, ...]]\n",
    "    fig, ax = plt.subplots()\n",
    "    palette = sns.color_palette(\"colorblind\")\n",
    "    \n",
    "    actual = [[x[0] for x in e_g_k[0]]]#, [x[0] for x in e_g_k[1]]]#, [x[0] for x in e_g_k[2]], [x[0] for x in e_g_k[3]], [x[0] for x in e_g_k[4]]]\n",
    "    estimated = [[x[1] for x in e_g_k[0]]]#, [x[1] for x in e_g_k[1]]]#, [x[1] for x in e_g_k[2]], [x[1] for x in e_g_k[3]], [x[1] for x in e_g_k[4]]]\n",
    "\n",
    "    actual_all = []\n",
    "    estimated_all = []\n",
    "    spearman_all = []\n",
    "    mae_all = []\n",
    "    \n",
    "    for x in range(len(actual)):\n",
    "        actual_all.extend(actual[x])\n",
    "        estimated_all.extend(estimated[x])\n",
    "    \n",
    "    for i in range(2):\n",
    "        spearman_all.append(spearmanr(actual[i], estimated[i]).correlation)\n",
    "        mae_all.append(mean_absolute_error(actual[i], estimated[i]))\n",
    "        \n",
    "    max_abs = np.max([np.abs(actual_all), np.abs(estimated_all)])\n",
    "    min_, max_ = -max_abs * 1.1, max_abs * 1.1\n",
    "    \n",
    "    plt.rcParams['figure.figsize'] = 6, 5\n",
    "    \n",
    "    ax.scatter(actual[0], estimated[0], zorder=2, s=10, color = palette[0], label='G1')\n",
    "    #ax.scatter(actual[1], estimated[1], zorder=2, s=10, color = palette[1], label='G2')\n",
    "#     ax.scatter(actual[2], estimated[2], zorder=2, s=10, color = palette[2], label='G3')\n",
    "#     ax.scatter(actual[3], estimated[3], zorder=2, s=10, color = palette[3], label='G4')\n",
    "#     ax.scatter(actual[4], estimated[4], zorder=2, s=10, color = palette[4], label='G5')\n",
    "\n",
    "    ax.set_title(f'Actual vs. Estimated loss for {title} = {ep}')\n",
    "    ax.set_xlabel('Actual loss diff')\n",
    "    ax.set_ylabel('Estimated loss diff')\n",
    "   \n",
    "    ax.set_xlim(-(np.max(np.abs(actual_all))+.05), .1)\n",
    "    ax.set_ylim(-(np.max(np.abs(estimated_all))+.005), .01)\n",
    "    ax.plot(ax.get_xlim(), ax.get_ylim(), ls=\"-\", color=palette[7])\n",
    "    text = 'MAE = {:.03}\\nP = {:.03}'.format(sum(mae_all)/5, sum(spearman_all)/5)\n",
    "    ax.text(.1-.02, -(np.max(np.abs(estimated_all))+.005)+.001, text, verticalalignment='bottom', horizontalalignment='right')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "741afa72",
   "metadata": {},
   "outputs": [],
   "source": [
    " class CreateData(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, targets, pert_status):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.pert = pert_status\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        out_data = self.data[idx]\n",
    "        out_label = self.targets[idx]\n",
    "        pert_label = self.pert[idx]\n",
    "\n",
    "        return out_data, out_label, pert_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09d3bfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(group_method, new_train_df, feature_set, label, k):    \n",
    "    if group_method == 1:\n",
    "        # based on Race or Gender\n",
    "        selected_group = new_train_df.loc[new_train_df['sex'] == 0]\n",
    "        \n",
    "        num_to_sample = round((k / 100)*len(selected_group))\n",
    "       \n",
    "        sampled_group = selected_group.sample(n=num_to_sample)\n",
    "        not_selected = new_train_df.drop(sampled_group.index)\n",
    "        \n",
    "        selected_group_X = sampled_group[feature_set]\n",
    "        selected_group_y = sampled_group[label]\n",
    "        \n",
    "        not_selected_group_X = not_selected[feature_set]\n",
    "        not_selected_group_y = not_selected[label]   \n",
    "    elif group_method == 2:\n",
    "        # based on Race and Gender\n",
    "        selected_group = new_train_df.loc[new_train_df['sex'] == 0]\n",
    "        selected_group = selected_group.loc[selected_group['race_White'] == 0]\n",
    "        \n",
    "        num_to_sample = round((k / 100)*len(selected_group))\n",
    "       \n",
    "        sampled_group = selected_group.sample(n=num_to_sample)\n",
    "        not_selected = new_train_df.drop(sampled_group.index)\n",
    "        \n",
    "        selected_group_X = sampled_group[feature_set]\n",
    "        selected_group_y = sampled_group[label]\n",
    "        \n",
    "        not_selected_group_X = not_selected[feature_set]\n",
    "        not_selected_group_y = not_selected[label]\n",
    "    elif group_method == 3:\n",
    "        # based on Class\n",
    "        selected_group = new_train_df.loc[new_train_df[label] == 0]\n",
    "        \n",
    "        num_to_sample = round((k / 100)*len(selected_group))\n",
    "       \n",
    "        sampled_group = selected_group.sample(n=num_to_sample)\n",
    "        not_selected = new_train_df.drop(sampled_group.index)\n",
    "        \n",
    "        selected_group_X = sampled_group[feature_set]\n",
    "        selected_group_y = sampled_group[label]\n",
    "        \n",
    "        not_selected_group_X = not_selected[feature_set]\n",
    "        not_selected_group_y = not_selected[label]\n",
    "    elif group_method == 4:\n",
    "        # based on Race or Gender and Class\n",
    "        selected_group = new_train_df.loc[new_train_df['sex'] == 0]\n",
    "        selected_group = selected_group.loc[selected_group[label] == 0]\n",
    "        \n",
    "        num_to_sample = round((k / 100)*len(selected_group))\n",
    "       \n",
    "        sampled_group = selected_group.sample(n=num_to_sample)\n",
    "        not_selected = new_train_df.drop(sampled_group.index)\n",
    "        \n",
    "        selected_group_X = sampled_group[feature_set]\n",
    "        selected_group_y = sampled_group[label]\n",
    "        \n",
    "        not_selected_group_X = not_selected[feature_set]\n",
    "        not_selected_group_y = not_selected[label]\n",
    "    elif group_method == 5: \n",
    "        # based on Race and Gender and Class\n",
    "        selected_group = new_train_df.loc[new_train_df['sex'] == 0]\n",
    "        selected_group = selected_group.loc[selected_group['race_White'] == 0]\n",
    "        selected_group = selected_group.loc[selected_group[label] == 0]\n",
    "        \n",
    "        num_to_sample = round((k / 100)*len(selected_group))\n",
    "       \n",
    "        sampled_group = selected_group.sample(n=num_to_sample)\n",
    "        not_selected = new_train_df.drop(sampled_group.index)\n",
    "        \n",
    "        selected_group_X = sampled_group[feature_set]\n",
    "        selected_group_y = sampled_group[label]\n",
    "        \n",
    "        not_selected_group_X = not_selected[feature_set]\n",
    "        not_selected_group_y = not_selected[label]\n",
    "    \n",
    "    return selected_group_X, selected_group_y, not_selected_group_X, not_selected_group_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e210d31",
   "metadata": {},
   "source": [
    "### Randomized Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "096c08d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize_resp(label, epsilon):\n",
    "\n",
    "    probability = float(E ** epsilon) / float(1 + (E ** epsilon))\n",
    "    \n",
    "    if label == 0:\n",
    "        new_label = np.random.choice([0,1], p=[probability, 1-probability])\n",
    "    else:\n",
    "        new_label = np.random.choice([0,1], p=[1-probability, probability])\n",
    "\n",
    "    return new_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f20c9e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_p(epsilon):\n",
    "    probability = float(E ** epsilon) / float(1 + (E ** epsilon))\n",
    "    p = torch.FloatTensor([[probability, 1-probability], [1-probability, probability]])\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07756821",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9daf5801",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(num_features, 1, bias=False)\n",
    "        self.criterion = torch.nn.BCEWithLogitsLoss()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        logits = self.fc1(x)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "    def loss(self, logits, y):\n",
    "        loss = self.criterion(logits.ravel(), y)\n",
    "        \n",
    "        probabilities = torch.sigmoid(logits)\n",
    "        thresh_results = []\n",
    "        \n",
    "        for p in probabilities:\n",
    "            if p>.5:\n",
    "                thresh_results.append(1)\n",
    "            else:\n",
    "                thresh_results.append(0)\n",
    "                \n",
    "        num_correct = 0\n",
    "        for r,y_ in zip(thresh_results, y):\n",
    "            if r == y_:\n",
    "                num_correct += 1\n",
    "                \n",
    "        acc = num_correct / len(y)\n",
    "        \n",
    "        return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5c1a9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataset, epsilon, lengths):\n",
    "    model.train()\n",
    "    opt = torch.optim.SGD(model.parameters(), lr=.005, weight_decay=0)\n",
    "    \n",
    "    criterion1 = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    pert_status = np.zeros(len(dataset[0]))\n",
    "            \n",
    "    train_data = CreateData(dataset[0], dataset[1], pert_status)\n",
    "    train_dataloader = DataLoader(train_data, batch_size=1, shuffle=True)\n",
    "\n",
    "    for itr in range(0, 7):\n",
    "        for i, [x,y,p] in enumerate(train_dataloader):\n",
    "            opt.zero_grad()\n",
    "            oupt = model(x)\n",
    "            \n",
    "            try:\n",
    "                loss_val = criterion1(oupt.ravel(), y)\n",
    "            except ValueError:\n",
    "                loss_val = criterion1(oupt, y)\n",
    "            \n",
    "            loss_val.backward()\n",
    "            opt.step() \n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3c7499",
   "metadata": {},
   "source": [
    "### Influence Calculation Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cc60cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_influence_single(model, epsilon, train_data, test_data, group_data, device, rec_depth, r, damp, scale, est_hess, num_features, weight_decay, criterion):\n",
    "    est_hess = inv_hess(model, [train_data[0], train_data[1]], device, rec_depth, r, criterion)\n",
    "    \n",
    "    grad_test = grad_z([test_data[0], test_data[1]], model, device, criterion)\n",
    "    s_test_vec = torch.mm(grad_test[0], est_hess)\n",
    "\n",
    "    P = get_p(epsilon)\n",
    "    \n",
    "    p_01, p_10 = P[0][1].item(), P[1][0].item()\n",
    "    \n",
    "    pi_1 = sum(list(group_data[1]))\n",
    "    pi_0 = len(group_data[1]) - pi_1\n",
    "    \n",
    "    lam_0 = round(p_01 * pi_1)\n",
    "    lam_1 = round(p_10 * pi_0)\n",
    "\n",
    "    S_pert = 1 - group_data[1]\n",
    "    \n",
    "    y_w_group_pert = pd.concat([group_data[3], S_pert], axis = 0, ignore_index=True)\n",
    "    y_wo_pert = pd.concat([group_data[3], group_data[1]], axis = 0, ignore_index=True)\n",
    "    reconstructed_x = pd.concat([group_data[2], group_data[0]], axis = 0, ignore_index=True)\n",
    "  \n",
    "    assert len(S_pert) == len(group_data[1])\n",
    "    grad_z_vec = grad_training([group_data[0],group_data[1]], S_pert, [model], device, [lam_0, lam_1], criterion)\n",
    "  \n",
    "    with torch.no_grad():\n",
    "        influence = -sum([torch.sum(k * j).data for k, j in zip(grad_z_vec, s_test_vec)]) * ((lam_0 + lam_1)/len(train_data[0]))\n",
    "       \n",
    "    return influence.cpu(), est_hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e371a36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_hess(model, train_data, device, rec_depth, r, criterion):\n",
    "    \n",
    "    H_inv = []\n",
    "    \n",
    "    i_hess = torch.zeros([len(model.fc1.weight), len(model.fc1.weight)]).to(device)\n",
    "    \n",
    "    logits = model(train_data[0])\n",
    "    loss = criterion(logits, torch.atleast_2d(train_data[1]).T)\n",
    "    \n",
    "    v = grad(loss, model.parameters())\n",
    "    \n",
    "    for i in range(r):\n",
    "        \n",
    "        inv_hess = v[0]\n",
    "        \n",
    "        for j in range(rec_depth):\n",
    "         \n",
    "            rand_index = random.sample(range(len(train_data[0])), 1)\n",
    "            rand_point = [train_data[0][rand_index], train_data[1][rand_index]]\n",
    "            \n",
    "            a = -1.0*rand_point[1]*torch.matmul(v[0], rand_point[0].T) \n",
    "            output = rand_point[1]*((torch.pow(torch.exp(a),0.5))/(torch.exp(a)+1))*rand_point[0]\n",
    "            \n",
    "            batch_hess = torch.outer(output.ravel(), output.ravel()) \n",
    "            batch_hess = torch.add(batch_hess, 2*1e-4*torch.eye(len(v[0])).to(device))\n",
    "            \n",
    "            inv_hess = torch.add(inv_hess, batch_hess)\n",
    "        \n",
    "        H_inv.append(inv_hess)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(r):\n",
    "            i_hess = torch.add(i_hess, H_inv[i])\n",
    "        \n",
    "        i_hess = i_hess / r\n",
    "\n",
    "    return i_hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1656b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_z(test_data, model, device, criterion):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    test_data_features = test_data[0]\n",
    "    test_data_labels = test_data[1]\n",
    "\n",
    "    logits = model(test_data_features)\n",
    "    loss = criterion(logits, torch.atleast_2d(test_data_labels).T)\n",
    "    \n",
    "    return grad(loss, model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38ac6b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_training(train_data, y_perts, parameters, device, epsilon, criterion):\n",
    "    \n",
    "    lam_0, lam_1 = epsilon\n",
    "    lam = lam_0 + lam_1\n",
    "    len_s = len(y_perts)\n",
    "    \n",
    "    train_data_features = torch.FloatTensor(train_data[0].values).to(device)\n",
    "    train_data_labels = torch.FloatTensor(train_data[1].values).to(device)\n",
    "    train_pert_data_labels = torch.FloatTensor(y_perts.values).to(device)\n",
    "    \n",
    "    model = parameters[0]\n",
    "    model.eval()\n",
    "\n",
    "    logits = model(train_data_features)\n",
    "\n",
    "    orig_loss = criterion(logits, torch.atleast_2d(train_data_labels).T)\n",
    "    pert_loss = criterion(logits, torch.atleast_2d(train_pert_data_labels).T)\n",
    "    loss = (lam / len_s)*(pert_loss -  orig_loss)\n",
    "\n",
    "    to_return = grad(loss, model.parameters())\n",
    "    \n",
    "        \n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f72f48",
   "metadata": {},
   "source": [
    "### Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe081fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Main(epsilon, weight_decay, rec_depth, r, scale, damp, est_hess, dataset, group_method, k):\n",
    "\n",
    "    if dataset == 'adult':\n",
    "        data = get_adult()\n",
    "        label = 'income_class'\n",
    "    elif dataset == 'diabetes':\n",
    "        data = get_diabetes()\n",
    "        label = 'readmitted'\n",
    "    else:\n",
    "        data = get_law()\n",
    "        label = 'admit'\n",
    "\n",
    "    device = 'cuda:3' if torch.cuda.is_available() else 'cpu'\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    feature_set = set(data.columns) - {label}\n",
    "    num_features = len(feature_set)\n",
    "    \n",
    "    X = data[feature_set]\n",
    "    y = data[label]\n",
    "\n",
    "    if dataset == 'diabetes':\n",
    "        undersample = RandomUnderSampler(random_state=42)\n",
    "        new_X, new_y = undersample.fit_resample(X, y)\n",
    "    else:\n",
    "        new_X = X\n",
    "        new_y = y\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(new_X, new_y, test_size=0.20, random_state=42)\n",
    "  \n",
    "    new_train_df = pd.concat([X_train, y_train], axis=1)\n",
    "  \n",
    "    train_sample_num = len(X_train)\n",
    "\n",
    "    selected_group_X, selected_group_y, not_selected_group_X, not_selected_group_y = get_data(group_method, new_train_df, feature_set, label, k)\n",
    "    \n",
    "    x_test_input = torch.FloatTensor(X_test.values).to(device)\n",
    "    y_test_input = torch.FloatTensor(y_test.values).to(device)\n",
    "\n",
    "    x_train_input = torch.FloatTensor(X_train.values).to(device)\n",
    "    y_train_input = torch.FloatTensor(y_train.values).to(device)\n",
    "   \n",
    "    torch_model = LogisticRegression(num_features)\n",
    "    torch.save(torch_model.state_dict(), 'models/initial_config.pth')\n",
    "    torch_model.to(device)\n",
    "    torch_model = train(torch_model, [x_train_input, y_train_input], epsilon, None)\n",
    "    \n",
    "    test_loss_ori, acc_ori = torch_model.loss(torch_model(x_test_input), y_test_input)\n",
    "\n",
    "    loss_diff_approx, est_hess = calc_influence_single(torch_model, epsilon, [x_train_input, y_train_input], [x_test_input, y_test_input], [selected_group_X, selected_group_y, not_selected_group_X, not_selected_group_y], device, rec_depth, r, damp, scale, est_hess, num_features, weight_decay, criterion)\n",
    "    loss_diff_approx = - torch.FloatTensor(loss_diff_approx).cpu().numpy()\n",
    "\n",
    "    # retrain\n",
    "    P = get_p(epsilon)\n",
    "    \n",
    "    p_01, p_10 = P[0][1].item(), P[1][0].item()\n",
    "    \n",
    "    pi_1 = sum(list(selected_group_y))\n",
    "    pi_0 = len(selected_group_y) - pi_1\n",
    "    \n",
    "    lam_0 = round(p_01 * pi_1)\n",
    "    lam_1 = round(p_10 * pi_0)\n",
    "\n",
    "    S = pd.concat([selected_group_X, selected_group_y], axis=1, ignore_index=False)\n",
    "\n",
    "    G0 = S[label][S[label].eq(1)].sample(lam_0).index\n",
    "    G1 = S[label][S[label].eq(0)].sample(lam_1).index\n",
    "  \n",
    "    G = S.loc[G0.union(G1)]\n",
    "    not_g = S.drop(G0.union(G1))\n",
    "    \n",
    "    G_pert = 1 - selected_group_y\n",
    "    \n",
    "    y_w_group_pert = pd.concat([not_selected_group_y, not_g[label], G_pert], axis = 0, ignore_index=True)\n",
    "    y_wo_pert = pd.concat([not_selected_group_y, not_g[label], G[label]], axis = 0, ignore_index=True)\n",
    "    reconstructed_x = pd.concat([not_selected_group_X, not_g[feature_set], G[feature_set]], axis = 0, ignore_index=True)\n",
    "    \n",
    "    model_pert = LogisticRegression(num_features)\n",
    "    model_pert.load_state_dict(torch.load('models/initial_config.pth'))\n",
    "    model_pert.to(device)\n",
    "    model_pert = train(model_pert, [torch.FloatTensor(reconstructed_x.values).to(device), torch.FloatTensor(y_w_group_pert.values).to(device)], epsilon, None)\n",
    "    test_loss_retrain, acc_retrain = model_pert.loss(model_pert(x_test_input), y_test_input)\n",
    "\n",
    "     # get true loss diff\n",
    "    loss_diff_true = test_loss_retrain - test_loss_ori\n",
    "    print(f\"Original/Retrain Loss: {test_loss_ori:.3f}/{test_loss_retrain:.3f}\")\n",
    "    est_loss_diff = loss_diff_approx\n",
    "    avg_loss_diff = loss_diff_true.detach().cpu().numpy()\n",
    "\n",
    "    return avg_loss_diff, est_loss_diff, est_hess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dfc9de",
   "metadata": {},
   "source": [
    "### Perform Experiment "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2747a8a8",
   "metadata": {},
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "391d9b94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epsilons = [.01, .1, 1] #.05, .1, .15, .2, .25, .3, .35, .4, .45, .5, .55, .6, .65, .7, .75, .8, .85, .9, .95, 1]\n",
    "\n",
    "k = [25]\n",
    "k_3 = np.linspace(1, 25, 5)\n",
    "\n",
    "group_method = [1]#, 2]#, 3, 4, 5]\n",
    "\n",
    "weight_decay = 0.01\n",
    "rec_depth = 5350\n",
    "r_ = 5\n",
    "scale = 10\n",
    "damp = 0.01 \n",
    "rounds = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20b5c9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25]\n"
     ]
    }
   ],
   "source": [
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff4d7588",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Working on epsilon:  0.01\n",
      "Original/Retrain Loss: 0.341/0.361\n",
      "Round:  0  -  0.020061463 780.223\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m est_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(rounds):\n\u001b[0;32m---> 22\u001b[0m     avg_loss_diff, est_loss_diff, est_hess \u001b[38;5;241m=\u001b[39m \u001b[43mMain\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrec_depth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdamp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mest_hess\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43madult\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRound: \u001b[39m\u001b[38;5;124m'\u001b[39m, r, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m - \u001b[39m\u001b[38;5;124m'\u001b[39m, avg_loss_diff, est_loss_diff)\n\u001b[1;32m     24\u001b[0m     avg_losses\u001b[38;5;241m.\u001b[39mappend(avg_loss_diff)\n",
      "Cell \u001b[0;32mIn[15], line 50\u001b[0m, in \u001b[0;36mMain\u001b[0;34m(epsilon, weight_decay, rec_depth, r, scale, damp, est_hess, dataset, group_method, k)\u001b[0m\n\u001b[1;32m     46\u001b[0m torch_model \u001b[38;5;241m=\u001b[39m train(torch_model, [x_train_input, y_train_input], epsilon, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     48\u001b[0m test_loss_ori, acc_ori \u001b[38;5;241m=\u001b[39m torch_model\u001b[38;5;241m.\u001b[39mloss(torch_model(x_test_input), y_test_input)\n\u001b[0;32m---> 50\u001b[0m loss_diff_approx, est_hess \u001b[38;5;241m=\u001b[39m \u001b[43mcalc_influence_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mx_train_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_input\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mx_test_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_input\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mselected_group_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_group_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnot_selected_group_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnot_selected_group_y\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrec_depth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdamp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mest_hess\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m loss_diff_approx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(loss_diff_approx)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# retrain\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m, in \u001b[0;36mcalc_influence_single\u001b[0;34m(model, epsilon, train_data, test_data, group_data, device, rec_depth, r, damp, scale, est_hess, num_features, weight_decay, criterion)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalc_influence_single\u001b[39m(model, epsilon, train_data, test_data, group_data, device, rec_depth, r, damp, scale, est_hess, num_features, weight_decay, criterion):\n\u001b[0;32m----> 2\u001b[0m     est_hess \u001b[38;5;241m=\u001b[39m \u001b[43minv_hess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrec_depth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     grad_test \u001b[38;5;241m=\u001b[39m grad_z([test_data[\u001b[38;5;241m0\u001b[39m], test_data[\u001b[38;5;241m1\u001b[39m]], model, device, criterion)\n\u001b[1;32m      5\u001b[0m     s_test_vec \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmm(grad_test[\u001b[38;5;241m0\u001b[39m], est_hess)\n",
      "Cell \u001b[0;32mIn[12], line 21\u001b[0m, in \u001b[0;36minv_hess\u001b[0;34m(model, train_data, device, rec_depth, r, criterion)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ri \u001b[38;5;129;01min\u001b[39;00m rand_index:\n\u001b[1;32m     20\u001b[0m     rand_features\u001b[38;5;241m.\u001b[39mappend(train_data[\u001b[38;5;241m0\u001b[39m][ri]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m---> 21\u001b[0m     rand_labels\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mri\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     23\u001b[0m rand_features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(rand_features)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     24\u001b[0m rand_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39matleast_2d(torch\u001b[38;5;241m.\u001b[39mFloatTensor(rand_labels)\u001b[38;5;241m.\u001b[39mto(device))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "est_hess = None\n",
    "\n",
    "e_k_g_avg_losses = []\n",
    "e_k_g_est_losses = []\n",
    "\n",
    "\n",
    "for e in epsilons:\n",
    "    print('\\nWorking on epsilon: ', e)\n",
    "    \n",
    "    k_g_avg_losses = []\n",
    "    k_g_est_losses = []\n",
    "    \n",
    "    for i, k_ in enumerate(k):\n",
    "        g_avg_losses = []\n",
    "        g_est_losses = []\n",
    "        \n",
    "        for group in group_method:\n",
    "            avg_losses = []\n",
    "            est_losses = []\n",
    "                \n",
    "            for r in range(rounds):\n",
    "                avg_loss_diff, est_loss_diff, est_hess = Main(e, weight_decay, rec_depth, r_, scale, damp, est_hess, 'adult', group, k_)\n",
    "                print('Round: ', r, ' - ', avg_loss_diff, est_loss_diff)\n",
    "                avg_losses.append(avg_loss_diff)\n",
    "                est_losses.append(est_loss_diff)\n",
    "            \n",
    "            g_avg_losses.append(np.mean(avg_losses))\n",
    "            g_est_losses.append(np.mean(est_losses))\n",
    "            print('Avg. Loss: ', np.mean(avg_losses))\n",
    "            print('Est. Loss: ', np.mean(est_losses))  \n",
    "                  \n",
    "        k_g_avg_losses.append(g_avg_losses)\n",
    "        k_g_est_losses.append(g_est_losses)\n",
    "    \n",
    "    e_k_g_avg_losses.append(k_g_avg_losses)\n",
    "    e_k_g_est_losses.append(k_g_est_losses)\n",
    "    \n",
    "    with open('e_k_g_avg_losses.txt', \"wb\") as file:   #Pickling\n",
    "        pickle.dump(e_k_g_avg_losses, file)\n",
    "        \n",
    "    with open('e_k_g_est_losses.txt', \"wb\") as file2:   #Pickling\n",
    "        pickle.dump(e_k_g_est_losses, file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1619a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('e_k_g_est_losses.txt', 'rb') as f:\n",
    "    e_k_g_est_losses = pickle.load(f)\n",
    "    \n",
    "with open('e_k_g_avg_losses.txt', 'rb') as f:\n",
    "    e_k_g_avg_losses = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68ef165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [actual, estimate]\n",
    "e_g_k = [[[[[] for R in range(2)] for u in range(len(k))] for _ in range(len(group_method))] for p in range(len(epsilons))]\n",
    "k_g_e = [[[[[] for R in range(2)] for u in range(len(epsilons))] for _ in range(len(group_method))] for p in range(len(k))]\n",
    "\n",
    "for e_ in range(len(epsilons)):\n",
    "    k_g_avg = e_k_g_avg_losses[e_]\n",
    "    k_g_est = e_k_g_est_losses[e_]\n",
    "    \n",
    "    for k_ in range(len(k_g_avg)):\n",
    "        \n",
    "        g_avg = k_g_avg[k_]\n",
    "        g_est = k_g_est[k_]\n",
    "        \n",
    "        for g_ in range(len(g_avg)):\n",
    "                e_g_k[e_][g_][k_][0] = g_avg[g_]\n",
    "                e_g_k[e_][g_][k_][1] = g_est[g_]\n",
    "                    \n",
    "                k_g_e[k_][g_][e_][0] = g_avg[g_]\n",
    "                k_g_e[k_][g_][e_][1] = g_est[g_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e107d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('label-only-egk.txt', \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(e_g_k, fp)\n",
    "    \n",
    "with open('label-only-egk.txt', \"wb\") as p:   #Pickling\n",
    "    pickle.dump(k_g_e, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28fa022",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# graph_k([e_g_k[5], k], str(epsilons[5]), 'Real')\n",
    "# graph_k([e_g_k[5], k], str(epsilons[5]), 'Est.')\n",
    "# graph_ep([k_g_e[5], epsilons], str(k[5]), 'Real')\n",
    "# graph_ep([k_g_e[5], epsilons], str(k[5]), 'Est.')\n",
    "\n",
    "graph_k([e_g_k[0], k], str(epsilons[0]), 'Real')\n",
    "graph_k([e_g_k[0], k], str(epsilons[0]), 'Est.')\n",
    "graph_ep([k_g_e[0], epsilons], str(k[0]), 'Real')\n",
    "graph_ep([k_g_e[0], epsilons], str(k[0]), 'Est.')\n",
    "\n",
    "# graph_k([e_g_k[-1], k], str(epsilons[-1]), 'Real')\n",
    "# graph_k([e_g_k[-1], k], str(epsilons[-1]), 'Est.')\n",
    "# graph_ep([k_g_e[-1], epsilons], str(k[-1]), 'Real')\n",
    "# graph_ep([k_g_e[-1], epsilons], str(k[-1]), 'Est.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7328f6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_k([e_g_k[0], k], str(epsilons[0]), 'Real')\n",
    "graph_k([e_g_k[0], k], str(epsilons[0]), 'Est.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752355d6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# r2_s = visualize_result(e_g_k[5], str(epsilons[5]), 'epsilon')\n",
    "# r2_s = visualize_result(k_g_e[5], str(k[5]), 'k')\n",
    "\n",
    "r2_s = visualize_result(e_g_k[0], str(epsilons[0]), 'epsilon')\n",
    "r2_s = visualize_result(k_g_e[0], str(k[0]), 'k')\n",
    "\n",
    "# r2_s = visualize_result(e_g_k[-1], str(epsilons[-1]), 'epsilon')\n",
    "# r2_s = visualize_result(k_g_e[-1], str(k[-1]), 'k')\n",
    "\n",
    "# r2_s = visualize_result(e_g_k[12], str(epsilons[12]), 'epsilon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d815add",
   "metadata": {},
   "outputs": [],
   "source": [
    "spearman_g_ep = []\n",
    "spearman_g_k = []\n",
    "\n",
    "mae_g_ep = []\n",
    "mae_g_k = []\n",
    "    \n",
    "for i, egk in enumerate(e_g_k):\n",
    "    actual = [[x[0] for x in egk[0]], [x[0] for x in egk[1]]]#, [x[0] for x in egk[2]], [x[0] for x in egk[3]], [x[0] for x in egk[4]]]\n",
    "    estimated = [[x[1] for x in egk[0]], [x[1] for x in egk[1]]]#, [x[1] for x in egk[2]], [x[1] for x in egk[3]], [x[1] for x in egk[4]]]\n",
    "\n",
    "    spearman = []\n",
    "    mae = []\n",
    "    \n",
    "    for j in range(len(actual)):\n",
    "        spearman.append(spearmanr(actual[j], estimated[j]).correlation)\n",
    "        mae.append(mean_absolute_error(actual[j], estimated[j]))\n",
    "        \n",
    "    spearman_g_ep.append(spearman)\n",
    "    mae_g_ep.append(mae)\n",
    "    \n",
    "for i, kge in enumerate(k_g_e):\n",
    "    actual = [[x[0] for x in kge[0]], [x[0] for x in kge[1]]]#, [x[0] for x in kge[2]], [x[0] for x in kge[3]], [x[0] for x in kge[4]]]\n",
    "    estimated = [[x[1] for x in kge[0]], [x[1] for x in kge[1]]]#, [x[1] for x in kge[2]], [x[1] for x in kge[3]], [x[1] for x in kge[4]]]\n",
    "\n",
    "    spearman = []\n",
    "    mae = []\n",
    "    \n",
    "    for j in range(len(actual)):\n",
    "        spearman.append(spearmanr(actual[j], estimated[j]).correlation)\n",
    "        mae.append(mean_absolute_error(actual[j], estimated[j]))\n",
    "        \n",
    "    spearman_g_k.append(spearman)\n",
    "    mae_g_k.append(mae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4998b0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(spearman_g_ep), len(spearman_g_ep[0])\n",
    "spearman_g_ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f1ae1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(spearman_g_k), len(spearman_g_k[0])\n",
    "spearman_g_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c888d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mae_g_ep), len(mae_g_ep[0])\n",
    "mae_g_ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e4f546",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mae_g_k), len(mae_g_k[0])\n",
    "mae_g_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d614d025",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_est = [22, 20,23,21,19,18,16,17,15,14,12,13,11,10,9,8,7,6,4,5,3,2,1]\n",
    "rank_act = [23,20,21,22,17,18,16,19,14,15,13,12,11,10,8,7,9,6,4,5,2,3,1]\n",
    "a= np.cov(np.array(rank_est), np.array(rank_act), bias=True)[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af31895",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.std(np.array(rank_est))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53aa47f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.std(np.array(rank_act))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df00495f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a /(b*c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f76788c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a, b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb85715",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
