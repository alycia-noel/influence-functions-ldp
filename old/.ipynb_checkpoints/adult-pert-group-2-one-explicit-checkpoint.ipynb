{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e364451",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import copy\n",
    "import torch\n",
    "import random\n",
    "import logging\n",
    "import warnings\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import torch.optim as opt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn import linear_model\n",
    "from torch.autograd import grad\n",
    "from torch.autograd.functional import vhp\n",
    "from data_processing import get_data_adult\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, accuracy_score\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "E = math.e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d88723b",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dafa202c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph(data, title, x_label, y_label, labels):\n",
    "    sns.set(font_scale=1)\n",
    "\n",
    "    ticks = np.arange(0, len(data[2]), step=1)\n",
    "    plt.xticks(ticks=ticks, labels=data[2], rotation='vertical')\n",
    "    \n",
    "    plt.plot(data[0], 'b-', linewidth=2.0, label=labels[0])\n",
    "    plt.plot(data[1], 'r-', linewidth=2.0, label=labels[1])\n",
    "    plt.plot([a_i - b_i for a_i, b_i in zip(data[0], data[1])], 'k', linewidth=2.0, label='difference')\n",
    "    \n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "741afa72",
   "metadata": {},
   "outputs": [],
   "source": [
    " class CreateData(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        out_data = self.data[idx]\n",
    "        out_label = self.targets[idx]\n",
    "\n",
    "        return out_data, out_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e210d31",
   "metadata": {},
   "source": [
    "### Randomized Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "096c08d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize_resp(label, epsilon):\n",
    "\n",
    "    probability = float(E ** epsilon) / float(1 + (E ** epsilon))\n",
    "    \n",
    "    if label == 0:\n",
    "        new_label = np.random.choice([0,1], p=[probability, 1-probability])\n",
    "    else:\n",
    "        new_label = np.random.choice([0,1], p=[1-probability, probability])\n",
    "\n",
    "    return new_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07756821",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9daf5801",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, num_features, weight_decay, device):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "\n",
    "        self.wd = torch.FloatTensor([weight_decay]).to(device)\n",
    "        self.w = torch.nn.Parameter(torch.zeros([num_features], requires_grad=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = torch.matmul(x, torch.reshape(self.w, [-1, 1]))\n",
    "\n",
    "        return logits\n",
    "    \n",
    "    def loss(self, logits, y):\n",
    "        preds = torch.sigmoid(logits)\n",
    "        loss = -torch.mean(y * log_clip(preds) + (1 - y) * log_clip(1 - preds))\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5c1a9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_clip(x):\n",
    "    return torch.log(torch.clamp(x, 1e-10, None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3c7499",
   "metadata": {},
   "source": [
    "### Influence Calculation Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cc60cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_influence_single(model, epsilon, train_data, test_data, device, rec_depth, r, damp, scale):\n",
    "    \n",
    "        x_test, y_test = test_data[1], test_data[2]\n",
    "        x_test = test_data[0].collate_fn([x_test])\n",
    "        y_test = test_data[0].collate_fn([y_test])\n",
    "        \n",
    "        s_test_vec = s_test_sample(model, train_data, [x_test, y_test], device, rec_depth, r, damp, scale)\n",
    "\n",
    "        # Calculate the influence function\n",
    "        train_dataset_size = len(train_data[0].dataset)\n",
    "        y_perts = []\n",
    "\n",
    "        for i, y_ in enumerate(train_data[2]):\n",
    "            y_pert = randomize_resp(y_, epsilon)\n",
    "            y_perts.append(y_pert)\n",
    "\n",
    "        time_a = datetime.datetime.now()\n",
    "\n",
    "        grad_z_vec = grad_training([train_data[1], train_data[2]], y_perts, model, device)\n",
    "\n",
    "        time_b = datetime.datetime.now()\n",
    "        time_delta = time_b - time_a\n",
    "        logging.info(f\"Time for grad_z iter:\" f\" {time_delta.total_seconds() * 1000}\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            influence = sum([torch.sum(k * j).data for k, j in zip(grad_z_vec, s_test_vec)])\n",
    "\n",
    "        return influence.cpu(), y_perts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e371a36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def s_test_sample(model, train_data, test_data, device, rec_depth, r, damp, scale):\n",
    "\n",
    "    inverse_hvp = [torch.zeros_like(params, dtype=torch.float) for params in model.parameters()]\n",
    "\n",
    "    for i in range(r):\n",
    "\n",
    "        hessian_loader = DataLoader(train_data[0].dataset, sampler=torch.utils.data.RandomSampler(train_data[0].dataset, True, num_samples=rec_depth),batch_size=1,num_workers=4,)\n",
    "\n",
    "        cur_estimate = s_test(test_data, model, i, hessian_loader, device, damp, scale)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inverse_hvp = [old + (cur / scale) for old, cur in zip(inverse_hvp, cur_estimate)]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inverse_hvp = [component / r for component in inverse_hvp]\n",
    "\n",
    "    return inverse_hvp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f2e41c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_z(test_data, model, device):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    test_data_features = test_data[0]\n",
    "    test_data_labels = test_data[1]\n",
    "\n",
    "    logits = model(test_data_features)\n",
    "    prediction = torch.sigmoid(logits)\n",
    "    loss = -torch.mean(test_data_labels * log_clip(prediction) + (1 - test_data_labels) * log_clip(1 - prediction))\n",
    "    \n",
    "    return grad(loss, model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38ac6b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_training(train_data, y_perts, model, device):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    train_data_features = torch.FloatTensor(train_data[0]).to(device)\n",
    "    train_data_labels = torch.FloatTensor(train_data[1]).to(device)\n",
    "    train_pert_data_labels = torch.FloatTensor(y_perts).to(device)\n",
    "\n",
    "    logits = model(train_data_features)\n",
    "    prediction = torch.sigmoid(logits)\n",
    "    orig_loss = -torch.mean(train_data_labels * log_clip(prediction) + (1 - train_data_labels) * log_clip(1 - prediction))\n",
    "    pert_loss = -torch.mean(train_pert_data_labels * log_clip(prediction) + (1 - train_pert_data_labels) * log_clip(1 - prediction))\n",
    "    \n",
    "    loss = (pert_loss -  orig_loss)\n",
    "    \n",
    "    return grad(loss, model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "956886b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def s_test(test_data, model, i, hessian_loader, device, damp, scale):\n",
    "\n",
    "    v = grad_z(test_data, model, device)\n",
    "    h_estimate = v\n",
    "\n",
    "    params, names = make_functional(model)\n",
    "    params = tuple(p.detach().requires_grad_() for p in params)\n",
    "\n",
    "    progress_bar = tqdm(hessian_loader, desc=f\"IHVP sample {i}\")\n",
    "    for i, (x_train, y_train) in enumerate(progress_bar):\n",
    "\n",
    "        x_train, y_train = x_train.type(torch.FloatTensor).to(device), y_train.type(torch.FloatTensor).to(device)\n",
    "\n",
    "        def f(*new_params):\n",
    "            load_weights(model, names, new_params)\n",
    "            out = model(x_train)\n",
    "            loss = calc_loss(out, y_train)\n",
    "            return loss\n",
    "\n",
    "        hv = vhp(f, params, tuple(h_estimate), strict=True)[1]\n",
    "\n",
    "        # Recursively calculate h_estimate\n",
    "        \n",
    "        # Influence = grad(x_test)^T*(H_1 + lambda*I)*\\sum grad(x_train)\n",
    "        with torch.no_grad():\n",
    "            h_estimate = [_v + (1 - damp) * _h_e - _hv / scale for _v, _h_e, _hv in zip(v, h_estimate, hv)]\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                norm = sum([h_.norm() for h_ in h_estimate])\n",
    "                progress_bar.set_postfix({\"est_norm\": norm.item()})\n",
    "\n",
    "    with torch.no_grad():\n",
    "        load_weights(model, names, params, as_params=True)\n",
    "\n",
    "    return h_estimate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b8e23c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(logits, labels):\n",
    "    preds = torch.sigmoid(logits)\n",
    "    loss = -torch.mean(labels * log_clip(preds) + (1 - labels) * log_clip(1 - preds))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1413935",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_functional(model):\n",
    "    orig_params = tuple(model.parameters())\n",
    "\n",
    "    names = []\n",
    "\n",
    "    for name, p in list(model.named_parameters()):\n",
    "        del_attr(model, name.split(\".\"))\n",
    "        names.append(name)\n",
    "\n",
    "    return orig_params, names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0232f0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_attr(obj, names):\n",
    "    if len(names) == 1:\n",
    "        delattr(obj, names[0])\n",
    "    else:\n",
    "        del_attr(getattr(obj, names[0]), names[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a98806be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_attr(obj, names, val):\n",
    "    if len(names) == 1:\n",
    "        setattr(obj, names[0], val)\n",
    "    else:\n",
    "        set_attr(getattr(obj, names[0]), names[1:], val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37c996a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights(model, names, params, as_params=False):\n",
    "    for name, p in zip(names, params):\n",
    "        if not as_params:\n",
    "            set_attr(model, name.split(\".\"), p)\n",
    "        else:\n",
    "            set_attr(model, name.split(\".\"), torch.nn.Parameter(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f72f48",
   "metadata": {},
   "source": [
    "### Perform Influence Calculation and LOO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe081fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestLeaveOneOut():\n",
    "    def test_leave_one_out(self, epsilon, weight_decay, rec_depth, r, scale, damp):\n",
    "\n",
    "        num_features, train_data, test_data = get_data_adult()\n",
    "\n",
    "        device = 'cuda:3' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "        train_sample_num = len(train_data[1])\n",
    "\n",
    "        train_dataset = CreateData(train_data[0], train_data[1])\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "        # prepare sklearn model to train w\n",
    "        C = 1.0 / (train_sample_num * weight_decay)\n",
    "        sklearn_model = linear_model.LogisticRegression(C=C, solver='lbfgs', tol=1e-8, fit_intercept=False)\n",
    "\n",
    "        # prepare pytorch model to compute influence function\n",
    "        torch_model = LogisticRegression(num_features, weight_decay, device)\n",
    "\n",
    "        # train\n",
    "        sklearn_model.fit(train_data[0], train_data[1])\n",
    "        pred_logr = sklearn_model.predict(test_data[0])\n",
    "        score = accuracy_score(test_data[1], pred_logr)\n",
    "        print(f'lbfgs training took {sklearn_model.n_iter_} iter. Accuracy: {score:0.3f}'  )\n",
    "\n",
    "        # assign W into pytorch model\n",
    "        w_opt = sklearn_model.coef_.ravel()\n",
    "        with torch.no_grad():\n",
    "            torch_model.w = torch.nn.Parameter(\n",
    "                torch.tensor(w_opt, dtype=torch.float)\n",
    "            )\n",
    "        torch_model = torch_model.to(device)\n",
    "        \n",
    "        # calculate original loss\n",
    "        x_test_input = torch.FloatTensor(test_data[0][12345]).to(device)\n",
    "        y_test_input = torch.LongTensor(test_data[1])[12345].to(device)\n",
    "\n",
    "        test_dataset = CreateData(test_data[0][12345], test_data[1][12345])\n",
    "        test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "      \n",
    "        test_loss_ori = torch_model.loss(torch_model(x_test_input), y_test_input).detach().cpu().numpy()\n",
    "\n",
    "        loss_diff_approx, train_data_perts = calc_influence_single(torch_model, epsilon, [train_loader, train_data[0], train_data[1]], [test_loader, x_test_input, y_test_input], device, rec_depth, r, damp, scale)\n",
    "        \n",
    "        loss_diff_approx = - torch.FloatTensor(loss_diff_approx).cpu().numpy()\n",
    "\n",
    "        # retrain\n",
    "        sklearn_model_pert = linear_model.LogisticRegression(C=C, fit_intercept=False, tol=1e-8, solver='lbfgs')\n",
    "        sklearn_model_pert.fit(train_data[0], train_data_perts)\n",
    "        \n",
    "        pred_logr = sklearn_model_pert.predict(test_data[0])\n",
    "        score = accuracy_score(test_data[1], pred_logr)\n",
    "        print(f'Perturbation lbfgs training took {sklearn_model_pert.n_iter_} iter. Accuracy: {score:0.3f}'  )\n",
    "       \n",
    "        w_retrain = sklearn_model_pert.coef_.T.ravel()\n",
    "        with torch.no_grad():\n",
    "            torch_model.w = torch.nn.Parameter(\n",
    "                torch.tensor(w_retrain, dtype=torch.float)\n",
    "            )\n",
    "\n",
    "        torch_model = torch_model.to(device)\n",
    "\n",
    "        # get retrain loss\n",
    "        test_loss_retrain = torch_model.loss(torch_model(x_test_input), y_test_input).detach().cpu().numpy()\n",
    "\n",
    "        # get true loss diff\n",
    "        loss_diff_true = test_loss_retrain - test_loss_ori\n",
    "        \n",
    "        est_loss_diff = loss_diff_approx\n",
    "        avg_loss_diff = loss_diff_true\n",
    "     \n",
    "        print('Real avg. loss diff: ', avg_loss_diff, 'Est. avg. loss diff: ', est_loss_diff)\n",
    "        \n",
    "        return avg_loss_diff, est_loss_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd5b9aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_result(actual_loss_diff, estimated_loss_diff):\n",
    "    r2_s = r2_score(actual_loss_diff, estimated_loss_diff)\n",
    "\n",
    "    max_abs = np.max([np.abs(actual_loss_diff), np.abs(estimated_loss_diff)])\n",
    "    min_, max_ = -max_abs * 1.1, max_abs * 1.1\n",
    "    plt.rcParams['figure.figsize'] = 6, 5\n",
    "    plt.scatter(actual_loss_diff, estimated_loss_diff, zorder=2, s=10)\n",
    "    plt.title('Loss diff Pert.')\n",
    "    plt.xlabel('Actual loss diff')\n",
    "    plt.ylabel('Estimated loss diff')\n",
    "    range_ = [min_, max_]\n",
    "    plt.plot(range_, range_, 'k-', alpha=0.2, zorder=1)\n",
    "    text = 'MAE = {:.03}\\nR2 score = {:.03}'.format(mean_absolute_error(actual_loss_diff, estimated_loss_diff),\n",
    "                                                    r2_s)\n",
    "    plt.text(max_abs, -max_abs, text, verticalalignment='bottom', horizontalalignment='right')\n",
    "    plt.xlim(min_, max_)\n",
    "    plt.ylim(min_, max_)\n",
    "\n",
    "    plt.savefig(\"result.png\")\n",
    "\n",
    "    return r2_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "391d9b94",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on epsilon:  0.001\n",
      "lbfgs training took [21] iter. Accuracy: 0.868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IHVP sample 0: 100%|████████████████████████████████████████████████| 5350/5350 [00:16<00:00, 325.50it/s, est_norm=11.5]\n",
      "IHVP sample 1: 100%|████████████████████████████████████████████████| 5350/5350 [00:17<00:00, 309.41it/s, est_norm=11.2]\n",
      "IHVP sample 2: 100%|████████████████████████████████████████████████| 5350/5350 [00:17<00:00, 298.49it/s, est_norm=11.4]\n",
      "IHVP sample 3: 100%|████████████████████████████████████████████████| 5350/5350 [00:12<00:00, 424.54it/s, est_norm=11.5]\n",
      "IHVP sample 4: 100%|████████████████████████████████████████████████| 5350/5350 [00:17<00:00, 311.21it/s, est_norm=11.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perturbation lbfgs training took [17] iter. Accuracy: 0.430\n",
      "Real avg. loss diff:  0.655242 Est. avg. loss diff:  0.19198672\n",
      "Working on epsilon:  0.005\n",
      "lbfgs training took [21] iter. Accuracy: 0.868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IHVP sample 0: 100%|████████████████████████████████████████████████| 5350/5350 [00:15<00:00, 335.07it/s, est_norm=11.3]\n",
      "IHVP sample 1: 100%|████████████████████████████████████████████████| 5350/5350 [00:17<00:00, 299.06it/s, est_norm=11.3]\n",
      "IHVP sample 2: 100%|████████████████████████████████████████████████| 5350/5350 [00:18<00:00, 293.49it/s, est_norm=11.5]\n",
      "IHVP sample 3: 100%|████████████████████████████████████████████████| 5350/5350 [00:16<00:00, 324.28it/s, est_norm=11.6]\n",
      "IHVP sample 4: 100%|████████████████████████████████████████████████| 5350/5350 [00:17<00:00, 298.02it/s, est_norm=11.7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perturbation lbfgs training took [13] iter. Accuracy: 0.414\n",
      "Real avg. loss diff:  0.66193706 Est. avg. loss diff:  0.17748609\n",
      "Working on epsilon:  0.01\n",
      "lbfgs training took [21] iter. Accuracy: 0.868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IHVP sample 0: 100%|████████████████████████████████████████████████| 5350/5350 [00:16<00:00, 330.67it/s, est_norm=11.3]\n",
      "IHVP sample 1: 100%|████████████████████████████████████████████████| 5350/5350 [00:17<00:00, 303.33it/s, est_norm=11.6]\n",
      "IHVP sample 2: 100%|████████████████████████████████████████████████| 5350/5350 [00:16<00:00, 317.57it/s, est_norm=11.7]\n",
      "IHVP sample 3: 100%|████████████████████████████████████████████████| 5350/5350 [00:17<00:00, 300.66it/s, est_norm=11.8]\n",
      "IHVP sample 4: 100%|████████████████████████████████████████████████| 5350/5350 [00:17<00:00, 300.66it/s, est_norm=11.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perturbation lbfgs training took [16] iter. Accuracy: 0.420\n",
      "Real avg. loss diff:  0.6502082 Est. avg. loss diff:  0.1774888\n",
      "Working on epsilon:  0.05\n",
      "lbfgs training took [21] iter. Accuracy: 0.868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IHVP sample 0:  35%|████████████████▋                               | 1866/5350 [00:06<00:12, 289.17it/s, est_norm=11.2]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m epsilons:\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWorking on epsilon: \u001b[39m\u001b[38;5;124m'\u001b[39m, e)\n\u001b[0;32m---> 20\u001b[0m     avg_loss_diff, est_loss_diff \u001b[38;5;241m=\u001b[39m \u001b[43mLOO\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_leave_one_out\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrec_depth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdamp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     avg_losses\u001b[38;5;241m.\u001b[39mappend(avg_loss_diff)\n\u001b[1;32m     22\u001b[0m     est_losses\u001b[38;5;241m.\u001b[39mappend(est_loss_diff)\n",
      "Cell \u001b[0;32mIn[17], line 43\u001b[0m, in \u001b[0;36mTestLeaveOneOut.test_leave_one_out\u001b[0;34m(self, epsilon, weight_decay, rec_depth, r, scale, damp)\u001b[0m\n\u001b[1;32m     39\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(test_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     41\u001b[0m test_loss_ori \u001b[38;5;241m=\u001b[39m torch_model\u001b[38;5;241m.\u001b[39mloss(torch_model(x_test_input), y_test_input)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m---> 43\u001b[0m loss_diff_approx, train_data_perts \u001b[38;5;241m=\u001b[39m \u001b[43mcalc_influence_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_test_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_input\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrec_depth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdamp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m loss_diff_approx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(loss_diff_approx)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# retrain\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m, in \u001b[0;36mcalc_influence_single\u001b[0;34m(model, epsilon, train_data, test_data, device, rec_depth, r, damp, scale)\u001b[0m\n\u001b[1;32m      4\u001b[0m x_test \u001b[38;5;241m=\u001b[39m test_data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcollate_fn([x_test])\n\u001b[1;32m      5\u001b[0m y_test \u001b[38;5;241m=\u001b[39m test_data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcollate_fn([y_test])\n\u001b[0;32m----> 7\u001b[0m s_test_vec \u001b[38;5;241m=\u001b[39m \u001b[43ms_test_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrec_depth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdamp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Calculate the influence function\u001b[39;00m\n\u001b[1;32m     10\u001b[0m train_dataset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdataset)\n",
      "Cell \u001b[0;32mIn[8], line 9\u001b[0m, in \u001b[0;36ms_test_sample\u001b[0;34m(model, train_data, test_data, device, rec_depth, r, damp, scale)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(r):\n\u001b[1;32m      7\u001b[0m     hessian_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdataset, sampler\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mRandomSampler(train_data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;28;01mTrue\u001b[39;00m, num_samples\u001b[38;5;241m=\u001b[39mrec_depth),batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,)\n\u001b[0;32m----> 9\u001b[0m     cur_estimate \u001b[38;5;241m=\u001b[39m \u001b[43ms_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhessian_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdamp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     12\u001b[0m         inverse_hvp \u001b[38;5;241m=\u001b[39m [old \u001b[38;5;241m+\u001b[39m (cur \u001b[38;5;241m/\u001b[39m scale) \u001b[38;5;28;01mfor\u001b[39;00m old, cur \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(inverse_hvp, cur_estimate)]\n",
      "Cell \u001b[0;32mIn[11], line 10\u001b[0m, in \u001b[0;36ms_test\u001b[0;34m(test_data, model, i, hessian_loader, device, damp, scale)\u001b[0m\n\u001b[1;32m      7\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(p\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mrequires_grad_() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m params)\n\u001b[1;32m      9\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m tqdm(hessian_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIHVP sample \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (x_train, y_train) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(progress_bar):\n\u001b[1;32m     12\u001b[0m     x_train, y_train \u001b[38;5;241m=\u001b[39m x_train\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mFloatTensor)\u001b[38;5;241m.\u001b[39mto(device), y_train\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mFloatTensor)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf\u001b[39m(\u001b[38;5;241m*\u001b[39mnew_params):\n",
      "File \u001b[0;32m~/.conda/envs/influence/lib/python3.9/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/influence/lib/python3.9/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.conda/envs/influence/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1316\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1315\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1316\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1317\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1319\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/influence/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1282\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1279\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1280\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1282\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1283\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1284\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.conda/envs/influence/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1120\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1108\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1109\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1117\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1119\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1120\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1121\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1122\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1123\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/influence/lib/python3.9/multiprocessing/queues.py:122\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rlock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# unserialize the data after having released the lock\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ForkingPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/influence/lib/python3.9/site-packages/torch/multiprocessing/reductions.py:305\u001b[0m, in \u001b[0;36mrebuild_storage_fd\u001b[0;34m(cls, df, size)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrebuild_storage_fd\u001b[39m(\u001b[38;5;28mcls\u001b[39m, df, size):\n\u001b[0;32m--> 305\u001b[0m     fd \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    307\u001b[0m         storage \u001b[38;5;241m=\u001b[39m storage_from_cache(\u001b[38;5;28mcls\u001b[39m, fd_id(fd))\n",
      "File \u001b[0;32m~/.conda/envs/influence/lib/python3.9/multiprocessing/resource_sharer.py:57\u001b[0m, in \u001b[0;36mDupFd.detach\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdetach\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;124;03m'''Get the fd.  This should only be called once.'''\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_resource_sharer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_id\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m conn:\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m reduction\u001b[38;5;241m.\u001b[39mrecv_handle(conn)\n",
      "File \u001b[0;32m~/.conda/envs/influence/lib/python3.9/multiprocessing/resource_sharer.py:87\u001b[0m, in \u001b[0;36m_ResourceSharer.get_connection\u001b[0;34m(ident)\u001b[0m\n\u001b[1;32m     85\u001b[0m address, key \u001b[38;5;241m=\u001b[39m ident\n\u001b[1;32m     86\u001b[0m c \u001b[38;5;241m=\u001b[39m Client(address, authkey\u001b[38;5;241m=\u001b[39mprocess\u001b[38;5;241m.\u001b[39mcurrent_process()\u001b[38;5;241m.\u001b[39mauthkey)\n\u001b[0;32m---> 87\u001b[0m \u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetpid\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m c\n",
      "File \u001b[0;32m~/.conda/envs/influence/lib/python3.9/multiprocessing/connection.py:211\u001b[0m, in \u001b[0;36m_ConnectionBase.send\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_writable()\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_ForkingPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/influence/lib/python3.9/multiprocessing/connection.py:416\u001b[0m, in \u001b[0;36mConnection._send_bytes\u001b[0;34m(self, buf)\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send(buf)\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;66;03m# Issue #20540: concatenate before sending, to avoid delays due\u001b[39;00m\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;66;03m# to Nagle's algorithm on a TCP socket.\u001b[39;00m\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;66;03m# Also note we want to avoid sending a 0-length buffer separately,\u001b[39;00m\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;66;03m# to avoid \"broken pipe\" errors if the other end closed the pipe.\u001b[39;00m\n\u001b[0;32m--> 416\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/influence/lib/python3.9/multiprocessing/connection.py:373\u001b[0m, in \u001b[0;36mConnection._send\u001b[0;34m(self, buf, write)\u001b[0m\n\u001b[1;32m    371\u001b[0m remaining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(buf)\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 373\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m     remaining \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m n\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epsilons = [.001, .005, .01, .05, .1, .15, .2, .25, .3, .35, .4, .45, .5, .55, .6, .65, .7, .75, .8, .85, .9, .95, 1, 5, 10]\n",
    "\n",
    "all_avg = []\n",
    "all_est = []\n",
    "\n",
    "weight_decay = 0.01\n",
    "rec_depth = 5350\n",
    "r = 5\n",
    "scale = 10\n",
    "damp = 0.01 \n",
    "rounds = 5\n",
    "\n",
    "LOO = TestLeaveOneOut()\n",
    "\n",
    "for ro in range(rounds):\n",
    "    avg_losses = []\n",
    "    est_losses = []\n",
    "    for e in epsilons:\n",
    "        print('Working on epsilon: ', e)\n",
    "        avg_loss_diff, est_loss_diff = LOO.test_leave_one_out(e, weight_decay, rec_depth, r, scale, damp)\n",
    "        avg_losses.append(avg_loss_diff)\n",
    "        est_losses.append(est_loss_diff)\n",
    "    all_avg.append(avg_losses)\n",
    "    all_est.append(est_losses)\n",
    "    \n",
    "final_real = [0 for x in range(len(epsilons))]\n",
    "final_est = [0 for x in range(len(epsilons))]\n",
    "\n",
    "for i in range(len(final_real)):\n",
    "    for j in range(len(all_avg)):\n",
    "        final_real[i] = final_real[i] + all_avg[j][i]\n",
    "        final_est[i] = final_est[i] + all_est[j][i]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060094e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph([final_real, final_est, epsilons], 'Actual vs. Estimated Loss per Epsilon', 'epsilon', 'Average Loss', ['Actual', 'Estimated'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752355d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_s = visualize_result(final_real, final_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cc7497",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d815add",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
