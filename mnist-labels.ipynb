{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02f3a89d",
   "metadata": {},
   "source": [
    "### Import Packages and Set Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f320e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import copy\n",
    "import torch\n",
    "import pickle\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scienceplots\n",
    "import torch.nn as nn\n",
    "import torch.optim as opt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from copy import deepcopy\n",
    "from torch.autograd import grad\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from folktables import ACSDataSource, ACSPublicCoverage\n",
    "from sklearn.metrics import mean_absolute_error, log_loss, accuracy_score\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "E = math.e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d88723b",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ead329",
   "metadata": {},
   "source": [
    "#### Get and transform 4 class mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93fe248",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MNIST():\n",
    "    train_dataset = dsets.MNIST(root='', train=True, transform=transforms.ToTensor(), download=True)\n",
    "    test_dataset = dsets.MNIST(root='', train=False, transform=transforms.ToTensor(), download=True)\n",
    "    \n",
    "    idx = (train_dataset.targets==1) | (train_dataset.targets==7) | (train_dataset.targets==3) | (train_dataset.targets==8)\n",
    "    train_dataset.targets = train_dataset.targets[idx]\n",
    "    train_dataset.data = train_dataset.data[idx]\n",
    "    \n",
    "    new_targets = []\n",
    "    \n",
    "    for old_lab in train_dataset.targets:\n",
    "        if old_lab == 1:\n",
    "            new_targets.append(0)\n",
    "        elif old_lab == 3:\n",
    "            new_targets.append(1)\n",
    "        elif old_lab == 7:\n",
    "            new_targets.append(2)\n",
    "        elif old_lab == 8:\n",
    "            new_targets.append(3)\n",
    "            \n",
    "    train_dataset.targets = new_targets\n",
    "    \n",
    "    idx = (test_dataset.targets==1) | (test_dataset.targets==7) | (test_dataset.targets==3) | (test_dataset.targets==8)\n",
    "    test_dataset.targets = test_dataset.targets[idx]\n",
    "    test_dataset.data = test_dataset.data[idx]\n",
    "    \n",
    "    new_targets = []\n",
    "    \n",
    "    for old_lab in test_dataset.targets:\n",
    "        if old_lab == 1:\n",
    "            new_targets.append(0)\n",
    "        elif old_lab == 3:\n",
    "            new_targets.append(1)\n",
    "        elif old_lab == 7:\n",
    "            new_targets.append(2)\n",
    "        elif old_lab == 8:\n",
    "            new_targets.append(3)\n",
    "            \n",
    "    test_dataset.targets = new_targets\n",
    "    print(len(train_dataset))\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31e327c",
   "metadata": {},
   "source": [
    "#### Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5b9aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_result(e_k_actual, e_k_estimated, ep, k_):\n",
    "    plt.rcParams['figure.dpi'] = 300\n",
    "    plt.style.use(['science'])\n",
    "    colors = cm.cool(np.linspace(0, 1, len(e_k_estimated)))\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    ax.yaxis.set_major_formatter(FormatStrFormatter('%.4f'))\n",
    "    ax.xaxis.set_major_formatter(FormatStrFormatter('%.4f'))\n",
    "    \n",
    "    min_x = np.min(e_k_actual)\n",
    "    max_x = np.max(e_k_actual)\n",
    "    min_y = np.min(e_k_estimated)\n",
    "    max_y = np.max(e_k_estimated)\n",
    "    \n",
    "    z = np.polyfit(e_k_actual,  e_k_estimated, 1)\n",
    "    p = np.poly1d(z)\n",
    "    xx = np.linspace(-p(2)/p(1), max(e_k_actual)+.0001)\n",
    "    yy = np.polyval(p, xx)\n",
    "    \n",
    "    ax.plot(xx, yy, ls=\"-\", color='k')\n",
    "    \n",
    "    for k in range(len(e_k_actual)):\n",
    "        ax.scatter(e_k_actual[k], e_k_estimated[k], zorder=2, s=15, color=colors[k])\n",
    "\n",
    "    ax.set_title(f'Actual vs. Estimated loss for k={k_:.2f}%', fontsize=8)\n",
    "    ax.set_xlabel('Actual loss difference', fontsize=8)\n",
    "    ax.set_ylabel('Estimated loss difference', fontsize=8)\n",
    "   \n",
    "    ax.set_xlim(min_x-.0001, max_x+.0001)\n",
    "    ax.set_ylim(min_y-.0001, max_y+.0001)\n",
    "\n",
    "    text = 'MAE = {:.03}\\nP = {:.03}'.format(mean_absolute_error(e_k_actual, e_k_estimated), spearmanr(e_k_actual, e_k_estimated).correlation)\n",
    "    print(text)\n",
    "    plt.xticks(rotation = 45, fontsize=7, visible=True)\n",
    "    plt.yticks(fontsize=7)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4d8af4",
   "metadata": {},
   "source": [
    "#### Custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741afa72",
   "metadata": {},
   "outputs": [],
   "source": [
    " class CreateData(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        out_data = self.data[idx]\n",
    "        out_label = self.targets[idx]\n",
    "\n",
    "        return out_data, out_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9fb573",
   "metadata": {},
   "source": [
    "#### Select k% of a group (based on label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d3bfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(new_train_df, k):    \n",
    "\n",
    "    selected_group = new_train_df.loc[new_train_df['label'] == 0]\n",
    "\n",
    "    num_to_sample = round((k / 100)*len(new_train_df))\n",
    "\n",
    "    try:\n",
    "        sampled_group = selected_group.sample(n=num_to_sample, replace=False)\n",
    "    except ValueError:\n",
    "        sampled_group = selected_group.sample(n=num_to_sample, replace=True)\n",
    "    not_selected = new_train_df.drop(sampled_group.index)\n",
    "\n",
    "    feats = set(new_train_df.columns) - {'label'}\n",
    "    selected_group_X = sampled_group[feats]\n",
    "    selected_group_y = sampled_group['label']\n",
    "\n",
    "    not_selected_group_X = not_selected[feats]\n",
    "    not_selected_group_y = not_selected['label']   \n",
    "    \n",
    "    return selected_group_X, selected_group_y, not_selected_group_X, not_selected_group_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e210d31",
   "metadata": {},
   "source": [
    "### Randomized Response\n",
    "Get the corresponding p and q values based on an epsilon value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20c9e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_p(epsilon):\n",
    "    probability = float(E ** epsilon) / float(3 + (E ** epsilon))\n",
    "    p = torch.FloatTensor([[probability, 1-probability], [1-probability, probability]])\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07756821",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9daf5801",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(784, 4)\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        logits = self.fc1(x)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "    def loss(self, test_loader, print_, device):\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_labels = []\n",
    "        all_predicted = []\n",
    "        all_loss = 0\n",
    "        for images, labels in test_loader:\n",
    "            images = Variable(images.view(-1, 28*28)).to(device)\n",
    "            labels = Variable(labels).to(device)\n",
    "            outputs = self.fc1(images)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            all_loss += loss\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total+= labels.size(0)\n",
    "            \n",
    "            all_labels.extend(list(labels.detach().cpu().numpy()))\n",
    "            all_predicted.extend(list(predicted.detach().cpu().numpy()))\n",
    "            correct+= (predicted.detach().cpu().numpy() == labels.detach().cpu().numpy()).sum()\n",
    "        acc = 100 * correct/total\n",
    "        \n",
    "        return loss/len(test_loader), acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c1a9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data, device):\n",
    "    model.train()\n",
    "    \n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=.5, weight_decay=0)\n",
    "    \n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    train_dataloader = DataLoader(train_data, batch_size=100, shuffle=True)\n",
    "\n",
    "    for itr in range(0, 10):\n",
    "        itr_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, (images, labels) in enumerate(train_dataloader):\n",
    "            images = Variable(images.view(-1, 28*28)).to(device)\n",
    "            labels = Variable(labels).to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            itr_loss += loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total+= labels.size(0)\n",
    "            correct+= (predicted.detach().cpu() == labels.detach().cpu()).sum()\n",
    "                        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3c7499",
   "metadata": {},
   "source": [
    "### Influence Calculation Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc60cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_influence_single(model, epsilon, train_dataset, train_dataloader, test_dataloader, group_data, device, criterion, hessian):\n",
    "    start = time.time()\n",
    "    \n",
    "    if hessian is None:\n",
    "        s_test_vec = s_test_sample(model, test_dataloader, train_dataset, device, criterion)\n",
    "    else:\n",
    "        s_test_vec = hessian \n",
    "        \n",
    "    grad_z_vec = grad_training([group_data[0], group_data[1]], model, device, epsilon)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        influence = (sum([torch.sum(k * j).data for k, j in zip(grad_z_vec, s_test_vec)]) / len(train_dataset))\n",
    "            \n",
    "    end = time.time() - start\n",
    "\n",
    "    return influence.cpu(), end, s_test_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8a2a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def s_test_sample(model, test_dataloader, train_dataset, device, criterion):\n",
    "    scale = 25\n",
    "    damp = 0.01\n",
    "    recursion_depth = 7500\n",
    "    r = 3\n",
    "    \n",
    "    inverse_hvp = [torch.zeros_like(params, dtype=torch.float) for params in model.parameters()]\n",
    "    \n",
    "    for i in range(r):\n",
    "        hessian_loader = DataLoader(train_dataset, sampler=torch.utils.data.RandomSampler(train_dataset, True, num_samples=recursion_depth), batch_size = 1, num_workers=4)\n",
    "        \n",
    "        cur_estimate = s_test(test_dataloader, model, i, hessian_loader, device, damp, scale, criterion)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            inverse_hvp = [old + (cur/scale) for old,cur in zip(inverse_hvp, cur_estimate)]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        inverse_hvp = [component / r for component in inverse_hvp]\n",
    "        \n",
    "    return inverse_hvp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf3e233",
   "metadata": {},
   "outputs": [],
   "source": [
    "def s_test(test_dataloader, model, i, hessian_loader, device, damp, scale, criterion):\n",
    "    v = grad_z(test_dataloader, model, device, criterion)\n",
    "    h_estimate = v\n",
    "    \n",
    "    params, names = make_functional(model)\n",
    "    params = tuple(p.detach().requires_grad_() for p in params)\n",
    "    \n",
    "    progress_bar = tqdm(hessian_loader, desc=f\"IHVP sample {i}\")\n",
    "    \n",
    "    for i, (x_train, y_train) in enumerate(progress_bar):\n",
    "        x_train = Variable(x_train.view(-1, 28*28)).to(device)\n",
    "        y_train = Variable(y_train).to(device)\n",
    "        \n",
    "        def f(*new_params):\n",
    "            load_weights(model, names, new_params)\n",
    "            out = model(x_train)\n",
    "            loss = criterion(out, y_train)\n",
    "            return loss\n",
    "    \n",
    "        hv = vhp(f, params, tuple(h_estimate), strict=True)[1]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            h_estimate = [\n",
    "                _v + (1-damp) * _h_e - _hv / scale for _v, _h_e, _hv in zip(v, h_estimate, hv)\n",
    "            ]\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                norm = sum([h_.norm() for h_ in h_estimate])\n",
    "                progress_bar.set_postfix({\"est norm\": norm.item()})\n",
    "                \n",
    "    with torch.no_grad():\n",
    "        load_weights(model, names, params, as_params=True)\n",
    "        \n",
    "    return h_estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36503a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_functional(model):\n",
    "    orig_params = tuple(model.parameters())\n",
    "    names = []\n",
    "    \n",
    "    for name, p in list(model.named_parameters()):\n",
    "        del_attr(model, name.split(\".\"))\n",
    "        names.append(name)\n",
    "    \n",
    "    return orig_params, names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c305e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_attr(obj, names):\n",
    "    if len(names) == 1:\n",
    "        delattr(obj, names[0])\n",
    "    else:\n",
    "        del_attr(getattr(obj, names[0]), names[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb133a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_attr(obj, names, val):\n",
    "    if len(names) == 1:\n",
    "        setattr(obj, names[0], val)\n",
    "    else:\n",
    "        set_attr(getattr(obj, names[0]), names[1:], val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789f0369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights(model, names, params, as_params=False):\n",
    "    for name, p in zip(names, params):\n",
    "        if not as_params:\n",
    "            set_attr(model, name.split(\".\"), p)\n",
    "        else:\n",
    "            set_attr(model, name.split(\".\"), torch.nn.Parameter(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1656b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_z(test_data, model, device, criterion):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    itr_loss = 0\n",
    "    for i, (images, labels) in enumerate(test_data):\n",
    "        images = Variable(images.view(-1, 28*28)).to(device)\n",
    "        labels = Variable(labels).to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        itr_loss += loss\n",
    "\n",
    "    loss_ = itr_loss / len(test_data)\n",
    "    return grad(loss_, model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ac6b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_training(train_data, model, device, epsilon):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    x_train_input = torch.FloatTensor(train_data[0].values).to(device)\n",
    "    y_train_input = torch.LongTensor(train_data[1].values).to(device)\n",
    "\n",
    "    train_data = CreateData(x_train_input, y_train_input)\n",
    "    train_dataloader = DataLoader(train_data, batch_size=1, shuffle=True)\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "    \n",
    "    agg_loss = 0\n",
    "    possible_labels = [0,1,2,3]\n",
    "    for i, (image, label) in enumerate(train_dataloader):\n",
    "        pert_agg_loss = 0\n",
    "        \n",
    "        output = model(image)\n",
    "        orig_loss = criterion(output, label)\n",
    "\n",
    "        for j in possible_labels:\n",
    "            if j == label.item():\n",
    "                continue\n",
    "            else:\n",
    "                pert_label = torch.LongTensor([j]).to(device)\n",
    "               \n",
    "                pert_loss = criterion(output, pert_label)\n",
    "                pert_agg_loss += (pert_loss - orig_loss)\n",
    "            \n",
    "        itr_loss = pert_agg_loss \n",
    "        agg_loss += itr_loss\n",
    "        \n",
    "    loss = float(1/(3+(E ** epsilon)))*(agg_loss)\n",
    "    \n",
    "    to_return = grad(loss, model.parameters())\n",
    "    \n",
    "        \n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f72f48",
   "metadata": {},
   "source": [
    "### Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe081fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Main(dataset, epsilons, ks, num_rounds):\n",
    "\n",
    "    device = 'cuda:4' if torch.cuda.is_available() else 'cpu'\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    all_orig_loss_e_k = []\n",
    "    all_est_loss_e_k = []\n",
    "    all_time = []\n",
    "    \n",
    "    for nr in range(num_rounds):\n",
    "        print(f'\\nRound {nr+1}')\n",
    "        ############\n",
    "        # Get data #\n",
    "        ############\n",
    "        print('\\nGetting Data...')\n",
    "\n",
    "        train_dataset, test_dataset = get_MNIST()\n",
    "\n",
    "        images_train = []\n",
    "        labels_train = []\n",
    "        \n",
    "        images_test = []\n",
    "        labels_test = []\n",
    "        \n",
    "        for i, (image, label) in enumerate(train_dataset):\n",
    "            images_train.append(image.view(-1, 28*28).tolist()[0])\n",
    "            labels_train.append(label)\n",
    "        \n",
    "        X_train = pd.DataFrame(images_train)\n",
    "        y_train = pd.DataFrame(labels_test, columns =['label'])\n",
    "        \n",
    "        for i, (image, label) in enumerate(test_dataset):\n",
    "            images_test.append(image.view(-1, 28*28).tolist()[0])\n",
    "            labels_test.append(label)\n",
    "        \n",
    "        X_train = pd.DataFrame(images_train)\n",
    "        y_train = pd.DataFrame(labels_train, columns =['label'])\n",
    "        \n",
    "        X_test = pd.DataFrame(images_test)\n",
    "        y_test= pd.DataFrame(labels_test, columns =['label'])\n",
    "        \n",
    "        x_test_input = torch.FloatTensor(X_test.values).to(device)\n",
    "        y_test_input = torch.LongTensor(y_test.values).to(device)\n",
    "\n",
    "        x_train_input = torch.FloatTensor(X_train.values).to(device)\n",
    "        y_train_input = torch.LongTensor(y_train.values).to(device)\n",
    "   \n",
    "        new_train_df = pd.concat([X_train, y_train], axis=1)\n",
    "      \n",
    "        train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100, shuffle=True)\n",
    "        test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=100, shuffle=False)\n",
    "   \n",
    "        ##############################################\n",
    "        # Train original model and get original loss #\n",
    "        ##############################################\n",
    "        print('Training original model...')\n",
    "        torch_model = LogisticRegression()\n",
    "        torch.save(torch_model.state_dict(), 'initial_config_g2g.pth')\n",
    "        torch_model.to(device)\n",
    "        torch_model = train(torch_model, train_dataset, device)\n",
    "        test_loss_ori, acc_ori = torch_model.loss(test_loader, True, device)\n",
    "\n",
    "        e_k_act_losses = []\n",
    "        e_k_est_losses = []\n",
    "        influence_time = []\n",
    "        \n",
    "        ################################################################\n",
    "        # Perform influence and retraining for all epsilons a k values #\n",
    "        ################################################################\n",
    "        print('\\nBegining epsilon and k rounds')\n",
    "        print('-----------------------------')\n",
    "        for k_elem, k in enumerate(ks):\n",
    "            print(f'\\nk: {k}')\n",
    "            hessian = None\n",
    "            k_act_losses = []\n",
    "            k_est_losses = []\n",
    "            inf_time = []\n",
    "            \n",
    "            for ep_elem, ep in enumerate(epsilons):\n",
    "                # Influence\n",
    "                print(f'ep: {ep}')\n",
    "                selected_group_X, selected_group_y, not_selected_group_X, not_selected_group_y = get_data(new_train_df, k)\n",
    "                loss_diff_approx, tot_time, hessian = calc_influence_single(torch_model, ep, train_dataset, train_loader, test_loader, [selected_group_X, selected_group_y, not_selected_group_X, not_selected_group_y], device, criterion, hessian)\n",
    "                loss_diff_approx = -torch.FloatTensor(loss_diff_approx).cpu().numpy()\n",
    "                print(f'Approx difference: {loss_diff_approx:.5f}')\n",
    "                \n",
    "                # Retrain - need to actually perturb\n",
    "                p_stay = ((E**ep)/(3+(E**ep)))\n",
    "                p_change = (1/(3+(E**ep)))\n",
    "\n",
    "                G_pert = []\n",
    "                G_labels = selected_group_y.values.tolist()\n",
    "                \n",
    "                for lab in G_labels:\n",
    "                    poss_labels = [0,1,2,3]\n",
    "                    \n",
    "                    weights = [p_change for i in range(len(poss_labels))]\n",
    "                    weights[lab] = p_stay\n",
    "                    \n",
    "                    pert_lab = random.choices(poss_labels, weights=weights, k=1)\n",
    "                    G_pert.append(pert_lab[0])\n",
    "                    \n",
    "                y_w_group_pert = pd.concat([not_selected_group_y, pd.DataFrame(G_pert)], axis = 0, ignore_index=True)\n",
    "                y_w_group_pert = y_w_group_pert.values.tolist()\n",
    "                y_w_group_pert = [yw for sublist in y_w_group_pert for yw in sublist]\n",
    "               \n",
    "                y_wo_pert = pd.concat([not_selected_group_y, selected_group_y], axis = 0, ignore_index=True)\n",
    "                reconstructed_x = pd.concat([not_selected_group_X, selected_group_X], axis = 0, ignore_index=True)\n",
    "                x_train_input_pert = torch.FloatTensor(reconstructed_x.values).to(device)\n",
    "                y_train_input_pert = torch.LongTensor(y_w_group_pert).to(device)\n",
    "\n",
    "                train_data_pert = CreateData(x_train_input_pert, y_train_input_pert)\n",
    "                model_pert = LogisticRegression()\n",
    "                model_pert.load_state_dict(torch.load('initial_config_g2g.pth'))\n",
    "                model_pert.to(device)\n",
    "                torch_model = train(model_pert, train_data_pert, device)\n",
    "                test_loss_retrain, acc_retrain = model_pert.loss(test_loader, True, device)\n",
    "\n",
    "                 # get true loss diff\n",
    "                loss_diff_true = (test_loss_retrain - test_loss_ori).detach().cpu().item()\n",
    "                print(f'True difference: {loss_diff_true:.5f}')\n",
    "                k_act_losses.append(loss_diff_true)\n",
    "                k_est_losses.append(loss_diff_approx)\n",
    "                inf_time.append(tot_time)\n",
    "            \n",
    "            visualize_result(k_act_losses, k_est_losses, epsilons, k)\n",
    "            \n",
    "            e_k_act_losses.append(k_act_losses)\n",
    "            e_k_est_losses.append(k_est_losses)\n",
    "            influence_time.append(inf_time)\n",
    "            \n",
    "        all_orig_loss_e_k.append(e_k_act_losses)\n",
    "        all_est_loss_e_k.append(e_k_est_losses) \n",
    "        all_time.append(influence_time)\n",
    "    \n",
    "    return all_orig_loss_e_k, all_est_loss_e_k, all_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dfc9de",
   "metadata": {},
   "source": [
    "### Perform Experiment "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2747a8a8",
   "metadata": {},
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e3ef67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epsilons = np.linspace(.001, 5, 30) #30 5\n",
    "k = np.linspace(1, 30, 10) #10\n",
    "rounds = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4d7588",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_orig_loss_e_k, all_est_loss_e_k, all_time = Main('mnist', epsilons, k, rounds)\n",
    "\n",
    "with open('results/mnist/all_orig_loss_e_k_mnist.txt', \"wb\") as file:   #Pickling\n",
    "    pickle.dump(all_orig_loss_e_k, file)\n",
    "\n",
    "with open('results/mnist/all_est_loss_e_k_mnist.txt', \"wb\") as file2:   #Pickling\n",
    "    pickle.dump(all_est_loss_e_k, file2)\n",
    "    \n",
    "with open('results/mnist/all_time_mnist.txt', \"wb\") as file3:   #Pickling\n",
    "    pickle.dump(all_time, file3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68ef165",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_orig_loss_e_k = [[0 for _ in range(len(k))] for _ in range(len(epsilons))]\n",
    "sum_est_loss_e_k = [[0 for _ in range(len(k))] for _ in range(len(epsilons))]\n",
    "sum_time = [[0 for _ in range(len(k))] for _ in range(len(epsilons))]\n",
    "\n",
    "avg_orig_loss = []\n",
    "avg_est_loss = []\n",
    "avg_time = []\n",
    "\n",
    "for round_ in range(len(all_orig_loss_e_k)):\n",
    "    for e in range(len(epsilons)):\n",
    "        for k_ in range(len(k)):\n",
    "            sum_orig_loss_e_k[e][k_] = sum_orig_loss_e_k[e][k_] + all_orig_loss_e_k[round_][e][k_]\n",
    "            sum_est_loss_e_k[e][k_] = sum_est_loss_e_k[e][k_] + all_est_loss_e_k[round_][e][k_]\n",
    "            sum_time[e][k_] = sum_time[e][k_] + all_time[round_][e][k_]\n",
    "\n",
    "for e in range(len(epsilons)):\n",
    "    avg_orig_loss.append([ elem / len(all_orig_loss_e_k) for elem in sum_orig_loss_e_k[e]])\n",
    "    avg_est_loss.append([elem/ len(all_orig_loss_e_k) for elem in sum_est_loss_e_k[e]])\n",
    "    avg_time.append([elem/ len(all_orig_loss_e_k) for elem in sum_time[e]])\n",
    "\n",
    "k_e_orig = [[] for _ in range(len(k))]\n",
    "k_e_est = [[] for _ in range(len(k))]\n",
    "\n",
    "for e in range(len(epsilons)):\n",
    "    for k_ in range(len(k)):\n",
    "        k_e_orig[k_].append(avg_orig_loss[e][k_])\n",
    "        k_e_est[k_].append(avg_est_loss[e][k_])\n",
    "\n",
    "averaged_time = []\n",
    "\n",
    "for e in range(len(epsilons)):\n",
    "    averaged_time.append(sum_time[e][0])\n",
    "\n",
    "average_time_final = sum(averaged_time) / len(averaged_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752355d6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(k_e_orig)):\n",
    "    visualize_result(k_e_orig[i], k_e_est[i], epsilons, k_[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
